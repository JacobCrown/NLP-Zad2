{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stałe\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LEN = 128\n",
    "TRAIN_SIZE = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence_id  word_id        word  Olek  Kuba Zgodne?  Stachu  \\\n",
      "0            1        1          Do     3     3       T     NaN   \n",
      "1            1        2       Bosch     1     1       T     NaN   \n",
      "2            1        3  SMV53L10EU     1     1       T     NaN   \n",
      "3            1        4      pasuje     2     2       T     NaN   \n",
      "4            1        5    IDEALNIE     2     2       T     NaN   \n",
      "\n",
      "   final-annotation  Unnamed: 8  \n",
      "0                 3         NaN  \n",
      "1                 1         NaN  \n",
      "2                 1         NaN  \n",
      "3                 2         NaN  \n",
      "4                 2         NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Wczytanie i przygotowanie danych\n",
    "df = pd.read_csv('data/annotations_all_batches - WORD - SECOND BATCH.csv')\n",
    "print(df.head())\n",
    "df = df.fillna(method='ffill')\n",
    "\n",
    "# Grupowanie po sentence_id\n",
    "sentences = df.groupby('sentence_id').agg({\n",
    "    'word': lambda x: list(x),\n",
    "    'final-annotation': lambda x: list(x)\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b768b70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model is ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Model setup with PEFT\n",
    "model_name = \"allegro/herbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],  # Layers to apply LoRA\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Wrap the model with PEFT\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "print(\"PEFT model is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Klasa dataset\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        # Tokenizacja\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Dostosowanie etykiet do tokenów\n",
    "        word_ids = encoding.word_ids()\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(labels[word_id])\n",
    "                \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label_ids)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Przygotowanie danych\n",
    "texts = sentences['word'].values\n",
    "labels = sentences['final-annotation'].values\n",
    "\n",
    "# Podział na zbiór treningowy i testowy\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, train_size=TRAIN_SIZE, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Przygotowanie datasetów\n",
    "train_dataset = TokenClassificationDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "test_dataset = TokenClassificationDataset(test_texts, test_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3591282069683075\n",
      "Epoch 2, Loss: 0.35966864228248596\n",
      "Epoch 3, Loss: 0.3706051707267761\n",
      "Epoch 4, Loss: 0.375672310590744\n",
      "Epoch 5, Loss: 0.36563655734062195\n",
      "Epoch 6, Loss: 0.36577484011650085\n",
      "Epoch 7, Loss: 0.3424343466758728\n",
      "Epoch 8, Loss: 0.3571338355541229\n",
      "Epoch 9, Loss: 0.3628283739089966\n",
      "Epoch 10, Loss: 0.3712102174758911\n",
      "Epoch 11, Loss: 0.35093939304351807\n",
      "Epoch 12, Loss: 0.3522355854511261\n",
      "Epoch 13, Loss: 0.3515315353870392\n",
      "Epoch 14, Loss: 0.37684470415115356\n",
      "Epoch 15, Loss: 0.3551165461540222\n",
      "Epoch 16, Loss: 0.34163784980773926\n",
      "Epoch 17, Loss: 0.36297473311424255\n",
      "Epoch 18, Loss: 0.3253186047077179\n",
      "Epoch 19, Loss: 0.33071136474609375\n",
      "Epoch 20, Loss: 0.3614720106124878\n",
      "Epoch 21, Loss: 0.3497300446033478\n",
      "Epoch 22, Loss: 0.3621300160884857\n",
      "Epoch 23, Loss: 0.337888240814209\n",
      "Epoch 24, Loss: 0.35919275879859924\n",
      "Epoch 25, Loss: 0.3435458838939667\n",
      "Epoch 26, Loss: 0.34478920698165894\n",
      "Epoch 27, Loss: 0.3438310921192169\n",
      "Epoch 28, Loss: 0.3561217784881592\n",
      "Epoch 29, Loss: 0.36235255002975464\n",
      "Epoch 30, Loss: 0.32979923486709595\n",
      "Epoch 31, Loss: 0.3296760022640228\n",
      "Epoch 32, Loss: 0.34714245796203613\n",
      "Epoch 33, Loss: 0.34279540181159973\n",
      "Epoch 34, Loss: 0.3651546239852905\n",
      "Epoch 35, Loss: 0.3328595459461212\n",
      "Epoch 36, Loss: 0.3193976879119873\n",
      "Epoch 37, Loss: 0.32470834255218506\n",
      "Epoch 38, Loss: 0.3363846242427826\n",
      "Epoch 39, Loss: 0.32663336396217346\n",
      "Epoch 40, Loss: 0.36801305413246155\n",
      "Epoch 41, Loss: 0.3442985713481903\n",
      "Epoch 42, Loss: 0.34212955832481384\n",
      "Epoch 43, Loss: 0.3078090250492096\n",
      "Epoch 44, Loss: 0.33744969964027405\n",
      "Epoch 45, Loss: 0.3201165497303009\n",
      "Epoch 46, Loss: 0.32051530480384827\n",
      "Epoch 47, Loss: 0.3250543475151062\n",
      "Epoch 48, Loss: 0.3447370231151581\n",
      "Epoch 49, Loss: 0.32178014516830444\n",
      "Epoch 50, Loss: 0.3229541778564453\n",
      "Epoch 51, Loss: 0.31299325823783875\n",
      "Epoch 52, Loss: 0.33160918951034546\n",
      "Epoch 53, Loss: 0.35062333941459656\n",
      "Epoch 54, Loss: 0.3217849135398865\n",
      "Epoch 55, Loss: 0.3295023441314697\n",
      "Epoch 56, Loss: 0.30674585700035095\n",
      "Epoch 57, Loss: 0.347599059343338\n",
      "Epoch 58, Loss: 0.340727299451828\n",
      "Epoch 59, Loss: 0.31796830892562866\n",
      "Epoch 60, Loss: 0.34090426564216614\n",
      "Epoch 61, Loss: 0.3192034661769867\n",
      "Epoch 62, Loss: 0.32590416073799133\n",
      "Epoch 63, Loss: 0.31899455189704895\n",
      "Epoch 64, Loss: 0.3547728657722473\n",
      "Epoch 65, Loss: 0.3118301033973694\n",
      "Epoch 66, Loss: 0.29896119236946106\n",
      "Epoch 67, Loss: 0.3390505313873291\n",
      "Epoch 68, Loss: 0.3211064636707306\n",
      "Epoch 69, Loss: 0.32988688349723816\n",
      "Epoch 70, Loss: 0.30968722701072693\n",
      "Epoch 71, Loss: 0.2963748872280121\n",
      "Epoch 72, Loss: 0.2888357639312744\n",
      "Epoch 73, Loss: 0.30556461215019226\n",
      "Epoch 74, Loss: 0.31307917833328247\n",
      "Epoch 75, Loss: 0.2960660755634308\n",
      "Epoch 76, Loss: 0.3243778645992279\n",
      "Epoch 77, Loss: 0.30764907598495483\n",
      "Epoch 78, Loss: 0.2897828221321106\n",
      "Epoch 79, Loss: 0.3058222830295563\n",
      "Epoch 80, Loss: 0.27778735756874084\n",
      "Epoch 81, Loss: 0.3394189476966858\n",
      "Epoch 82, Loss: 0.3343789279460907\n",
      "Epoch 83, Loss: 0.302757203578949\n",
      "Epoch 84, Loss: 0.3276374936103821\n",
      "Epoch 85, Loss: 0.3079450726509094\n",
      "Epoch 86, Loss: 0.31743738055229187\n",
      "Epoch 87, Loss: 0.32214853167533875\n",
      "Epoch 88, Loss: 0.3042451739311218\n",
      "Epoch 89, Loss: 0.32389453053474426\n",
      "Epoch 90, Loss: 0.30904197692871094\n",
      "Epoch 91, Loss: 0.29082679748535156\n",
      "Epoch 92, Loss: 0.32206210494041443\n",
      "Epoch 93, Loss: 0.3314107656478882\n",
      "Epoch 94, Loss: 0.2974155843257904\n",
      "Epoch 95, Loss: 0.30009493231773376\n",
      "Epoch 96, Loss: 0.27662593126296997\n",
      "Epoch 97, Loss: 0.298658549785614\n",
      "Epoch 98, Loss: 0.2820359468460083\n",
      "Epoch 99, Loss: 0.274183452129364\n",
      "Epoch 100, Loss: 0.28174564242362976\n",
      "Epoch 101, Loss: 0.2932751178741455\n",
      "Epoch 102, Loss: 0.3094491958618164\n",
      "Epoch 103, Loss: 0.301676869392395\n",
      "Epoch 104, Loss: 0.292034387588501\n",
      "Epoch 105, Loss: 0.30541032552719116\n",
      "Epoch 106, Loss: 0.293995201587677\n",
      "Epoch 107, Loss: 0.27347108721733093\n",
      "Epoch 108, Loss: 0.33257946372032166\n",
      "Epoch 109, Loss: 0.31414783000946045\n",
      "Epoch 110, Loss: 0.3127800524234772\n",
      "Epoch 111, Loss: 0.2811834216117859\n",
      "Epoch 112, Loss: 0.3025782108306885\n",
      "Epoch 113, Loss: 0.3048914074897766\n",
      "Epoch 114, Loss: 0.27297332882881165\n",
      "Epoch 115, Loss: 0.30723413825035095\n",
      "Epoch 116, Loss: 0.27850428223609924\n",
      "Epoch 117, Loss: 0.30191224813461304\n",
      "Epoch 118, Loss: 0.2848520576953888\n",
      "Epoch 119, Loss: 0.2823348641395569\n",
      "Epoch 120, Loss: 0.290167897939682\n",
      "Epoch 121, Loss: 0.28588244318962097\n",
      "Epoch 122, Loss: 0.26548704504966736\n",
      "Epoch 123, Loss: 0.27951908111572266\n",
      "Epoch 124, Loss: 0.29175320267677307\n",
      "Epoch 125, Loss: 0.287293940782547\n",
      "Epoch 126, Loss: 0.2793266475200653\n",
      "Epoch 127, Loss: 0.2992905080318451\n",
      "Epoch 128, Loss: 0.2848556637763977\n",
      "Epoch 129, Loss: 0.2869177460670471\n",
      "Epoch 130, Loss: 0.2937912344932556\n",
      "Epoch 131, Loss: 0.28079456090927124\n",
      "Epoch 132, Loss: 0.2801877558231354\n",
      "Epoch 133, Loss: 0.287476509809494\n",
      "Epoch 134, Loss: 0.2854095995426178\n",
      "Epoch 135, Loss: 0.2615356147289276\n",
      "Epoch 136, Loss: 0.29775264859199524\n",
      "Epoch 137, Loss: 0.2836558222770691\n",
      "Epoch 138, Loss: 0.30318912863731384\n",
      "Epoch 139, Loss: 0.256879597902298\n",
      "Epoch 140, Loss: 0.28469279408454895\n",
      "Epoch 141, Loss: 0.27574896812438965\n",
      "Epoch 142, Loss: 0.276592493057251\n",
      "Epoch 143, Loss: 0.2775821089744568\n",
      "Epoch 144, Loss: 0.271542489528656\n",
      "Epoch 145, Loss: 0.2646231949329376\n",
      "Epoch 146, Loss: 0.2719793915748596\n",
      "Epoch 147, Loss: 0.25882506370544434\n",
      "Epoch 148, Loss: 0.31282979249954224\n",
      "Epoch 149, Loss: 0.2879863381385803\n",
      "Epoch 150, Loss: 0.2779528498649597\n",
      "Epoch 151, Loss: 0.2476550191640854\n",
      "Epoch 152, Loss: 0.27480658888816833\n",
      "Epoch 153, Loss: 0.2516411542892456\n",
      "Epoch 154, Loss: 0.2706835865974426\n",
      "Epoch 155, Loss: 0.27858445048332214\n",
      "Epoch 156, Loss: 0.2639653980731964\n",
      "Epoch 157, Loss: 0.2674492299556732\n",
      "Epoch 158, Loss: 0.253491073846817\n",
      "Epoch 159, Loss: 0.2747806906700134\n",
      "Epoch 160, Loss: 0.27847859263420105\n",
      "Epoch 161, Loss: 0.26359450817108154\n",
      "Epoch 162, Loss: 0.26221463084220886\n",
      "Epoch 163, Loss: 0.24554021656513214\n",
      "Epoch 164, Loss: 0.24811947345733643\n",
      "Epoch 165, Loss: 0.28869372606277466\n",
      "Epoch 166, Loss: 0.2579612731933594\n",
      "Epoch 167, Loss: 0.2528636157512665\n",
      "Epoch 168, Loss: 0.28067219257354736\n",
      "Epoch 169, Loss: 0.2766391336917877\n",
      "Epoch 170, Loss: 0.2751733958721161\n",
      "Epoch 171, Loss: 0.26655492186546326\n",
      "Epoch 172, Loss: 0.2571101188659668\n",
      "Epoch 173, Loss: 0.23984041810035706\n",
      "Epoch 174, Loss: 0.2437266856431961\n",
      "Epoch 175, Loss: 0.255298912525177\n",
      "Epoch 176, Loss: 0.27060970664024353\n",
      "Epoch 177, Loss: 0.2558861970901489\n",
      "Epoch 178, Loss: 0.24840350449085236\n",
      "Epoch 179, Loss: 0.2578801214694977\n",
      "Epoch 180, Loss: 0.25109124183654785\n",
      "Epoch 181, Loss: 0.2526751160621643\n",
      "Epoch 182, Loss: 0.25977274775505066\n",
      "Epoch 183, Loss: 0.2460930496454239\n",
      "Epoch 184, Loss: 0.24681444466114044\n",
      "Epoch 185, Loss: 0.23301035165786743\n",
      "Epoch 186, Loss: 0.23352573812007904\n",
      "Epoch 187, Loss: 0.2440795600414276\n",
      "Epoch 188, Loss: 0.2416326254606247\n",
      "Epoch 189, Loss: 0.2338794469833374\n",
      "Epoch 190, Loss: 0.24449263513088226\n",
      "Epoch 191, Loss: 0.23664648830890656\n",
      "Epoch 192, Loss: 0.2635509669780731\n",
      "Epoch 193, Loss: 0.24469411373138428\n",
      "Epoch 194, Loss: 0.24085263907909393\n",
      "Epoch 195, Loss: 0.24179615080356598\n",
      "Epoch 196, Loss: 0.22524838149547577\n",
      "Epoch 197, Loss: 0.2666929066181183\n",
      "Epoch 198, Loss: 0.22839617729187012\n",
      "Epoch 199, Loss: 0.2469426840543747\n",
      "Epoch 200, Loss: 0.2427840381860733\n",
      "Epoch 201, Loss: 0.240397647023201\n",
      "Epoch 202, Loss: 0.24163076281547546\n",
      "Epoch 203, Loss: 0.23912560939788818\n",
      "Epoch 204, Loss: 0.21940965950489044\n",
      "Epoch 205, Loss: 0.2779313623905182\n",
      "Epoch 206, Loss: 0.2492733895778656\n",
      "Epoch 207, Loss: 0.2409948855638504\n",
      "Epoch 208, Loss: 0.231054425239563\n",
      "Epoch 209, Loss: 0.23423542082309723\n",
      "Epoch 210, Loss: 0.21010597050189972\n",
      "Epoch 211, Loss: 0.24794574081897736\n",
      "Epoch 212, Loss: 0.25812262296676636\n",
      "Epoch 213, Loss: 0.24155403673648834\n",
      "Epoch 214, Loss: 0.25868454575538635\n",
      "Epoch 215, Loss: 0.23104120790958405\n",
      "Epoch 216, Loss: 0.23009814321994781\n",
      "Epoch 217, Loss: 0.23960131406784058\n",
      "Epoch 218, Loss: 0.22679269313812256\n",
      "Epoch 219, Loss: 0.229742169380188\n",
      "Epoch 220, Loss: 0.22167691588401794\n",
      "Epoch 221, Loss: 0.22310808300971985\n",
      "Epoch 222, Loss: 0.2371063530445099\n",
      "Epoch 223, Loss: 0.2444743663072586\n",
      "Epoch 224, Loss: 0.24944603443145752\n",
      "Epoch 225, Loss: 0.21312150359153748\n",
      "Epoch 226, Loss: 0.23953017592430115\n",
      "Epoch 227, Loss: 0.19480708241462708\n",
      "Epoch 228, Loss: 0.21744616329669952\n",
      "Epoch 229, Loss: 0.2148631513118744\n",
      "Epoch 230, Loss: 0.22349417209625244\n",
      "Epoch 231, Loss: 0.2181168794631958\n",
      "Epoch 232, Loss: 0.22940614819526672\n",
      "Epoch 233, Loss: 0.23720891773700714\n",
      "Epoch 234, Loss: 0.2388582080602646\n",
      "Epoch 235, Loss: 0.2254127562046051\n",
      "Epoch 236, Loss: 0.20367011427879333\n",
      "Epoch 237, Loss: 0.22840002179145813\n",
      "Epoch 238, Loss: 0.2276066392660141\n",
      "Epoch 239, Loss: 0.23489288985729218\n",
      "Epoch 240, Loss: 0.22771941125392914\n",
      "Epoch 241, Loss: 0.21102125942707062\n",
      "Epoch 242, Loss: 0.215114563703537\n",
      "Epoch 243, Loss: 0.21345573663711548\n",
      "Epoch 244, Loss: 0.21623225510120392\n",
      "Epoch 245, Loss: 0.22002357244491577\n",
      "Epoch 246, Loss: 0.22189541161060333\n",
      "Epoch 247, Loss: 0.20299573242664337\n",
      "Epoch 248, Loss: 0.19794955849647522\n",
      "Epoch 249, Loss: 0.2274860292673111\n",
      "Epoch 250, Loss: 0.2066533863544464\n",
      "Epoch 251, Loss: 0.204793319106102\n",
      "Epoch 252, Loss: 0.2284676730632782\n",
      "Epoch 253, Loss: 0.2154289335012436\n",
      "Epoch 254, Loss: 0.2126207947731018\n",
      "Epoch 255, Loss: 0.20953473448753357\n",
      "Epoch 256, Loss: 0.184508815407753\n",
      "Epoch 257, Loss: 0.21386148035526276\n",
      "Epoch 258, Loss: 0.2018861025571823\n",
      "Epoch 259, Loss: 0.2015226185321808\n",
      "Epoch 260, Loss: 0.1861870139837265\n",
      "Epoch 261, Loss: 0.20463640987873077\n",
      "Epoch 262, Loss: 0.23672179877758026\n",
      "Epoch 263, Loss: 0.20167481899261475\n",
      "Epoch 264, Loss: 0.201997771859169\n",
      "Epoch 265, Loss: 0.20844964683055878\n",
      "Epoch 266, Loss: 0.20645387470722198\n",
      "Epoch 267, Loss: 0.19538374245166779\n",
      "Epoch 268, Loss: 0.18772582709789276\n",
      "Epoch 269, Loss: 0.21075862646102905\n",
      "Epoch 270, Loss: 0.23665818572044373\n",
      "Epoch 271, Loss: 0.18884217739105225\n",
      "Epoch 272, Loss: 0.19812844693660736\n",
      "Epoch 273, Loss: 0.1891814023256302\n",
      "Epoch 274, Loss: 0.20055444538593292\n",
      "Epoch 275, Loss: 0.23743300139904022\n",
      "Epoch 276, Loss: 0.1925046145915985\n",
      "Epoch 277, Loss: 0.20766668021678925\n",
      "Epoch 278, Loss: 0.19197124242782593\n",
      "Epoch 279, Loss: 0.21405138075351715\n",
      "Epoch 280, Loss: 0.19602060317993164\n",
      "Epoch 281, Loss: 0.20335830748081207\n",
      "Epoch 282, Loss: 0.20824381709098816\n",
      "Epoch 283, Loss: 0.18642090260982513\n",
      "Epoch 284, Loss: 0.17970801889896393\n",
      "Epoch 285, Loss: 0.20194758474826813\n",
      "Epoch 286, Loss: 0.20492732524871826\n",
      "Epoch 287, Loss: 0.1940958946943283\n",
      "Epoch 288, Loss: 0.1829088032245636\n",
      "Epoch 289, Loss: 0.18384872376918793\n",
      "Epoch 290, Loss: 0.22345443069934845\n",
      "Epoch 291, Loss: 0.1848234385251999\n",
      "Epoch 292, Loss: 0.18037794530391693\n",
      "Epoch 293, Loss: 0.19653750956058502\n",
      "Epoch 294, Loss: 0.18177984654903412\n",
      "Epoch 295, Loss: 0.210844025015831\n",
      "Epoch 296, Loss: 0.18062786757946014\n",
      "Epoch 297, Loss: 0.1718658059835434\n",
      "Epoch 298, Loss: 0.19604645669460297\n",
      "Epoch 299, Loss: 0.20691244304180145\n",
      "Epoch 300, Loss: 0.18461166322231293\n",
      "Epoch 301, Loss: 0.1756644994020462\n",
      "Epoch 302, Loss: 0.18427371978759766\n",
      "Epoch 303, Loss: 0.17268607020378113\n",
      "Epoch 304, Loss: 0.1805071383714676\n",
      "Epoch 305, Loss: 0.17956867814064026\n",
      "Epoch 306, Loss: 0.1982896625995636\n",
      "Epoch 307, Loss: 0.19560261070728302\n",
      "Epoch 308, Loss: 0.20723886787891388\n",
      "Epoch 309, Loss: 0.17570126056671143\n",
      "Epoch 310, Loss: 0.18155021965503693\n",
      "Epoch 311, Loss: 0.18780948221683502\n",
      "Epoch 312, Loss: 0.18897826969623566\n",
      "Epoch 313, Loss: 0.18049772083759308\n",
      "Epoch 314, Loss: 0.17908097803592682\n",
      "Epoch 315, Loss: 0.15967713296413422\n",
      "Epoch 316, Loss: 0.18250536918640137\n",
      "Epoch 317, Loss: 0.16525697708129883\n",
      "Epoch 318, Loss: 0.1770242303609848\n",
      "Epoch 319, Loss: 0.1493193805217743\n",
      "Epoch 320, Loss: 0.18304583430290222\n",
      "Epoch 321, Loss: 0.1749092936515808\n",
      "Epoch 322, Loss: 0.16364572942256927\n",
      "Epoch 323, Loss: 0.19468705356121063\n",
      "Epoch 324, Loss: 0.15444974601268768\n",
      "Epoch 325, Loss: 0.16703394055366516\n",
      "Epoch 326, Loss: 0.18006950616836548\n",
      "Epoch 327, Loss: 0.15920792520046234\n",
      "Epoch 328, Loss: 0.15174709260463715\n",
      "Epoch 329, Loss: 0.17548730969429016\n",
      "Epoch 330, Loss: 0.173439621925354\n",
      "Epoch 331, Loss: 0.15164047479629517\n",
      "Epoch 332, Loss: 0.16195480525493622\n",
      "Epoch 333, Loss: 0.17510828375816345\n",
      "Epoch 334, Loss: 0.17548032104969025\n",
      "Epoch 335, Loss: 0.2043384164571762\n",
      "Epoch 336, Loss: 0.17119692265987396\n",
      "Epoch 337, Loss: 0.16686874628067017\n",
      "Epoch 338, Loss: 0.14693301916122437\n",
      "Epoch 339, Loss: 0.14955201745033264\n",
      "Epoch 340, Loss: 0.16774791479110718\n",
      "Epoch 341, Loss: 0.1508806198835373\n",
      "Epoch 342, Loss: 0.16856615245342255\n",
      "Epoch 343, Loss: 0.15335722267627716\n",
      "Epoch 344, Loss: 0.15312433242797852\n",
      "Epoch 345, Loss: 0.16280581057071686\n",
      "Epoch 346, Loss: 0.16977447271347046\n",
      "Epoch 347, Loss: 0.15403951704502106\n",
      "Epoch 348, Loss: 0.17072656750679016\n",
      "Epoch 349, Loss: 0.1677001267671585\n",
      "Epoch 350, Loss: 0.17030911147594452\n",
      "Epoch 351, Loss: 0.15593191981315613\n",
      "Epoch 352, Loss: 0.15870973467826843\n",
      "Epoch 353, Loss: 0.1716310828924179\n",
      "Epoch 354, Loss: 0.18201769888401031\n",
      "Epoch 355, Loss: 0.17519812285900116\n",
      "Epoch 356, Loss: 0.14919915795326233\n",
      "Epoch 357, Loss: 0.18642666935920715\n",
      "Epoch 358, Loss: 0.16328205168247223\n",
      "Epoch 359, Loss: 0.16914279758930206\n",
      "Epoch 360, Loss: 0.127409890294075\n",
      "Epoch 361, Loss: 0.14023205637931824\n",
      "Epoch 362, Loss: 0.1510932892560959\n",
      "Epoch 363, Loss: 0.17186781764030457\n",
      "Epoch 364, Loss: 0.1644742786884308\n",
      "Epoch 365, Loss: 0.1460903137922287\n",
      "Epoch 366, Loss: 0.15976068377494812\n",
      "Epoch 367, Loss: 0.14932340383529663\n",
      "Epoch 368, Loss: 0.15105749666690826\n",
      "Epoch 369, Loss: 0.1555725336074829\n",
      "Epoch 370, Loss: 0.14006608724594116\n",
      "Epoch 371, Loss: 0.1526096761226654\n",
      "Epoch 372, Loss: 0.16170212626457214\n",
      "Epoch 373, Loss: 0.15944185853004456\n",
      "Epoch 374, Loss: 0.1407540738582611\n",
      "Epoch 375, Loss: 0.1806834489107132\n",
      "Epoch 376, Loss: 0.1558820903301239\n",
      "Epoch 377, Loss: 0.18625789880752563\n",
      "Epoch 378, Loss: 0.14626182615756989\n",
      "Epoch 379, Loss: 0.13866323232650757\n",
      "Epoch 380, Loss: 0.15215253829956055\n",
      "Epoch 381, Loss: 0.16861245036125183\n",
      "Epoch 382, Loss: 0.15807098150253296\n",
      "Epoch 383, Loss: 0.15975943207740784\n",
      "Epoch 384, Loss: 0.15048466622829437\n",
      "Epoch 385, Loss: 0.14388470351696014\n",
      "Epoch 386, Loss: 0.15850107371807098\n",
      "Epoch 387, Loss: 0.1580733060836792\n",
      "Epoch 388, Loss: 0.15761321783065796\n",
      "Epoch 389, Loss: 0.14388403296470642\n",
      "Epoch 390, Loss: 0.15782609581947327\n",
      "Epoch 391, Loss: 0.12675030529499054\n",
      "Epoch 392, Loss: 0.14221391081809998\n",
      "Epoch 393, Loss: 0.15654240548610687\n",
      "Epoch 394, Loss: 0.15117914974689484\n",
      "Epoch 395, Loss: 0.12826180458068848\n",
      "Epoch 396, Loss: 0.13410289585590363\n",
      "Epoch 397, Loss: 0.1492471694946289\n",
      "Epoch 398, Loss: 0.13035179674625397\n",
      "Epoch 399, Loss: 0.12422576546669006\n",
      "Epoch 400, Loss: 0.13392144441604614\n",
      "Epoch 401, Loss: 0.15377476811408997\n",
      "Epoch 402, Loss: 0.2598584294319153\n",
      "Epoch 403, Loss: 0.132196307182312\n",
      "Epoch 404, Loss: 0.14449144899845123\n",
      "Epoch 405, Loss: 0.14155232906341553\n",
      "Epoch 406, Loss: 0.15028700232505798\n",
      "Epoch 407, Loss: 0.15472713112831116\n",
      "Epoch 408, Loss: 0.14786599576473236\n",
      "Epoch 409, Loss: 0.15451273322105408\n",
      "Epoch 410, Loss: 0.14129674434661865\n",
      "Epoch 411, Loss: 0.1437217891216278\n",
      "Epoch 412, Loss: 0.11816652864217758\n",
      "Epoch 413, Loss: 0.1290508359670639\n",
      "Epoch 414, Loss: 0.14485923945903778\n",
      "Epoch 415, Loss: 0.13565540313720703\n",
      "Epoch 416, Loss: 0.1374189704656601\n",
      "Epoch 417, Loss: 0.12709547579288483\n",
      "Epoch 418, Loss: 0.12665219604969025\n",
      "Epoch 419, Loss: 0.13864612579345703\n",
      "Epoch 420, Loss: 0.1419411450624466\n",
      "Epoch 421, Loss: 0.11658499389886856\n",
      "Epoch 422, Loss: 0.1508733034133911\n",
      "Epoch 423, Loss: 0.11320225149393082\n",
      "Epoch 424, Loss: 0.13637831807136536\n",
      "Epoch 425, Loss: 0.13242410123348236\n",
      "Epoch 426, Loss: 0.12462536245584488\n",
      "Epoch 427, Loss: 0.1179998442530632\n",
      "Epoch 428, Loss: 0.16906201839447021\n",
      "Epoch 429, Loss: 0.10710703581571579\n",
      "Epoch 430, Loss: 0.12393402308225632\n",
      "Epoch 431, Loss: 0.14199860394001007\n",
      "Epoch 432, Loss: 0.1510315090417862\n",
      "Epoch 433, Loss: 0.13505636155605316\n",
      "Epoch 434, Loss: 0.12570111453533173\n",
      "Epoch 435, Loss: 0.13093605637550354\n",
      "Epoch 436, Loss: 0.14517199993133545\n",
      "Epoch 437, Loss: 0.11641845852136612\n",
      "Epoch 438, Loss: 0.1365157663822174\n",
      "Epoch 439, Loss: 0.12835712730884552\n",
      "Epoch 440, Loss: 0.1263761669397354\n",
      "Epoch 441, Loss: 0.12049633264541626\n",
      "Epoch 442, Loss: 0.10936008393764496\n",
      "Epoch 443, Loss: 0.12185494601726532\n",
      "Epoch 444, Loss: 0.11909515410661697\n",
      "Epoch 445, Loss: 0.10645052790641785\n",
      "Epoch 446, Loss: 0.12635621428489685\n",
      "Epoch 447, Loss: 0.10860704630613327\n",
      "Epoch 448, Loss: 0.14025957882404327\n",
      "Epoch 449, Loss: 0.12303034216165543\n",
      "Epoch 450, Loss: 0.11543828248977661\n",
      "Epoch 451, Loss: 0.09965591877698898\n",
      "Epoch 452, Loss: 0.11519782990217209\n",
      "Epoch 453, Loss: 0.11930372565984726\n",
      "Epoch 454, Loss: 0.12684503197669983\n",
      "Epoch 455, Loss: 0.11317002028226852\n",
      "Epoch 456, Loss: 0.14920420944690704\n",
      "Epoch 457, Loss: 0.11779144406318665\n",
      "Epoch 458, Loss: 0.1438112109899521\n",
      "Epoch 459, Loss: 0.1145818829536438\n",
      "Epoch 460, Loss: 0.11468618363142014\n",
      "Epoch 461, Loss: 0.11532067507505417\n",
      "Epoch 462, Loss: 0.12231098115444183\n",
      "Epoch 463, Loss: 0.10806997865438461\n",
      "Epoch 464, Loss: 0.11281691491603851\n",
      "Epoch 465, Loss: 0.12366563826799393\n",
      "Epoch 466, Loss: 0.12520822882652283\n",
      "Epoch 467, Loss: 0.08973157405853271\n",
      "Epoch 468, Loss: 0.10961748659610748\n",
      "Epoch 469, Loss: 0.1330718845129013\n",
      "Epoch 470, Loss: 0.1311168670654297\n",
      "Epoch 471, Loss: 0.12322333455085754\n",
      "Epoch 472, Loss: 0.13026432693004608\n",
      "Epoch 473, Loss: 0.1106996089220047\n",
      "Epoch 474, Loss: 0.11548172682523727\n",
      "Epoch 475, Loss: 0.125766322016716\n",
      "Epoch 476, Loss: 0.11880173534154892\n",
      "Epoch 477, Loss: 0.11819414794445038\n",
      "Epoch 478, Loss: 0.10711793601512909\n",
      "Epoch 479, Loss: 0.0963512733578682\n",
      "Epoch 480, Loss: 0.11760451644659042\n",
      "Epoch 481, Loss: 0.09845549613237381\n",
      "Epoch 482, Loss: 0.11840669065713882\n",
      "Epoch 483, Loss: 0.0846722275018692\n",
      "Epoch 484, Loss: 0.10330641269683838\n",
      "Epoch 485, Loss: 0.09240388125181198\n",
      "Epoch 486, Loss: 0.11275075376033783\n",
      "Epoch 487, Loss: 0.09451320022344589\n",
      "Epoch 488, Loss: 0.10619759559631348\n",
      "Epoch 489, Loss: 0.09612899273633957\n",
      "Epoch 490, Loss: 0.12515805661678314\n",
      "Epoch 491, Loss: 0.10040834546089172\n",
      "Epoch 492, Loss: 0.10915118455886841\n",
      "Epoch 493, Loss: 0.11269703507423401\n",
      "Epoch 494, Loss: 0.1085357815027237\n",
      "Epoch 495, Loss: 0.11444665491580963\n",
      "Epoch 496, Loss: 0.09942495077848434\n",
      "Epoch 497, Loss: 0.10019378364086151\n",
      "Epoch 498, Loss: 0.08704844117164612\n",
      "Epoch 499, Loss: 0.1231784075498581\n",
      "Epoch 500, Loss: 0.10288501530885696\n",
      "Epoch 501, Loss: 0.1183522567152977\n",
      "Epoch 502, Loss: 0.09687180817127228\n",
      "Epoch 503, Loss: 0.10457919538021088\n",
      "Epoch 504, Loss: 0.12234620749950409\n",
      "Epoch 505, Loss: 0.11011074483394623\n",
      "Epoch 506, Loss: 0.10109766572713852\n",
      "Epoch 507, Loss: 0.09316328167915344\n",
      "Epoch 508, Loss: 0.10513638705015182\n",
      "Epoch 509, Loss: 0.09457693994045258\n",
      "Epoch 510, Loss: 0.09937107563018799\n",
      "Epoch 511, Loss: 0.09901891648769379\n",
      "Epoch 512, Loss: 0.10652569681406021\n",
      "Epoch 513, Loss: 0.09507844597101212\n",
      "Epoch 514, Loss: 0.09559734165668488\n",
      "Epoch 515, Loss: 0.07968994230031967\n",
      "Epoch 516, Loss: 0.08414953202009201\n",
      "Epoch 517, Loss: 0.12846246361732483\n",
      "Epoch 518, Loss: 0.09924225509166718\n",
      "Epoch 519, Loss: 0.08383190631866455\n",
      "Epoch 520, Loss: 0.08656762540340424\n",
      "Epoch 521, Loss: 0.11063208431005478\n",
      "Epoch 522, Loss: 0.10955165326595306\n",
      "Epoch 523, Loss: 0.08841601759195328\n",
      "Epoch 524, Loss: 0.09568939357995987\n",
      "Epoch 525, Loss: 0.09574717283248901\n",
      "Epoch 526, Loss: 0.0955105870962143\n",
      "Epoch 527, Loss: 0.09595748782157898\n",
      "Epoch 528, Loss: 0.09481312334537506\n",
      "Epoch 529, Loss: 0.09480102360248566\n",
      "Epoch 530, Loss: 0.1026497334241867\n",
      "Epoch 531, Loss: 0.08376523107290268\n",
      "Epoch 532, Loss: 0.09866894781589508\n",
      "Epoch 533, Loss: 0.10700805485248566\n",
      "Epoch 534, Loss: 0.07665035873651505\n",
      "Epoch 535, Loss: 0.08234331011772156\n",
      "Epoch 536, Loss: 0.11522888392210007\n",
      "Epoch 537, Loss: 0.09760357439517975\n",
      "Epoch 538, Loss: 0.09049806743860245\n",
      "Epoch 539, Loss: 0.10238958895206451\n",
      "Epoch 540, Loss: 0.07890446484088898\n",
      "Epoch 541, Loss: 0.08428215235471725\n",
      "Epoch 542, Loss: 0.10824697464704514\n",
      "Epoch 543, Loss: 0.07860948890447617\n",
      "Epoch 544, Loss: 0.07633005827665329\n",
      "Epoch 545, Loss: 0.07278613746166229\n",
      "Epoch 546, Loss: 0.0804421454668045\n",
      "Epoch 547, Loss: 0.08346471935510635\n",
      "Epoch 548, Loss: 0.09723269194364548\n",
      "Epoch 549, Loss: 0.10354553163051605\n",
      "Epoch 550, Loss: 0.08857041597366333\n",
      "Epoch 551, Loss: 0.08324132114648819\n",
      "Epoch 552, Loss: 0.10863923281431198\n",
      "Epoch 553, Loss: 0.0731293112039566\n",
      "Epoch 554, Loss: 0.08708959072828293\n",
      "Epoch 555, Loss: 0.07821472734212875\n",
      "Epoch 556, Loss: 0.09804096817970276\n",
      "Epoch 557, Loss: 0.09742479026317596\n",
      "Epoch 558, Loss: 0.1038234606385231\n",
      "Epoch 559, Loss: 0.09682206064462662\n",
      "Epoch 560, Loss: 0.06827745586633682\n",
      "Epoch 561, Loss: 0.0723467618227005\n",
      "Epoch 562, Loss: 0.08231037855148315\n",
      "Epoch 563, Loss: 0.09138842672109604\n",
      "Epoch 564, Loss: 0.09747617691755295\n",
      "Epoch 565, Loss: 0.09318133443593979\n",
      "Epoch 566, Loss: 0.07984039187431335\n",
      "Epoch 567, Loss: 0.08733594417572021\n",
      "Epoch 568, Loss: 0.09062386304140091\n",
      "Epoch 569, Loss: 0.06976865977048874\n",
      "Epoch 570, Loss: 0.06964029371738434\n",
      "Epoch 571, Loss: 0.09177583456039429\n",
      "Epoch 572, Loss: 0.09435603022575378\n",
      "Epoch 573, Loss: 0.08971968293190002\n",
      "Epoch 574, Loss: 0.10356183350086212\n",
      "Epoch 575, Loss: 0.09791915118694305\n",
      "Epoch 576, Loss: 0.06242550536990166\n",
      "Epoch 577, Loss: 0.09834878891706467\n",
      "Epoch 578, Loss: 0.10826116800308228\n",
      "Epoch 579, Loss: 0.08089738339185715\n",
      "Epoch 580, Loss: 0.07925883680582047\n",
      "Epoch 581, Loss: 0.08541512489318848\n",
      "Epoch 582, Loss: 0.0878058522939682\n",
      "Epoch 583, Loss: 0.09820044785737991\n",
      "Epoch 584, Loss: 0.07166249305009842\n",
      "Epoch 585, Loss: 0.10495585203170776\n",
      "Epoch 586, Loss: 0.08958245813846588\n",
      "Epoch 587, Loss: 0.09925778210163116\n",
      "Epoch 588, Loss: 0.10985782742500305\n",
      "Epoch 589, Loss: 0.07977811247110367\n",
      "Epoch 590, Loss: 0.07612188160419464\n",
      "Epoch 591, Loss: 0.06651020795106888\n",
      "Epoch 592, Loss: 0.09074041992425919\n",
      "Epoch 593, Loss: 0.0973917543888092\n",
      "Epoch 594, Loss: 0.07202589511871338\n",
      "Epoch 595, Loss: 0.06600116938352585\n",
      "Epoch 596, Loss: 0.06710115820169449\n",
      "Epoch 597, Loss: 0.08695811778306961\n",
      "Epoch 598, Loss: 0.08653999119997025\n",
      "Epoch 599, Loss: 0.07860221713781357\n",
      "Epoch 600, Loss: 0.08217282593250275\n",
      "Epoch 601, Loss: 0.07429461181163788\n",
      "Epoch 602, Loss: 0.06369772553443909\n",
      "Epoch 603, Loss: 0.0563502199947834\n",
      "Epoch 604, Loss: 0.07616190612316132\n",
      "Epoch 605, Loss: 0.08721484988927841\n",
      "Epoch 606, Loss: 0.08359751105308533\n",
      "Epoch 607, Loss: 0.07828900963068008\n",
      "Epoch 608, Loss: 0.11170240491628647\n",
      "Epoch 609, Loss: 0.07886872440576553\n",
      "Epoch 610, Loss: 0.059384047985076904\n",
      "Epoch 611, Loss: 0.055388495326042175\n",
      "Epoch 612, Loss: 0.07869535684585571\n",
      "Epoch 613, Loss: 0.05924219265580177\n",
      "Epoch 614, Loss: 0.059344835579395294\n",
      "Epoch 615, Loss: 0.08032159507274628\n",
      "Epoch 616, Loss: 0.07514425367116928\n",
      "Epoch 617, Loss: 0.07161256670951843\n",
      "Epoch 618, Loss: 0.06574651598930359\n",
      "Epoch 619, Loss: 0.07133307307958603\n",
      "Epoch 620, Loss: 0.07631774991750717\n",
      "Epoch 621, Loss: 0.07981696724891663\n",
      "Epoch 622, Loss: 0.07723356038331985\n",
      "Epoch 623, Loss: 0.05403077229857445\n",
      "Epoch 624, Loss: 0.093671053647995\n",
      "Epoch 625, Loss: 0.051315788179636\n",
      "Epoch 626, Loss: 0.04966811463236809\n",
      "Epoch 627, Loss: 0.06723333895206451\n",
      "Epoch 628, Loss: 0.08716315031051636\n",
      "Epoch 629, Loss: 0.06903483718633652\n",
      "Epoch 630, Loss: 0.07082658261060715\n",
      "Epoch 631, Loss: 0.062269002199172974\n",
      "Epoch 632, Loss: 0.0616629458963871\n",
      "Epoch 633, Loss: 0.0824732780456543\n",
      "Epoch 634, Loss: 0.08557766675949097\n",
      "Epoch 635, Loss: 0.06709503382444382\n",
      "Epoch 636, Loss: 0.06477667391300201\n",
      "Epoch 637, Loss: 0.07741273939609528\n",
      "Epoch 638, Loss: 0.06995006650686264\n",
      "Epoch 639, Loss: 0.07606564462184906\n",
      "Epoch 640, Loss: 0.08665227144956589\n",
      "Epoch 641, Loss: 0.08968991041183472\n",
      "Epoch 642, Loss: 0.07249120622873306\n",
      "Epoch 643, Loss: 0.07209575176239014\n",
      "Epoch 644, Loss: 0.06442727893590927\n",
      "Epoch 645, Loss: 0.06837254762649536\n",
      "Epoch 646, Loss: 0.062159884721040726\n",
      "Epoch 647, Loss: 0.05511923134326935\n",
      "Epoch 648, Loss: 0.0718134418129921\n",
      "Epoch 649, Loss: 0.0617142990231514\n",
      "Epoch 650, Loss: 0.06672339141368866\n",
      "Epoch 651, Loss: 0.08355619758367538\n",
      "Epoch 652, Loss: 0.05568685755133629\n",
      "Epoch 653, Loss: 0.06309954822063446\n",
      "Epoch 654, Loss: 0.06184924766421318\n",
      "Epoch 655, Loss: 0.06482518464326859\n",
      "Epoch 656, Loss: 0.06052999943494797\n",
      "Epoch 657, Loss: 0.05008574575185776\n",
      "Epoch 658, Loss: 0.04459737613797188\n",
      "Epoch 659, Loss: 0.048728495836257935\n",
      "Epoch 660, Loss: 0.05910184234380722\n",
      "Epoch 661, Loss: 0.07024171948432922\n",
      "Epoch 662, Loss: 0.06796635687351227\n",
      "Epoch 663, Loss: 0.05076899752020836\n",
      "Epoch 664, Loss: 0.065503790974617\n",
      "Epoch 665, Loss: 0.07799963653087616\n",
      "Epoch 666, Loss: 0.07346566021442413\n",
      "Epoch 667, Loss: 0.06789162755012512\n",
      "Epoch 668, Loss: 0.057897113263607025\n",
      "Epoch 669, Loss: 0.05440673977136612\n",
      "Epoch 670, Loss: 0.044888321310281754\n",
      "Epoch 671, Loss: 0.05777376517653465\n",
      "Epoch 672, Loss: 0.04953206703066826\n",
      "Epoch 673, Loss: 0.054681189358234406\n",
      "Epoch 674, Loss: 0.0632353350520134\n",
      "Epoch 675, Loss: 0.08059940487146378\n",
      "Epoch 676, Loss: 0.06713516265153885\n",
      "Epoch 677, Loss: 0.04960382729768753\n",
      "Epoch 678, Loss: 0.05905415862798691\n",
      "Epoch 679, Loss: 0.07525795698165894\n",
      "Epoch 680, Loss: 0.05123699828982353\n",
      "Epoch 681, Loss: 0.0607432946562767\n",
      "Epoch 682, Loss: 0.058032043278217316\n",
      "Epoch 683, Loss: 0.05522719770669937\n",
      "Epoch 684, Loss: 0.06905917078256607\n",
      "Epoch 685, Loss: 0.06114340201020241\n",
      "Epoch 686, Loss: 0.06031043455004692\n",
      "Epoch 687, Loss: 0.05138092115521431\n",
      "Epoch 688, Loss: 0.05790767818689346\n",
      "Epoch 689, Loss: 0.058097921311855316\n",
      "Epoch 690, Loss: 0.050959356129169464\n",
      "Epoch 691, Loss: 0.05184924975037575\n",
      "Epoch 692, Loss: 0.0426928848028183\n",
      "Epoch 693, Loss: 0.041444696485996246\n",
      "Epoch 694, Loss: 0.051188793033361435\n",
      "Epoch 695, Loss: 0.07625171542167664\n",
      "Epoch 696, Loss: 0.06172451749444008\n",
      "Epoch 697, Loss: 0.05092547461390495\n",
      "Epoch 698, Loss: 0.04477129876613617\n",
      "Epoch 699, Loss: 0.0675598680973053\n",
      "Epoch 700, Loss: 0.06002608686685562\n",
      "Epoch 701, Loss: 0.06837862730026245\n",
      "Epoch 702, Loss: 0.04918840900063515\n",
      "Epoch 703, Loss: 0.05336400121450424\n",
      "Epoch 704, Loss: 0.0711120069026947\n",
      "Epoch 705, Loss: 0.07355070114135742\n",
      "Epoch 706, Loss: 0.05087791755795479\n",
      "Epoch 707, Loss: 0.05271070823073387\n",
      "Epoch 708, Loss: 0.06636045128107071\n",
      "Epoch 709, Loss: 0.05964236333966255\n",
      "Epoch 710, Loss: 0.047450531274080276\n",
      "Epoch 711, Loss: 0.05107159540057182\n",
      "Epoch 712, Loss: 0.05100559443235397\n",
      "Epoch 713, Loss: 0.050782203674316406\n",
      "Epoch 714, Loss: 0.05341527983546257\n",
      "Epoch 715, Loss: 0.05226943641901016\n",
      "Epoch 716, Loss: 0.062139853835105896\n",
      "Epoch 717, Loss: 0.05397002026438713\n",
      "Epoch 718, Loss: 0.08234745264053345\n",
      "Epoch 719, Loss: 0.05910612642765045\n",
      "Epoch 720, Loss: 0.06424231082201004\n",
      "Epoch 721, Loss: 0.05372846871614456\n",
      "Epoch 722, Loss: 0.04268293082714081\n",
      "Epoch 723, Loss: 0.060269054025411606\n",
      "Epoch 724, Loss: 0.0539352148771286\n",
      "Epoch 725, Loss: 0.05082258954644203\n",
      "Epoch 726, Loss: 0.04291215538978577\n",
      "Epoch 727, Loss: 0.07504401355981827\n",
      "Epoch 728, Loss: 0.05998268350958824\n",
      "Epoch 729, Loss: 0.04098181053996086\n",
      "Epoch 730, Loss: 0.046521060168743134\n",
      "Epoch 731, Loss: 0.049650028347969055\n",
      "Epoch 732, Loss: 0.05103776231408119\n",
      "Epoch 733, Loss: 0.05315886810421944\n",
      "Epoch 734, Loss: 0.036689579486846924\n",
      "Epoch 735, Loss: 0.06806711852550507\n",
      "Epoch 736, Loss: 0.062495239078998566\n",
      "Epoch 737, Loss: 0.05450714752078056\n",
      "Epoch 738, Loss: 0.06681341677904129\n",
      "Epoch 739, Loss: 0.047698013484478\n",
      "Epoch 740, Loss: 0.04407936707139015\n",
      "Epoch 741, Loss: 0.06438750773668289\n",
      "Epoch 742, Loss: 0.056354887783527374\n",
      "Epoch 743, Loss: 0.058924928307533264\n",
      "Epoch 744, Loss: 0.06353271752595901\n",
      "Epoch 745, Loss: 0.06524233520030975\n",
      "Epoch 746, Loss: 0.060112472623586655\n",
      "Epoch 747, Loss: 0.06000758707523346\n",
      "Epoch 748, Loss: 0.05958687886595726\n",
      "Epoch 749, Loss: 0.0428706556558609\n",
      "Epoch 750, Loss: 0.041573986411094666\n",
      "Epoch 751, Loss: 0.05773867666721344\n",
      "Epoch 752, Loss: 0.0703490599989891\n",
      "Epoch 753, Loss: 0.04299052432179451\n",
      "Epoch 754, Loss: 0.0546264573931694\n",
      "Epoch 755, Loss: 0.0421394519507885\n",
      "Epoch 756, Loss: 0.051451146602630615\n",
      "Epoch 757, Loss: 0.06349612772464752\n",
      "Epoch 758, Loss: 0.03520609065890312\n",
      "Epoch 759, Loss: 0.04205600917339325\n",
      "Epoch 760, Loss: 0.05500524863600731\n",
      "Epoch 761, Loss: 0.05341801419854164\n",
      "Epoch 762, Loss: 0.08161906898021698\n",
      "Epoch 763, Loss: 0.06390267610549927\n",
      "Epoch 764, Loss: 0.03998088464140892\n",
      "Epoch 765, Loss: 0.05521870404481888\n",
      "Epoch 766, Loss: 0.057342592626810074\n",
      "Epoch 767, Loss: 0.04416291415691376\n",
      "Epoch 768, Loss: 0.032144173979759216\n",
      "Epoch 769, Loss: 0.055459149181842804\n",
      "Epoch 770, Loss: 0.06094120815396309\n",
      "Epoch 771, Loss: 0.042244259268045425\n",
      "Epoch 772, Loss: 0.05134173110127449\n",
      "Epoch 773, Loss: 0.03995797038078308\n",
      "Epoch 774, Loss: 0.03630536422133446\n",
      "Epoch 775, Loss: 0.05464740842580795\n",
      "Epoch 776, Loss: 0.03549728915095329\n",
      "Epoch 777, Loss: 0.05121490731835365\n",
      "Epoch 778, Loss: 0.03709399700164795\n",
      "Epoch 779, Loss: 0.04623148590326309\n",
      "Epoch 780, Loss: 0.06388784945011139\n",
      "Epoch 781, Loss: 0.051176849752664566\n",
      "Epoch 782, Loss: 0.04720594733953476\n",
      "Epoch 783, Loss: 0.06630133092403412\n",
      "Epoch 784, Loss: 0.05853930860757828\n",
      "Epoch 785, Loss: 0.0389125756919384\n",
      "Epoch 786, Loss: 0.04391070455312729\n",
      "Epoch 787, Loss: 0.05705553665757179\n",
      "Epoch 788, Loss: 0.0670786052942276\n",
      "Epoch 789, Loss: 0.04205069690942764\n",
      "Epoch 790, Loss: 0.044327955693006516\n",
      "Epoch 791, Loss: 0.04553939029574394\n",
      "Epoch 792, Loss: 0.03853194788098335\n",
      "Epoch 793, Loss: 0.034762851893901825\n",
      "Epoch 794, Loss: 0.045127708464860916\n",
      "Epoch 795, Loss: 0.042116209864616394\n",
      "Epoch 796, Loss: 0.04483561962842941\n",
      "Epoch 797, Loss: 0.037061259150505066\n",
      "Epoch 798, Loss: 0.04138439521193504\n",
      "Epoch 799, Loss: 0.03451798856258392\n",
      "Epoch 800, Loss: 0.0547681525349617\n",
      "Epoch 801, Loss: 0.059313736855983734\n",
      "Epoch 802, Loss: 0.048294004052877426\n",
      "Epoch 803, Loss: 0.03391703963279724\n",
      "Epoch 804, Loss: 0.04491126909852028\n",
      "Epoch 805, Loss: 0.04298647865653038\n",
      "Epoch 806, Loss: 0.058953993022441864\n",
      "Epoch 807, Loss: 0.03215537592768669\n",
      "Epoch 808, Loss: 0.04866410419344902\n",
      "Epoch 809, Loss: 0.047904565930366516\n",
      "Epoch 810, Loss: 0.04212309792637825\n",
      "Epoch 811, Loss: 0.03514700382947922\n",
      "Epoch 812, Loss: 0.052323196083307266\n",
      "Epoch 813, Loss: 0.08455600589513779\n",
      "Epoch 814, Loss: 0.04586745426058769\n",
      "Epoch 815, Loss: 0.05609435960650444\n",
      "Epoch 816, Loss: 0.034574974328279495\n",
      "Epoch 817, Loss: 0.03128141909837723\n",
      "Epoch 818, Loss: 0.05792895704507828\n",
      "Epoch 819, Loss: 0.042072683572769165\n",
      "Epoch 820, Loss: 0.0426468662917614\n",
      "Epoch 821, Loss: 0.0314931757748127\n",
      "Epoch 822, Loss: 0.049744635820388794\n",
      "Epoch 823, Loss: 0.03952238708734512\n",
      "Epoch 824, Loss: 0.03658227249979973\n",
      "Epoch 825, Loss: 0.05506430193781853\n",
      "Epoch 826, Loss: 0.04046525061130524\n",
      "Epoch 827, Loss: 0.04093607887625694\n",
      "Epoch 828, Loss: 0.033012505620718\n",
      "Epoch 829, Loss: 0.050329819321632385\n",
      "Epoch 830, Loss: 0.03376227244734764\n",
      "Epoch 831, Loss: 0.04494781792163849\n",
      "Epoch 832, Loss: 0.037904102355241776\n",
      "Epoch 833, Loss: 0.034701380878686905\n",
      "Epoch 834, Loss: 0.046126581728458405\n",
      "Epoch 835, Loss: 0.04649922996759415\n",
      "Epoch 836, Loss: 0.04186658188700676\n",
      "Epoch 837, Loss: 0.040026482194662094\n",
      "Epoch 838, Loss: 0.028273683041334152\n",
      "Epoch 839, Loss: 0.03756120800971985\n",
      "Epoch 840, Loss: 0.04163915663957596\n",
      "Epoch 841, Loss: 0.0341491661965847\n",
      "Epoch 842, Loss: 0.045289285480976105\n",
      "Epoch 843, Loss: 0.03360249847173691\n",
      "Epoch 844, Loss: 0.04066663235425949\n",
      "Epoch 845, Loss: 0.032496608793735504\n",
      "Epoch 846, Loss: 0.04979807883501053\n",
      "Epoch 847, Loss: 0.04434916377067566\n",
      "Epoch 848, Loss: 0.03804166615009308\n",
      "Epoch 849, Loss: 0.05648276209831238\n",
      "Epoch 850, Loss: 0.042687613517045975\n",
      "Epoch 851, Loss: 0.04739823564887047\n",
      "Epoch 852, Loss: 0.03873727098107338\n",
      "Epoch 853, Loss: 0.045568712055683136\n",
      "Epoch 854, Loss: 0.03721055015921593\n",
      "Epoch 855, Loss: 0.05865496024489403\n",
      "Epoch 856, Loss: 0.03206569328904152\n",
      "Epoch 857, Loss: 0.04846171662211418\n",
      "Epoch 858, Loss: 0.030399922281503677\n",
      "Epoch 859, Loss: 0.04364301636815071\n",
      "Epoch 860, Loss: 0.03904607146978378\n",
      "Epoch 861, Loss: 0.03892465680837631\n",
      "Epoch 862, Loss: 0.04324227198958397\n",
      "Epoch 863, Loss: 0.043495479971170425\n",
      "Epoch 864, Loss: 0.039988547563552856\n",
      "Epoch 865, Loss: 0.039632879197597504\n",
      "Epoch 866, Loss: 0.049759965389966965\n",
      "Epoch 867, Loss: 0.039087023586034775\n",
      "Epoch 868, Loss: 0.03901422396302223\n",
      "Epoch 869, Loss: 0.05054955184459686\n",
      "Epoch 870, Loss: 0.03149468079209328\n",
      "Epoch 871, Loss: 0.033422138541936874\n",
      "Epoch 872, Loss: 0.03542827069759369\n",
      "Epoch 873, Loss: 0.02989119105041027\n",
      "Epoch 874, Loss: 0.06182348355650902\n",
      "Epoch 875, Loss: 0.040425289422273636\n",
      "Epoch 876, Loss: 0.02736351266503334\n",
      "Epoch 877, Loss: 0.029622213914990425\n",
      "Epoch 878, Loss: 0.04411238431930542\n",
      "Epoch 879, Loss: 0.045795079320669174\n",
      "Epoch 880, Loss: 0.052847810089588165\n",
      "Epoch 881, Loss: 0.03248828649520874\n",
      "Epoch 882, Loss: 0.040323227643966675\n",
      "Epoch 883, Loss: 0.029504813253879547\n",
      "Epoch 884, Loss: 0.03765471279621124\n",
      "Epoch 885, Loss: 0.04949305206537247\n",
      "Epoch 886, Loss: 0.033797524869441986\n",
      "Epoch 887, Loss: 0.04752398654818535\n",
      "Epoch 888, Loss: 0.029445508494973183\n",
      "Epoch 889, Loss: 0.05632475018501282\n",
      "Epoch 890, Loss: 0.03878016769886017\n",
      "Epoch 891, Loss: 0.03248387575149536\n",
      "Epoch 892, Loss: 0.031347062438726425\n",
      "Epoch 893, Loss: 0.04931103065609932\n",
      "Epoch 894, Loss: 0.041457850486040115\n",
      "Epoch 895, Loss: 0.050628501921892166\n",
      "Epoch 896, Loss: 0.0365498922765255\n",
      "Epoch 897, Loss: 0.037414632737636566\n",
      "Epoch 898, Loss: 0.04133186489343643\n",
      "Epoch 899, Loss: 0.04126155748963356\n",
      "Epoch 900, Loss: 0.032645899802446365\n",
      "Epoch 901, Loss: 0.029721524566411972\n",
      "Epoch 902, Loss: 0.048674795776605606\n",
      "Epoch 903, Loss: 0.030626364052295685\n",
      "Epoch 904, Loss: 0.02855997532606125\n",
      "Epoch 905, Loss: 0.036431603133678436\n",
      "Epoch 906, Loss: 0.029514631256461143\n",
      "Epoch 907, Loss: 0.04895112290978432\n",
      "Epoch 908, Loss: 0.03414612263441086\n",
      "Epoch 909, Loss: 0.025279894471168518\n",
      "Epoch 910, Loss: 0.03435608744621277\n",
      "Epoch 911, Loss: 0.032715920358896255\n",
      "Epoch 912, Loss: 0.046189289540052414\n",
      "Epoch 913, Loss: 0.027872612699866295\n",
      "Epoch 914, Loss: 0.034619539976119995\n",
      "Epoch 915, Loss: 0.02554330602288246\n",
      "Epoch 916, Loss: 0.05341920256614685\n",
      "Epoch 917, Loss: 0.031200191006064415\n",
      "Epoch 918, Loss: 0.04448186978697777\n",
      "Epoch 919, Loss: 0.03555610403418541\n",
      "Epoch 920, Loss: 0.047282785177230835\n",
      "Epoch 921, Loss: 0.02638464793562889\n",
      "Epoch 922, Loss: 0.04432602971792221\n",
      "Epoch 923, Loss: 0.052236296236515045\n",
      "Epoch 924, Loss: 0.033510446548461914\n",
      "Epoch 925, Loss: 0.0429602675139904\n",
      "Epoch 926, Loss: 0.042686570435762405\n",
      "Epoch 927, Loss: 0.035860348492860794\n",
      "Epoch 928, Loss: 0.04072953388094902\n",
      "Epoch 929, Loss: 0.045033492147922516\n",
      "Epoch 930, Loss: 0.0384775772690773\n",
      "Epoch 931, Loss: 0.05258529260754585\n",
      "Epoch 932, Loss: 0.028050508350133896\n",
      "Epoch 933, Loss: 0.03732125833630562\n",
      "Epoch 934, Loss: 0.03126334026455879\n",
      "Epoch 935, Loss: 0.043400608003139496\n",
      "Epoch 936, Loss: 0.03572307527065277\n",
      "Epoch 937, Loss: 0.03160633519291878\n",
      "Epoch 938, Loss: 0.041676975786685944\n",
      "Epoch 939, Loss: 0.031720586121082306\n",
      "Epoch 940, Loss: 0.02687836065888405\n",
      "Epoch 941, Loss: 0.04670879989862442\n",
      "Epoch 942, Loss: 0.026410238817334175\n",
      "Epoch 943, Loss: 0.030981846153736115\n",
      "Epoch 944, Loss: 0.046402279287576675\n",
      "Epoch 945, Loss: 0.028927870094776154\n",
      "Epoch 946, Loss: 0.034773945808410645\n",
      "Epoch 947, Loss: 0.02428986318409443\n",
      "Epoch 948, Loss: 0.03615035489201546\n",
      "Epoch 949, Loss: 0.03613683953881264\n",
      "Epoch 950, Loss: 0.05107009783387184\n",
      "Epoch 951, Loss: 0.042146071791648865\n",
      "Epoch 952, Loss: 0.02970093861222267\n",
      "Epoch 953, Loss: 0.04026007279753685\n",
      "Epoch 954, Loss: 0.03607714921236038\n",
      "Epoch 955, Loss: 0.046091776341199875\n",
      "Epoch 956, Loss: 0.033358391374349594\n",
      "Epoch 957, Loss: 0.027606604620814323\n",
      "Epoch 958, Loss: 0.027182746678590775\n",
      "Epoch 959, Loss: 0.027097344398498535\n",
      "Epoch 960, Loss: 0.03149367496371269\n",
      "Epoch 961, Loss: 0.03461563214659691\n",
      "Epoch 962, Loss: 0.03403201699256897\n",
      "Epoch 963, Loss: 0.02463074028491974\n",
      "Epoch 964, Loss: 0.04086366295814514\n",
      "Epoch 965, Loss: 0.024238571524620056\n",
      "Epoch 966, Loss: 0.02857985720038414\n",
      "Epoch 967, Loss: 0.03776079788804054\n",
      "Epoch 968, Loss: 0.032005518674850464\n",
      "Epoch 969, Loss: 0.03073933906853199\n",
      "Epoch 970, Loss: 0.030575307086110115\n",
      "Epoch 971, Loss: 0.03229483217000961\n",
      "Epoch 972, Loss: 0.03341982886195183\n",
      "Epoch 973, Loss: 0.03638765960931778\n",
      "Epoch 974, Loss: 0.03137781843543053\n",
      "Epoch 975, Loss: 0.03887015953660011\n",
      "Epoch 976, Loss: 0.028669925406575203\n",
      "Epoch 977, Loss: 0.033639997243881226\n",
      "Epoch 978, Loss: 0.029422547668218613\n",
      "Epoch 979, Loss: 0.0368419773876667\n",
      "Epoch 980, Loss: 0.040340352803468704\n",
      "Epoch 981, Loss: 0.025355074554681778\n",
      "Epoch 982, Loss: 0.03378910571336746\n",
      "Epoch 983, Loss: 0.025751182809472084\n",
      "Epoch 984, Loss: 0.0346255898475647\n",
      "Epoch 985, Loss: 0.04382087290287018\n",
      "Epoch 986, Loss: 0.03586190566420555\n",
      "Epoch 987, Loss: 0.0363534539937973\n",
      "Epoch 988, Loss: 0.03287225216627121\n",
      "Epoch 989, Loss: 0.04295290634036064\n",
      "Epoch 990, Loss: 0.036355070769786835\n",
      "Epoch 991, Loss: 0.02503664791584015\n",
      "Epoch 992, Loss: 0.0314955934882164\n",
      "Epoch 993, Loss: 0.045294176787137985\n",
      "Epoch 994, Loss: 0.02587125077843666\n",
      "Epoch 995, Loss: 0.031552545726299286\n",
      "Epoch 996, Loss: 0.027682112529873848\n",
      "Epoch 997, Loss: 0.036882948130369186\n",
      "Epoch 998, Loss: 0.029762594029307365\n",
      "Epoch 999, Loss: 0.030905991792678833\n",
      "Epoch 1000, Loss: 0.023693639785051346\n",
      "Epoch 1001, Loss: 0.024228841066360474\n",
      "Epoch 1002, Loss: 0.030186761170625687\n",
      "Epoch 1003, Loss: 0.037717338651418686\n",
      "Epoch 1004, Loss: 0.03764171525835991\n",
      "Epoch 1005, Loss: 0.023468073457479477\n",
      "Epoch 1006, Loss: 0.030162375420331955\n",
      "Epoch 1007, Loss: 0.03865636885166168\n",
      "Epoch 1008, Loss: 0.023746836930513382\n",
      "Epoch 1009, Loss: 0.03336125984787941\n",
      "Epoch 1010, Loss: 0.06938475370407104\n",
      "Epoch 1011, Loss: 0.027427837252616882\n",
      "Epoch 1012, Loss: 0.049626629799604416\n",
      "Epoch 1013, Loss: 0.04551432281732559\n",
      "Epoch 1014, Loss: 0.04324091598391533\n",
      "Epoch 1015, Loss: 0.035953767597675323\n",
      "Epoch 1016, Loss: 0.02568885125219822\n",
      "Epoch 1017, Loss: 0.028846576809883118\n",
      "Epoch 1018, Loss: 0.031374868005514145\n",
      "Epoch 1019, Loss: 0.03490611910820007\n",
      "Epoch 1020, Loss: 0.03923878073692322\n",
      "Epoch 1021, Loss: 0.023299740627408028\n",
      "Epoch 1022, Loss: 0.02872084639966488\n",
      "Epoch 1023, Loss: 0.03564584627747536\n",
      "Epoch 1024, Loss: 0.0226003285497427\n",
      "Epoch 1025, Loss: 0.02346184104681015\n",
      "Epoch 1026, Loss: 0.05660978704690933\n",
      "Epoch 1027, Loss: 0.04033999890089035\n",
      "Epoch 1028, Loss: 0.023284876719117165\n",
      "Epoch 1029, Loss: 0.024684321135282516\n",
      "Epoch 1030, Loss: 0.0349954292178154\n",
      "Epoch 1031, Loss: 0.030231377109885216\n",
      "Epoch 1032, Loss: 0.026192734017968178\n",
      "Epoch 1033, Loss: 0.07004138827323914\n",
      "Epoch 1034, Loss: 0.03278891369700432\n",
      "Epoch 1035, Loss: 0.03438689559698105\n",
      "Epoch 1036, Loss: 0.024389058351516724\n",
      "Epoch 1037, Loss: 0.032657772302627563\n",
      "Epoch 1038, Loss: 0.025002488866448402\n",
      "Epoch 1039, Loss: 0.03842320665717125\n",
      "Epoch 1040, Loss: 0.053347181528806686\n",
      "Epoch 1041, Loss: 0.026540551334619522\n",
      "Epoch 1042, Loss: 0.02649557963013649\n",
      "Epoch 1043, Loss: 0.02314775437116623\n",
      "Epoch 1044, Loss: 0.022807221859693527\n",
      "Epoch 1045, Loss: 0.03448621928691864\n",
      "Epoch 1046, Loss: 0.0233891773968935\n",
      "Epoch 1047, Loss: 0.036046214401721954\n",
      "Epoch 1048, Loss: 0.030803458765149117\n",
      "Epoch 1049, Loss: 0.021197179332375526\n",
      "Epoch 1050, Loss: 0.021182313561439514\n",
      "Epoch 1051, Loss: 0.04074397683143616\n",
      "Epoch 1052, Loss: 0.02598697692155838\n",
      "Epoch 1053, Loss: 0.026983465999364853\n",
      "Epoch 1054, Loss: 0.04184181988239288\n",
      "Epoch 1055, Loss: 0.04442150890827179\n",
      "Epoch 1056, Loss: 0.03369247540831566\n",
      "Epoch 1057, Loss: 0.04263486713171005\n",
      "Epoch 1058, Loss: 0.04781041666865349\n",
      "Epoch 1059, Loss: 0.05398735776543617\n",
      "Epoch 1060, Loss: 0.03485587611794472\n",
      "Epoch 1061, Loss: 0.03811357170343399\n",
      "Epoch 1062, Loss: 0.036050647497177124\n",
      "Epoch 1063, Loss: 0.02886887826025486\n",
      "Epoch 1064, Loss: 0.023109637200832367\n",
      "Epoch 1065, Loss: 0.029197843745350838\n",
      "Epoch 1066, Loss: 0.025975560769438744\n",
      "Epoch 1067, Loss: 0.029442429542541504\n",
      "Epoch 1068, Loss: 0.027143515646457672\n",
      "Epoch 1069, Loss: 0.044795501977205276\n",
      "Epoch 1070, Loss: 0.02206367440521717\n",
      "Epoch 1071, Loss: 0.03540312498807907\n",
      "Epoch 1072, Loss: 0.025584207847714424\n",
      "Epoch 1073, Loss: 0.03070721961557865\n",
      "Epoch 1074, Loss: 0.025354085490107536\n",
      "Epoch 1075, Loss: 0.025758661329746246\n",
      "Epoch 1076, Loss: 0.02924329787492752\n",
      "Epoch 1077, Loss: 0.024637429043650627\n",
      "Epoch 1078, Loss: 0.02403365448117256\n",
      "Epoch 1079, Loss: 0.023066386580467224\n",
      "Epoch 1080, Loss: 0.021850712597370148\n",
      "Epoch 1081, Loss: 0.0225356612354517\n",
      "Epoch 1082, Loss: 0.022826623171567917\n",
      "Epoch 1083, Loss: 0.04106810688972473\n",
      "Epoch 1084, Loss: 0.03017960488796234\n",
      "Epoch 1085, Loss: 0.026903947815299034\n",
      "Epoch 1086, Loss: 0.02402186207473278\n",
      "Epoch 1087, Loss: 0.02420377917587757\n",
      "Epoch 1088, Loss: 0.03224743902683258\n",
      "Epoch 1089, Loss: 0.05330086499452591\n",
      "Epoch 1090, Loss: 0.02474493719637394\n",
      "Epoch 1091, Loss: 0.01792171224951744\n",
      "Epoch 1092, Loss: 0.03671886399388313\n",
      "Epoch 1093, Loss: 0.024510959163308144\n",
      "Epoch 1094, Loss: 0.022372955456376076\n",
      "Epoch 1095, Loss: 0.03553270548582077\n",
      "Epoch 1096, Loss: 0.02842739224433899\n",
      "Epoch 1097, Loss: 0.02814006246626377\n",
      "Epoch 1098, Loss: 0.026463661342859268\n",
      "Epoch 1099, Loss: 0.022323234006762505\n",
      "Epoch 1100, Loss: 0.03882932662963867\n",
      "Epoch 1101, Loss: 0.024288605898618698\n",
      "Epoch 1102, Loss: 0.034771330654621124\n",
      "Epoch 1103, Loss: 0.022402994334697723\n",
      "Epoch 1104, Loss: 0.03748692572116852\n",
      "Epoch 1105, Loss: 0.023786213248968124\n",
      "Epoch 1106, Loss: 0.02595328725874424\n",
      "Epoch 1107, Loss: 0.02563171274960041\n",
      "Epoch 1108, Loss: 0.01920359954237938\n",
      "Epoch 1109, Loss: 0.020703434944152832\n",
      "Epoch 1110, Loss: 0.041475940495729446\n",
      "Epoch 1111, Loss: 0.029291821643710136\n",
      "Epoch 1112, Loss: 0.03223994001746178\n",
      "Epoch 1113, Loss: 0.05023204907774925\n",
      "Epoch 1114, Loss: 0.022073907777667046\n",
      "Epoch 1115, Loss: 0.02101181261241436\n",
      "Epoch 1116, Loss: 0.04208933189511299\n",
      "Epoch 1117, Loss: 0.019416049122810364\n",
      "Epoch 1118, Loss: 0.03404117375612259\n",
      "Epoch 1119, Loss: 0.033855777233839035\n",
      "Epoch 1120, Loss: 0.01858837716281414\n",
      "Epoch 1121, Loss: 0.030073193833231926\n",
      "Epoch 1122, Loss: 0.03299858793616295\n",
      "Epoch 1123, Loss: 0.03122493252158165\n",
      "Epoch 1124, Loss: 0.022212238982319832\n",
      "Epoch 1125, Loss: 0.022778308019042015\n",
      "Epoch 1126, Loss: 0.024442404508590698\n",
      "Epoch 1127, Loss: 0.02010582759976387\n",
      "Epoch 1128, Loss: 0.026786169037222862\n",
      "Epoch 1129, Loss: 0.020207535475492477\n",
      "Epoch 1130, Loss: 0.0199266467243433\n",
      "Epoch 1131, Loss: 0.0230236928910017\n",
      "Epoch 1132, Loss: 0.018710412085056305\n",
      "Epoch 1133, Loss: 0.025329511612653732\n",
      "Epoch 1134, Loss: 0.02128443494439125\n",
      "Epoch 1135, Loss: 0.017909349873661995\n",
      "Epoch 1136, Loss: 0.02136252075433731\n",
      "Epoch 1137, Loss: 0.04233439266681671\n",
      "Epoch 1138, Loss: 0.024037888273596764\n",
      "Epoch 1139, Loss: 0.020266298204660416\n",
      "Epoch 1140, Loss: 0.028998475521802902\n",
      "Epoch 1141, Loss: 0.023273659870028496\n",
      "Epoch 1142, Loss: 0.03446868062019348\n",
      "Epoch 1143, Loss: 0.022917771711945534\n",
      "Epoch 1144, Loss: 0.024540076032280922\n",
      "Epoch 1145, Loss: 0.028933901339769363\n",
      "Epoch 1146, Loss: 0.02607148326933384\n",
      "Epoch 1147, Loss: 0.03988451510667801\n",
      "Epoch 1148, Loss: 0.03808091953396797\n",
      "Epoch 1149, Loss: 0.019059035927057266\n",
      "Epoch 1150, Loss: 0.020461678504943848\n",
      "Epoch 1151, Loss: 0.041930973529815674\n",
      "Epoch 1152, Loss: 0.02459350787103176\n",
      "Epoch 1153, Loss: 0.03330215439200401\n",
      "Epoch 1154, Loss: 0.0229398962110281\n",
      "Epoch 1155, Loss: 0.025064384564757347\n",
      "Epoch 1156, Loss: 0.02067728154361248\n",
      "Epoch 1157, Loss: 0.021789846941828728\n",
      "Epoch 1158, Loss: 0.03334181755781174\n",
      "Epoch 1159, Loss: 0.020699506625533104\n",
      "Epoch 1160, Loss: 0.020746169611811638\n",
      "Epoch 1161, Loss: 0.03211439028382301\n",
      "Epoch 1162, Loss: 0.026644021272659302\n",
      "Epoch 1163, Loss: 0.019511952996253967\n",
      "Epoch 1164, Loss: 0.021414419636130333\n",
      "Epoch 1165, Loss: 0.020990265533328056\n",
      "Epoch 1166, Loss: 0.03580140694975853\n",
      "Epoch 1167, Loss: 0.019960150122642517\n",
      "Epoch 1168, Loss: 0.021728545427322388\n",
      "Epoch 1169, Loss: 0.016888665035367012\n",
      "Epoch 1170, Loss: 0.017214659601449966\n",
      "Epoch 1171, Loss: 0.03227116912603378\n",
      "Epoch 1172, Loss: 0.01760428585112095\n",
      "Epoch 1173, Loss: 0.030707644298672676\n",
      "Epoch 1174, Loss: 0.028086990118026733\n",
      "Epoch 1175, Loss: 0.02332763746380806\n",
      "Epoch 1176, Loss: 0.02522445283830166\n",
      "Epoch 1177, Loss: 0.020415429025888443\n",
      "Epoch 1178, Loss: 0.024608630686998367\n",
      "Epoch 1179, Loss: 0.026766091585159302\n",
      "Epoch 1180, Loss: 0.02552231401205063\n",
      "Epoch 1181, Loss: 0.016517184674739838\n",
      "Epoch 1182, Loss: 0.02713930420577526\n",
      "Epoch 1183, Loss: 0.02655118890106678\n",
      "Epoch 1184, Loss: 0.02168208360671997\n",
      "Epoch 1185, Loss: 0.046751972287893295\n",
      "Epoch 1186, Loss: 0.0209941603243351\n",
      "Epoch 1187, Loss: 0.031488314270973206\n",
      "Epoch 1188, Loss: 0.027170289307832718\n",
      "Epoch 1189, Loss: 0.016884688287973404\n",
      "Epoch 1190, Loss: 0.030477695167064667\n",
      "Epoch 1191, Loss: 0.02792765386402607\n",
      "Epoch 1192, Loss: 0.0165853314101696\n",
      "Epoch 1193, Loss: 0.030500762164592743\n",
      "Epoch 1194, Loss: 0.027054278180003166\n",
      "Epoch 1195, Loss: 0.015893103554844856\n",
      "Epoch 1196, Loss: 0.036649249494075775\n",
      "Epoch 1197, Loss: 0.02232898212969303\n",
      "Epoch 1198, Loss: 0.019349344074726105\n",
      "Epoch 1199, Loss: 0.015430361963808537\n",
      "Epoch 1200, Loss: 0.021375246345996857\n",
      "Epoch 1201, Loss: 0.027673838660120964\n",
      "Epoch 1202, Loss: 0.02916494943201542\n",
      "Epoch 1203, Loss: 0.04092419892549515\n",
      "Epoch 1204, Loss: 0.020801324397325516\n",
      "Epoch 1205, Loss: 0.023836510255932808\n",
      "Epoch 1206, Loss: 0.01812056638300419\n",
      "Epoch 1207, Loss: 0.017562787979841232\n",
      "Epoch 1208, Loss: 0.030145172029733658\n",
      "Epoch 1209, Loss: 0.01734568364918232\n",
      "Epoch 1210, Loss: 0.016382813453674316\n",
      "Epoch 1211, Loss: 0.017466606572270393\n",
      "Epoch 1212, Loss: 0.021488411352038383\n",
      "Epoch 1213, Loss: 0.021231697872281075\n",
      "Epoch 1214, Loss: 0.01824333891272545\n",
      "Epoch 1215, Loss: 0.029504816979169846\n",
      "Epoch 1216, Loss: 0.02246805839240551\n",
      "Epoch 1217, Loss: 0.025377687066793442\n",
      "Epoch 1218, Loss: 0.027313940227031708\n",
      "Epoch 1219, Loss: 0.01728167198598385\n",
      "Epoch 1220, Loss: 0.016327673569321632\n",
      "Epoch 1221, Loss: 0.027979440987110138\n",
      "Epoch 1222, Loss: 0.022551050409674644\n",
      "Epoch 1223, Loss: 0.02459740824997425\n",
      "Epoch 1224, Loss: 0.021053381264209747\n",
      "Epoch 1225, Loss: 0.021238641813397408\n",
      "Epoch 1226, Loss: 0.024345247074961662\n",
      "Epoch 1227, Loss: 0.01818312332034111\n",
      "Epoch 1228, Loss: 0.0263473279774189\n",
      "Epoch 1229, Loss: 0.016049621626734734\n",
      "Epoch 1230, Loss: 0.022760137915611267\n",
      "Epoch 1231, Loss: 0.039871349930763245\n",
      "Epoch 1232, Loss: 0.015485959127545357\n",
      "Epoch 1233, Loss: 0.02000972256064415\n",
      "Epoch 1234, Loss: 0.02755904011428356\n",
      "Epoch 1235, Loss: 0.016124654561281204\n",
      "Epoch 1236, Loss: 0.018795007839798927\n",
      "Epoch 1237, Loss: 0.030989697203040123\n",
      "Epoch 1238, Loss: 0.028029141947627068\n",
      "Epoch 1239, Loss: 0.014307762496173382\n",
      "Epoch 1240, Loss: 0.021608641371130943\n",
      "Epoch 1241, Loss: 0.03055143728852272\n",
      "Epoch 1242, Loss: 0.020058443769812584\n",
      "Epoch 1243, Loss: 0.038107242435216904\n",
      "Epoch 1244, Loss: 0.019987236708402634\n",
      "Epoch 1245, Loss: 0.016616806387901306\n",
      "Epoch 1246, Loss: 0.027168983593583107\n",
      "Epoch 1247, Loss: 0.01953786052763462\n",
      "Epoch 1248, Loss: 0.01847033202648163\n",
      "Epoch 1249, Loss: 0.01662464253604412\n",
      "Epoch 1250, Loss: 0.02395782433450222\n",
      "Epoch 1251, Loss: 0.01875540055334568\n",
      "Epoch 1252, Loss: 0.02001262828707695\n",
      "Epoch 1253, Loss: 0.02126985229551792\n",
      "Epoch 1254, Loss: 0.026016706600785255\n",
      "Epoch 1255, Loss: 0.024612298235297203\n",
      "Epoch 1256, Loss: 0.01783951185643673\n",
      "Epoch 1257, Loss: 0.016458047553896904\n",
      "Epoch 1258, Loss: 0.01699884608387947\n",
      "Epoch 1259, Loss: 0.025838742032647133\n",
      "Epoch 1260, Loss: 0.027498876675963402\n",
      "Epoch 1261, Loss: 0.0187127236276865\n",
      "Epoch 1262, Loss: 0.029255295172333717\n",
      "Epoch 1263, Loss: 0.022485708817839622\n",
      "Epoch 1264, Loss: 0.014483630657196045\n",
      "Epoch 1265, Loss: 0.014714542776346207\n",
      "Epoch 1266, Loss: 0.020910168066620827\n",
      "Epoch 1267, Loss: 0.01706322841346264\n",
      "Epoch 1268, Loss: 0.01943111978471279\n",
      "Epoch 1269, Loss: 0.023745235055685043\n",
      "Epoch 1270, Loss: 0.03316555172204971\n",
      "Epoch 1271, Loss: 0.04531858116388321\n",
      "Epoch 1272, Loss: 0.027057237923145294\n",
      "Epoch 1273, Loss: 0.02236875519156456\n",
      "Epoch 1274, Loss: 0.021006980910897255\n",
      "Epoch 1275, Loss: 0.022695643827319145\n",
      "Epoch 1276, Loss: 0.024188147857785225\n",
      "Epoch 1277, Loss: 0.033943649381399155\n",
      "Epoch 1278, Loss: 0.020988496020436287\n",
      "Epoch 1279, Loss: 0.019634144380688667\n",
      "Epoch 1280, Loss: 0.03344263136386871\n",
      "Epoch 1281, Loss: 0.027053195983171463\n",
      "Epoch 1282, Loss: 0.028597058728337288\n",
      "Epoch 1283, Loss: 0.025034213438630104\n",
      "Epoch 1284, Loss: 0.032167863100767136\n",
      "Epoch 1285, Loss: 0.028656624257564545\n",
      "Epoch 1286, Loss: 0.04947630688548088\n",
      "Epoch 1287, Loss: 0.021045999601483345\n",
      "Epoch 1288, Loss: 0.0212725717574358\n",
      "Epoch 1289, Loss: 0.018296565860509872\n",
      "Epoch 1290, Loss: 0.02907239831984043\n",
      "Epoch 1291, Loss: 0.022654611617326736\n",
      "Epoch 1292, Loss: 0.019084196537733078\n",
      "Epoch 1293, Loss: 0.030689988285303116\n",
      "Epoch 1294, Loss: 0.01612067222595215\n",
      "Epoch 1295, Loss: 0.12964637577533722\n",
      "Epoch 1296, Loss: 0.02515265718102455\n",
      "Epoch 1297, Loss: 0.030993035063147545\n",
      "Epoch 1298, Loss: 0.021920381113886833\n",
      "Epoch 1299, Loss: 0.021237479522824287\n",
      "Epoch 1300, Loss: 0.016583869233727455\n",
      "Epoch 1301, Loss: 0.049606919288635254\n",
      "Epoch 1302, Loss: 0.027049800381064415\n",
      "Epoch 1303, Loss: 0.01912563294172287\n",
      "Epoch 1304, Loss: 0.024597853422164917\n",
      "Epoch 1305, Loss: 0.036594729870557785\n",
      "Epoch 1306, Loss: 0.022798657417297363\n",
      "Epoch 1307, Loss: 0.0247853584587574\n",
      "Epoch 1308, Loss: 0.016775056719779968\n",
      "Epoch 1309, Loss: 0.020558921620249748\n",
      "Epoch 1310, Loss: 0.019430458545684814\n",
      "Epoch 1311, Loss: 0.024326074868440628\n",
      "Epoch 1312, Loss: 0.01587575301527977\n",
      "Epoch 1313, Loss: 0.015792550519108772\n",
      "Epoch 1314, Loss: 0.01756848394870758\n",
      "Epoch 1315, Loss: 0.018376506865024567\n",
      "Epoch 1316, Loss: 0.024671055376529694\n",
      "Epoch 1317, Loss: 0.016795452684164047\n",
      "Epoch 1318, Loss: 0.04904554411768913\n",
      "Epoch 1319, Loss: 0.01921769604086876\n",
      "Epoch 1320, Loss: 0.01645270735025406\n",
      "Epoch 1321, Loss: 0.01856571063399315\n",
      "Epoch 1322, Loss: 0.018895497545599937\n",
      "Epoch 1323, Loss: 0.02238946594297886\n",
      "Epoch 1324, Loss: 0.06838014721870422\n",
      "Epoch 1325, Loss: 0.017119796946644783\n",
      "Epoch 1326, Loss: 0.015058639459311962\n",
      "Epoch 1327, Loss: 0.031235989183187485\n",
      "Epoch 1328, Loss: 0.022978762164711952\n",
      "Epoch 1329, Loss: 0.02169211581349373\n",
      "Epoch 1330, Loss: 0.01908382587134838\n",
      "Epoch 1331, Loss: 0.015213310718536377\n",
      "Epoch 1332, Loss: 0.016289353370666504\n",
      "Epoch 1333, Loss: 0.0167701318860054\n",
      "Epoch 1334, Loss: 0.014878861606121063\n",
      "Epoch 1335, Loss: 0.03171278163790703\n",
      "Epoch 1336, Loss: 0.02081701159477234\n",
      "Epoch 1337, Loss: 0.01571808196604252\n",
      "Epoch 1338, Loss: 0.020385518670082092\n",
      "Epoch 1339, Loss: 0.015844516456127167\n",
      "Epoch 1340, Loss: 0.015700386837124825\n",
      "Epoch 1341, Loss: 0.03131933882832527\n",
      "Epoch 1342, Loss: 0.01967049576342106\n",
      "Epoch 1343, Loss: 0.019250797107815742\n",
      "Epoch 1344, Loss: 0.01732116937637329\n",
      "Epoch 1345, Loss: 0.024510163813829422\n",
      "Epoch 1346, Loss: 0.024730656296014786\n",
      "Epoch 1347, Loss: 0.01474249642342329\n",
      "Epoch 1348, Loss: 0.017262881621718407\n",
      "Epoch 1349, Loss: 0.01822724938392639\n",
      "Epoch 1350, Loss: 0.022127259522676468\n",
      "Epoch 1351, Loss: 0.013631459325551987\n",
      "Epoch 1352, Loss: 0.012912735342979431\n",
      "Epoch 1353, Loss: 0.01491505466401577\n",
      "Epoch 1354, Loss: 0.017507072538137436\n",
      "Epoch 1355, Loss: 0.01650889217853546\n",
      "Epoch 1356, Loss: 0.033689457923173904\n",
      "Epoch 1357, Loss: 0.025357099249958992\n",
      "Epoch 1358, Loss: 0.0147392638027668\n",
      "Epoch 1359, Loss: 0.03644251823425293\n",
      "Epoch 1360, Loss: 0.01958264783024788\n",
      "Epoch 1361, Loss: 0.02593475580215454\n",
      "Epoch 1362, Loss: 0.023169856518507004\n",
      "Epoch 1363, Loss: 0.026028940454125404\n",
      "Epoch 1364, Loss: 0.013614856638014317\n",
      "Epoch 1365, Loss: 0.019276682287454605\n",
      "Epoch 1366, Loss: 0.030949225649237633\n",
      "Epoch 1367, Loss: 0.019430777058005333\n",
      "Epoch 1368, Loss: 0.03310214728116989\n",
      "Epoch 1369, Loss: 0.01959117501974106\n",
      "Epoch 1370, Loss: 0.015750573948025703\n",
      "Epoch 1371, Loss: 0.014386126771569252\n",
      "Epoch 1372, Loss: 0.021877359598875046\n",
      "Epoch 1373, Loss: 0.015663305297493935\n",
      "Epoch 1374, Loss: 0.026988133788108826\n",
      "Epoch 1375, Loss: 0.023146720603108406\n",
      "Epoch 1376, Loss: 0.0139120789244771\n",
      "Epoch 1377, Loss: 0.014753722585737705\n",
      "Epoch 1378, Loss: 0.024105357006192207\n",
      "Epoch 1379, Loss: 0.017855023965239525\n",
      "Epoch 1380, Loss: 0.016968218609690666\n",
      "Epoch 1381, Loss: 0.016221754252910614\n",
      "Epoch 1382, Loss: 0.016447754576802254\n",
      "Epoch 1383, Loss: 0.030064424499869347\n",
      "Epoch 1384, Loss: 0.013540154322981834\n",
      "Epoch 1385, Loss: 0.04981560632586479\n",
      "Epoch 1386, Loss: 0.0342491939663887\n",
      "Epoch 1387, Loss: 0.011921051889657974\n",
      "Epoch 1388, Loss: 0.01587209291756153\n",
      "Epoch 1389, Loss: 0.03039620630443096\n",
      "Epoch 1390, Loss: 0.018852364271879196\n",
      "Epoch 1391, Loss: 0.01555386558175087\n",
      "Epoch 1392, Loss: 0.015606390312314034\n",
      "Epoch 1393, Loss: 0.022763315588235855\n",
      "Epoch 1394, Loss: 0.02497394196689129\n",
      "Epoch 1395, Loss: 0.01703159138560295\n",
      "Epoch 1396, Loss: 0.016279906034469604\n",
      "Epoch 1397, Loss: 0.01513231173157692\n",
      "Epoch 1398, Loss: 0.019837504252791405\n",
      "Epoch 1399, Loss: 0.01452056784182787\n",
      "Epoch 1400, Loss: 0.03885116055607796\n",
      "Epoch 1401, Loss: 0.015471191145479679\n",
      "Epoch 1402, Loss: 0.025468232110142708\n",
      "Epoch 1403, Loss: 0.0217245165258646\n",
      "Epoch 1404, Loss: 0.01878987066447735\n",
      "Epoch 1405, Loss: 0.012845859862864017\n",
      "Epoch 1406, Loss: 0.02491108886897564\n",
      "Epoch 1407, Loss: 0.025810154154896736\n",
      "Epoch 1408, Loss: 0.015148816630244255\n",
      "Epoch 1409, Loss: 0.015348304994404316\n",
      "Epoch 1410, Loss: 0.025429118424654007\n",
      "Epoch 1411, Loss: 0.014047824777662754\n",
      "Epoch 1412, Loss: 0.022022955119609833\n",
      "Epoch 1413, Loss: 0.013737780973315239\n",
      "Epoch 1414, Loss: 0.014145049266517162\n",
      "Epoch 1415, Loss: 0.01547846570611\n",
      "Epoch 1416, Loss: 0.015814267098903656\n",
      "Epoch 1417, Loss: 0.019189050421118736\n",
      "Epoch 1418, Loss: 0.01614946499466896\n",
      "Epoch 1419, Loss: 0.014357775449752808\n",
      "Epoch 1420, Loss: 0.021937869489192963\n",
      "Epoch 1421, Loss: 0.02026047371327877\n",
      "Epoch 1422, Loss: 0.022898055613040924\n",
      "Epoch 1423, Loss: 0.01447029784321785\n",
      "Epoch 1424, Loss: 0.02469072863459587\n",
      "Epoch 1425, Loss: 0.021876398473978043\n",
      "Epoch 1426, Loss: 0.017769768834114075\n",
      "Epoch 1427, Loss: 0.01629277504980564\n",
      "Epoch 1428, Loss: 0.016248248517513275\n",
      "Epoch 1429, Loss: 0.019239019602537155\n",
      "Epoch 1430, Loss: 0.036193300038576126\n",
      "Epoch 1431, Loss: 0.01583493873476982\n",
      "Epoch 1432, Loss: 0.013123267330229282\n",
      "Epoch 1433, Loss: 0.020000522956252098\n",
      "Epoch 1434, Loss: 0.022113891318440437\n",
      "Epoch 1435, Loss: 0.013745916076004505\n",
      "Epoch 1436, Loss: 0.016988979652523994\n",
      "Epoch 1437, Loss: 0.018746422603726387\n",
      "Epoch 1438, Loss: 0.027237143367528915\n",
      "Epoch 1439, Loss: 0.026346471160650253\n",
      "Epoch 1440, Loss: 0.01368394773453474\n",
      "Epoch 1441, Loss: 0.02703363448381424\n",
      "Epoch 1442, Loss: 0.014005989767611027\n",
      "Epoch 1443, Loss: 0.025848910212516785\n",
      "Epoch 1444, Loss: 0.015186060220003128\n",
      "Epoch 1445, Loss: 0.01997118815779686\n",
      "Epoch 1446, Loss: 0.01595175266265869\n",
      "Epoch 1447, Loss: 0.012610462494194508\n",
      "Epoch 1448, Loss: 0.018892554566264153\n",
      "Epoch 1449, Loss: 0.015326735563576221\n",
      "Epoch 1450, Loss: 0.01480365265160799\n",
      "Epoch 1451, Loss: 0.026349760591983795\n",
      "Epoch 1452, Loss: 0.03590476140379906\n",
      "Epoch 1453, Loss: 0.01975340209901333\n",
      "Epoch 1454, Loss: 0.02028604969382286\n",
      "Epoch 1455, Loss: 0.02208545245230198\n",
      "Epoch 1456, Loss: 0.022862503305077553\n",
      "Epoch 1457, Loss: 0.011972932144999504\n",
      "Epoch 1458, Loss: 0.02458115853369236\n",
      "Epoch 1459, Loss: 0.01389897707849741\n",
      "Epoch 1460, Loss: 0.02319057658314705\n",
      "Epoch 1461, Loss: 0.03430751711130142\n",
      "Epoch 1462, Loss: 0.03510289639234543\n",
      "Epoch 1463, Loss: 0.013729562982916832\n",
      "Epoch 1464, Loss: 0.012601149268448353\n",
      "Epoch 1465, Loss: 0.018600044772028923\n",
      "Epoch 1466, Loss: 0.01848919875919819\n",
      "Epoch 1467, Loss: 0.016162579879164696\n",
      "Epoch 1468, Loss: 0.013314294628798962\n",
      "Epoch 1469, Loss: 0.014674567617475986\n",
      "Epoch 1470, Loss: 0.017863497138023376\n",
      "Epoch 1471, Loss: 0.012651142664253712\n",
      "Epoch 1472, Loss: 0.04717198386788368\n",
      "Epoch 1473, Loss: 0.024255335330963135\n",
      "Epoch 1474, Loss: 0.01705317758023739\n",
      "Epoch 1475, Loss: 0.027316825464367867\n",
      "Epoch 1476, Loss: 0.022893983870744705\n",
      "Epoch 1477, Loss: 0.02041804790496826\n",
      "Epoch 1478, Loss: 0.02768498845398426\n",
      "Epoch 1479, Loss: 0.017566604539752007\n",
      "Epoch 1480, Loss: 0.012447033077478409\n",
      "Epoch 1481, Loss: 0.013796758837997913\n",
      "Epoch 1482, Loss: 0.01961408369243145\n",
      "Epoch 1483, Loss: 0.013201079331338406\n",
      "Epoch 1484, Loss: 0.03747295215725899\n",
      "Epoch 1485, Loss: 0.015714121982455254\n",
      "Epoch 1486, Loss: 0.017295988276600838\n",
      "Epoch 1487, Loss: 0.017833074554800987\n",
      "Epoch 1488, Loss: 0.02272254042327404\n",
      "Epoch 1489, Loss: 0.03077908419072628\n",
      "Epoch 1490, Loss: 0.04169197380542755\n",
      "Epoch 1491, Loss: 0.01141924224793911\n",
      "Epoch 1492, Loss: 0.024857504293322563\n",
      "Epoch 1493, Loss: 0.023274218663573265\n",
      "Epoch 1494, Loss: 0.013412118889391422\n",
      "Epoch 1495, Loss: 0.013258446007966995\n",
      "Epoch 1496, Loss: 0.019574927166104317\n",
      "Epoch 1497, Loss: 0.01257331669330597\n",
      "Epoch 1498, Loss: 0.01313248835504055\n",
      "Epoch 1499, Loss: 0.012086398899555206\n",
      "Epoch 1500, Loss: 0.015121250413358212\n",
      "Epoch 1501, Loss: 0.01745183765888214\n",
      "Epoch 1502, Loss: 0.03910808637738228\n",
      "Epoch 1503, Loss: 0.012561812996864319\n",
      "Epoch 1504, Loss: 0.012437744997441769\n",
      "Epoch 1505, Loss: 0.01388594787567854\n",
      "Epoch 1506, Loss: 0.015134616754949093\n",
      "Epoch 1507, Loss: 0.012134172022342682\n",
      "Epoch 1508, Loss: 0.02429923228919506\n",
      "Epoch 1509, Loss: 0.020157277584075928\n",
      "Epoch 1510, Loss: 0.011255609802901745\n",
      "Epoch 1511, Loss: 0.01440874021500349\n",
      "Epoch 1512, Loss: 0.01154048927128315\n",
      "Epoch 1513, Loss: 0.02119702287018299\n",
      "Epoch 1514, Loss: 0.013676423579454422\n",
      "Epoch 1515, Loss: 0.011681958101689816\n",
      "Epoch 1516, Loss: 0.018344372510910034\n",
      "Epoch 1517, Loss: 0.013421253301203251\n",
      "Epoch 1518, Loss: 0.015192361548542976\n",
      "Epoch 1519, Loss: 0.026181533932685852\n",
      "Epoch 1520, Loss: 0.011010495945811272\n",
      "Epoch 1521, Loss: 0.01860658824443817\n",
      "Epoch 1522, Loss: 0.011344955302774906\n",
      "Epoch 1523, Loss: 0.017813963815569878\n",
      "Epoch 1524, Loss: 0.016217587515711784\n",
      "Epoch 1525, Loss: 0.0369303822517395\n",
      "Epoch 1526, Loss: 0.021964864805340767\n",
      "Epoch 1527, Loss: 0.013764840550720692\n",
      "Epoch 1528, Loss: 0.01419762708246708\n",
      "Epoch 1529, Loss: 0.012235988862812519\n",
      "Epoch 1530, Loss: 0.02667430229485035\n",
      "Epoch 1531, Loss: 0.01700994186103344\n",
      "Epoch 1532, Loss: 0.011112764477729797\n",
      "Epoch 1533, Loss: 0.015324949286878109\n",
      "Epoch 1534, Loss: 0.017128897830843925\n",
      "Epoch 1535, Loss: 0.01338163111358881\n",
      "Epoch 1536, Loss: 0.01443849503993988\n",
      "Epoch 1537, Loss: 0.020780565217137337\n",
      "Epoch 1538, Loss: 0.012492931447923183\n",
      "Epoch 1539, Loss: 0.012053101323544979\n",
      "Epoch 1540, Loss: 0.011892092414200306\n",
      "Epoch 1541, Loss: 0.013569171540439129\n",
      "Epoch 1542, Loss: 0.013508819974958897\n",
      "Epoch 1543, Loss: 0.021045193076133728\n",
      "Epoch 1544, Loss: 0.018845655024051666\n",
      "Epoch 1545, Loss: 0.011840304359793663\n",
      "Epoch 1546, Loss: 0.021993549540638924\n",
      "Epoch 1547, Loss: 0.0143814105540514\n",
      "Epoch 1548, Loss: 0.011062270030379295\n",
      "Epoch 1549, Loss: 0.01282106526196003\n",
      "Epoch 1550, Loss: 0.01473933830857277\n",
      "Epoch 1551, Loss: 0.016811054199934006\n",
      "Epoch 1552, Loss: 0.012195177376270294\n",
      "Epoch 1553, Loss: 0.012175938114523888\n",
      "Epoch 1554, Loss: 0.013208596967160702\n",
      "Epoch 1555, Loss: 0.015112597495317459\n",
      "Epoch 1556, Loss: 0.012530099600553513\n",
      "Epoch 1557, Loss: 0.011303849518299103\n",
      "Epoch 1558, Loss: 0.01600589230656624\n",
      "Epoch 1559, Loss: 0.017254343256354332\n",
      "Epoch 1560, Loss: 0.015021507628262043\n",
      "Epoch 1561, Loss: 0.011456911452114582\n",
      "Epoch 1562, Loss: 0.013002568855881691\n",
      "Epoch 1563, Loss: 0.01222799252718687\n",
      "Epoch 1564, Loss: 0.02342202328145504\n",
      "Epoch 1565, Loss: 0.015597016550600529\n",
      "Epoch 1566, Loss: 0.010229247622191906\n",
      "Epoch 1567, Loss: 0.022440338507294655\n",
      "Epoch 1568, Loss: 0.021024638786911964\n",
      "Epoch 1569, Loss: 0.014212642796337605\n",
      "Epoch 1570, Loss: 0.030216490849852562\n",
      "Epoch 1571, Loss: 0.0241884533315897\n",
      "Epoch 1572, Loss: 0.029799766838550568\n",
      "Epoch 1573, Loss: 0.012229607440531254\n",
      "Epoch 1574, Loss: 0.045582365244627\n",
      "Epoch 1575, Loss: 0.020216060802340508\n",
      "Epoch 1576, Loss: 0.013330115005373955\n",
      "Epoch 1577, Loss: 0.02022664248943329\n",
      "Epoch 1578, Loss: 0.012192430905997753\n",
      "Epoch 1579, Loss: 0.015244392678141594\n",
      "Epoch 1580, Loss: 0.011914367787539959\n",
      "Epoch 1581, Loss: 0.026756903156638145\n",
      "Epoch 1582, Loss: 0.012959873303771019\n",
      "Epoch 1583, Loss: 0.013647201471030712\n",
      "Epoch 1584, Loss: 0.01514585129916668\n",
      "Epoch 1585, Loss: 0.03543548658490181\n",
      "Epoch 1586, Loss: 0.014569277875125408\n",
      "Epoch 1587, Loss: 0.015037366189062595\n",
      "Epoch 1588, Loss: 0.016462281346321106\n",
      "Epoch 1589, Loss: 0.013035064563155174\n",
      "Epoch 1590, Loss: 0.012730681337416172\n",
      "Epoch 1591, Loss: 0.010791163891553879\n",
      "Epoch 1592, Loss: 0.012116698548197746\n",
      "Epoch 1593, Loss: 0.022023143246769905\n",
      "Epoch 1594, Loss: 0.01324990764260292\n",
      "Epoch 1595, Loss: 0.0215835552662611\n",
      "Epoch 1596, Loss: 0.020129889249801636\n",
      "Epoch 1597, Loss: 0.02028658613562584\n",
      "Epoch 1598, Loss: 0.01336398534476757\n",
      "Epoch 1599, Loss: 0.011939849704504013\n",
      "Epoch 1600, Loss: 0.012459400109946728\n",
      "Epoch 1601, Loss: 0.01239754818379879\n",
      "Epoch 1602, Loss: 0.029606863856315613\n",
      "Epoch 1603, Loss: 0.02731684409081936\n",
      "Epoch 1604, Loss: 0.013955463655292988\n",
      "Epoch 1605, Loss: 0.012728843837976456\n",
      "Epoch 1606, Loss: 0.023353490978479385\n",
      "Epoch 1607, Loss: 0.012463578954339027\n",
      "Epoch 1608, Loss: 0.04263118654489517\n",
      "Epoch 1609, Loss: 0.01097568403929472\n",
      "Epoch 1610, Loss: 0.01680401898920536\n",
      "Epoch 1611, Loss: 0.011612733826041222\n",
      "Epoch 1612, Loss: 0.016205379739403725\n",
      "Epoch 1613, Loss: 0.010545679368078709\n",
      "Epoch 1614, Loss: 0.011226531118154526\n",
      "Epoch 1615, Loss: 0.01113659143447876\n",
      "Epoch 1616, Loss: 0.010541249066591263\n",
      "Epoch 1617, Loss: 0.01264384388923645\n",
      "Epoch 1618, Loss: 0.011414973065257072\n",
      "Epoch 1619, Loss: 0.01041815709322691\n",
      "Epoch 1620, Loss: 0.02008974552154541\n",
      "Epoch 1621, Loss: 0.011043349280953407\n",
      "Epoch 1622, Loss: 0.01612207666039467\n",
      "Epoch 1623, Loss: 0.015649911016225815\n",
      "Epoch 1624, Loss: 0.01294359564781189\n",
      "Epoch 1625, Loss: 0.011438999325037003\n",
      "Epoch 1626, Loss: 0.012930702418088913\n",
      "Epoch 1627, Loss: 0.010734030045568943\n",
      "Epoch 1628, Loss: 0.015574675984680653\n",
      "Epoch 1629, Loss: 0.01751772128045559\n",
      "Epoch 1630, Loss: 0.013810711912810802\n",
      "Epoch 1631, Loss: 0.011085323058068752\n",
      "Epoch 1632, Loss: 0.014614466577768326\n",
      "Epoch 1633, Loss: 0.01597139984369278\n",
      "Epoch 1634, Loss: 0.015507759526371956\n",
      "Epoch 1635, Loss: 0.017186535522341728\n",
      "Epoch 1636, Loss: 0.011422461830079556\n",
      "Epoch 1637, Loss: 0.0181176345795393\n",
      "Epoch 1638, Loss: 0.029324809089303017\n",
      "Epoch 1639, Loss: 0.018495406955480576\n",
      "Epoch 1640, Loss: 0.01134665310382843\n",
      "Epoch 1641, Loss: 0.018904291093349457\n",
      "Epoch 1642, Loss: 0.013265391811728477\n",
      "Epoch 1643, Loss: 0.010850835591554642\n",
      "Epoch 1644, Loss: 0.011810231022536755\n",
      "Epoch 1645, Loss: 0.02245861105620861\n",
      "Epoch 1646, Loss: 0.011925699189305305\n",
      "Epoch 1647, Loss: 0.01115510519593954\n",
      "Epoch 1648, Loss: 0.012872756458818913\n",
      "Epoch 1649, Loss: 0.026081666350364685\n",
      "Epoch 1650, Loss: 0.011495555751025677\n",
      "Epoch 1651, Loss: 0.010425569489598274\n",
      "Epoch 1652, Loss: 0.012744661420583725\n",
      "Epoch 1653, Loss: 0.03155282512307167\n",
      "Epoch 1654, Loss: 0.012725705280900002\n",
      "Epoch 1655, Loss: 0.013963896781206131\n",
      "Epoch 1656, Loss: 0.021531308069825172\n",
      "Epoch 1657, Loss: 0.010050065815448761\n",
      "Epoch 1658, Loss: 0.012786293402314186\n",
      "Epoch 1659, Loss: 0.011845093220472336\n",
      "Epoch 1660, Loss: 0.02419290877878666\n",
      "Epoch 1661, Loss: 0.014295612461864948\n",
      "Epoch 1662, Loss: 0.011694516986608505\n",
      "Epoch 1663, Loss: 0.019624099135398865\n",
      "Epoch 1664, Loss: 0.009917483665049076\n",
      "Epoch 1665, Loss: 0.012597460299730301\n",
      "Epoch 1666, Loss: 0.011309467256069183\n",
      "Epoch 1667, Loss: 0.014334437437355518\n",
      "Epoch 1668, Loss: 0.0179534163326025\n",
      "Epoch 1669, Loss: 0.011194666847586632\n",
      "Epoch 1670, Loss: 0.009917931631207466\n",
      "Epoch 1671, Loss: 0.01908193528652191\n",
      "Epoch 1672, Loss: 0.01801501028239727\n",
      "Epoch 1673, Loss: 0.011414950713515282\n",
      "Epoch 1674, Loss: 0.011887923814356327\n",
      "Epoch 1675, Loss: 0.01319477241486311\n",
      "Epoch 1676, Loss: 0.011887549422681332\n",
      "Epoch 1677, Loss: 0.021401017904281616\n",
      "Epoch 1678, Loss: 0.012995834462344646\n",
      "Epoch 1679, Loss: 0.018817679956555367\n",
      "Epoch 1680, Loss: 0.01992436870932579\n",
      "Epoch 1681, Loss: 0.016123155131936073\n",
      "Epoch 1682, Loss: 0.011649115942418575\n",
      "Epoch 1683, Loss: 0.01901331916451454\n",
      "Epoch 1684, Loss: 0.01557177770882845\n",
      "Epoch 1685, Loss: 0.040766507387161255\n",
      "Epoch 1686, Loss: 0.011477058753371239\n",
      "Epoch 1687, Loss: 0.010260102339088917\n",
      "Epoch 1688, Loss: 0.016062891110777855\n",
      "Epoch 1689, Loss: 0.018086252734065056\n",
      "Epoch 1690, Loss: 0.011844497174024582\n",
      "Epoch 1691, Loss: 0.010359497740864754\n",
      "Epoch 1692, Loss: 0.013741298578679562\n",
      "Epoch 1693, Loss: 0.013847076334059238\n",
      "Epoch 1694, Loss: 0.010973208583891392\n",
      "Epoch 1695, Loss: 0.030243491753935814\n",
      "Epoch 1696, Loss: 0.010739052668213844\n",
      "Epoch 1697, Loss: 0.010361149907112122\n",
      "Epoch 1698, Loss: 0.01051278505474329\n",
      "Epoch 1699, Loss: 0.011628059670329094\n",
      "Epoch 1700, Loss: 0.02186485193669796\n",
      "Epoch 1701, Loss: 0.012171400710940361\n",
      "Epoch 1702, Loss: 0.013715717010200024\n",
      "Epoch 1703, Loss: 0.011061819270253181\n",
      "Epoch 1704, Loss: 0.013493151403963566\n",
      "Epoch 1705, Loss: 0.01504299696534872\n",
      "Epoch 1706, Loss: 0.01984495110809803\n",
      "Epoch 1707, Loss: 0.0121993413195014\n",
      "Epoch 1708, Loss: 0.019430290907621384\n",
      "Epoch 1709, Loss: 0.009616165421903133\n",
      "Epoch 1710, Loss: 0.017983781173825264\n",
      "Epoch 1711, Loss: 0.014115747064352036\n",
      "Epoch 1712, Loss: 0.01034997683018446\n",
      "Epoch 1713, Loss: 0.017052263021469116\n",
      "Epoch 1714, Loss: 0.011712725274264812\n",
      "Epoch 1715, Loss: 0.010827606543898582\n",
      "Epoch 1716, Loss: 0.013578614220023155\n",
      "Epoch 1717, Loss: 0.011019636876881123\n",
      "Epoch 1718, Loss: 0.012485546059906483\n",
      "Epoch 1719, Loss: 0.009827014990150928\n",
      "Epoch 1720, Loss: 0.011018744669854641\n",
      "Epoch 1721, Loss: 0.013301456347107887\n",
      "Epoch 1722, Loss: 0.012963409535586834\n",
      "Epoch 1723, Loss: 0.011298398487269878\n",
      "Epoch 1724, Loss: 0.015255208127200603\n",
      "Epoch 1725, Loss: 0.017897266894578934\n",
      "Epoch 1726, Loss: 0.010850048623979092\n",
      "Epoch 1727, Loss: 0.011414464563131332\n",
      "Epoch 1728, Loss: 0.01524823997169733\n",
      "Epoch 1729, Loss: 0.010531970299780369\n",
      "Epoch 1730, Loss: 0.020525185391306877\n",
      "Epoch 1731, Loss: 0.010235232301056385\n",
      "Epoch 1732, Loss: 0.021010879427194595\n",
      "Epoch 1733, Loss: 0.009830729104578495\n",
      "Epoch 1734, Loss: 0.009131750091910362\n",
      "Epoch 1735, Loss: 0.009543863125145435\n",
      "Epoch 1736, Loss: 0.011929381638765335\n",
      "Epoch 1737, Loss: 0.009295003488659859\n",
      "Epoch 1738, Loss: 0.014377445913851261\n",
      "Epoch 1739, Loss: 0.010136054828763008\n",
      "Epoch 1740, Loss: 0.011587060987949371\n",
      "Epoch 1741, Loss: 0.014610565267503262\n",
      "Epoch 1742, Loss: 0.01198441069573164\n",
      "Epoch 1743, Loss: 0.009749922901391983\n",
      "Epoch 1744, Loss: 0.009623084217309952\n",
      "Epoch 1745, Loss: 0.01460744347423315\n",
      "Epoch 1746, Loss: 0.01077946275472641\n",
      "Epoch 1747, Loss: 0.020265286788344383\n",
      "Epoch 1748, Loss: 0.010362447239458561\n",
      "Epoch 1749, Loss: 0.014188720844686031\n",
      "Epoch 1750, Loss: 0.009478680789470673\n",
      "Epoch 1751, Loss: 0.023933786898851395\n",
      "Epoch 1752, Loss: 0.01660570502281189\n",
      "Epoch 1753, Loss: 0.00955864880234003\n",
      "Epoch 1754, Loss: 0.008491413667798042\n",
      "Epoch 1755, Loss: 0.01311361975967884\n",
      "Epoch 1756, Loss: 0.012921859510242939\n",
      "Epoch 1757, Loss: 0.03653275966644287\n",
      "Epoch 1758, Loss: 0.009854238480329514\n",
      "Epoch 1759, Loss: 0.00954009871929884\n",
      "Epoch 1760, Loss: 0.009843481704592705\n",
      "Epoch 1761, Loss: 0.011163871735334396\n",
      "Epoch 1762, Loss: 0.01752426102757454\n",
      "Epoch 1763, Loss: 0.017115235328674316\n",
      "Epoch 1764, Loss: 0.009980034083127975\n",
      "Epoch 1765, Loss: 0.028911463916301727\n",
      "Epoch 1766, Loss: 0.017745131626725197\n",
      "Epoch 1767, Loss: 0.008973200805485249\n",
      "Epoch 1768, Loss: 0.010663896799087524\n",
      "Epoch 1769, Loss: 0.00970161147415638\n",
      "Epoch 1770, Loss: 0.009540800005197525\n",
      "Epoch 1771, Loss: 0.010287575423717499\n",
      "Epoch 1772, Loss: 0.024067221209406853\n",
      "Epoch 1773, Loss: 0.013816343620419502\n",
      "Epoch 1774, Loss: 0.012978142127394676\n",
      "Epoch 1775, Loss: 0.010326487943530083\n",
      "Epoch 1776, Loss: 0.011561956256628036\n",
      "Epoch 1777, Loss: 0.02572648786008358\n",
      "Epoch 1778, Loss: 0.020725978538393974\n",
      "Epoch 1779, Loss: 0.01673739403486252\n",
      "Epoch 1780, Loss: 0.011235183104872704\n",
      "Epoch 1781, Loss: 0.011521494016051292\n",
      "Epoch 1782, Loss: 0.015917664393782616\n",
      "Epoch 1783, Loss: 0.011026982218027115\n",
      "Epoch 1784, Loss: 0.009818121790885925\n",
      "Epoch 1785, Loss: 0.014914591796696186\n",
      "Epoch 1786, Loss: 0.008624143898487091\n",
      "Epoch 1787, Loss: 0.01654921844601631\n",
      "Epoch 1788, Loss: 0.01321468222886324\n",
      "Epoch 1789, Loss: 0.009732258506119251\n",
      "Epoch 1790, Loss: 0.010653258301317692\n",
      "Epoch 1791, Loss: 0.011166827753186226\n",
      "Epoch 1792, Loss: 0.010547547601163387\n",
      "Epoch 1793, Loss: 0.014522626996040344\n",
      "Epoch 1794, Loss: 0.011862982995808125\n",
      "Epoch 1795, Loss: 0.009728249162435532\n",
      "Epoch 1796, Loss: 0.011677651666104794\n",
      "Epoch 1797, Loss: 0.009761123917996883\n",
      "Epoch 1798, Loss: 0.011708003468811512\n",
      "Epoch 1799, Loss: 0.009196525439620018\n",
      "Epoch 1800, Loss: 0.009971017017960548\n",
      "Epoch 1801, Loss: 0.012500916607677937\n",
      "Epoch 1802, Loss: 0.010674471966922283\n",
      "Epoch 1803, Loss: 0.011209876276552677\n",
      "Epoch 1804, Loss: 0.013108576647937298\n",
      "Epoch 1805, Loss: 0.030814331024885178\n",
      "Epoch 1806, Loss: 0.009493034332990646\n",
      "Epoch 1807, Loss: 0.015284053981304169\n",
      "Epoch 1808, Loss: 0.010430161841213703\n",
      "Epoch 1809, Loss: 0.01016125176101923\n",
      "Epoch 1810, Loss: 0.017059559002518654\n",
      "Epoch 1811, Loss: 0.015300385653972626\n",
      "Epoch 1812, Loss: 0.009992903098464012\n",
      "Epoch 1813, Loss: 0.01629803329706192\n",
      "Epoch 1814, Loss: 0.017167195677757263\n",
      "Epoch 1815, Loss: 0.014809224754571915\n",
      "Epoch 1816, Loss: 0.014335673302412033\n",
      "Epoch 1817, Loss: 0.02274218015372753\n",
      "Epoch 1818, Loss: 0.010769071988761425\n",
      "Epoch 1819, Loss: 0.022949811071157455\n",
      "Epoch 1820, Loss: 0.009110867977142334\n",
      "Epoch 1821, Loss: 0.011896591633558273\n",
      "Epoch 1822, Loss: 0.012920846231281757\n",
      "Epoch 1823, Loss: 0.009296000935137272\n",
      "Epoch 1824, Loss: 0.010865551419556141\n",
      "Epoch 1825, Loss: 0.031146036460995674\n",
      "Epoch 1826, Loss: 0.014449250884354115\n",
      "Epoch 1827, Loss: 0.016393888741731644\n",
      "Epoch 1828, Loss: 0.010237608104944229\n",
      "Epoch 1829, Loss: 0.01009297277778387\n",
      "Epoch 1830, Loss: 0.00909335445612669\n",
      "Epoch 1831, Loss: 0.009655260480940342\n",
      "Epoch 1832, Loss: 0.011335997842252254\n",
      "Epoch 1833, Loss: 0.011017903685569763\n",
      "Epoch 1834, Loss: 0.011562325991690159\n",
      "Epoch 1835, Loss: 0.00955241359770298\n",
      "Epoch 1836, Loss: 0.01825823076069355\n",
      "Epoch 1837, Loss: 0.012989267706871033\n",
      "Epoch 1838, Loss: 0.01018438208848238\n",
      "Epoch 1839, Loss: 0.009106893092393875\n",
      "Epoch 1840, Loss: 0.012410216964781284\n",
      "Epoch 1841, Loss: 0.007768826559185982\n",
      "Epoch 1842, Loss: 0.010089372284710407\n",
      "Epoch 1843, Loss: 0.03694024309515953\n",
      "Epoch 1844, Loss: 0.012038148008286953\n",
      "Epoch 1845, Loss: 0.008653068915009499\n",
      "Epoch 1846, Loss: 0.009331529960036278\n",
      "Epoch 1847, Loss: 0.00959787331521511\n",
      "Epoch 1848, Loss: 0.009847015142440796\n",
      "Epoch 1849, Loss: 0.01985928788781166\n",
      "Epoch 1850, Loss: 0.01341273169964552\n",
      "Epoch 1851, Loss: 0.014211704954504967\n",
      "Epoch 1852, Loss: 0.010518708266317844\n",
      "Epoch 1853, Loss: 0.009747082367539406\n",
      "Epoch 1854, Loss: 0.009111225605010986\n",
      "Epoch 1855, Loss: 0.01777150295674801\n",
      "Epoch 1856, Loss: 0.01099979318678379\n",
      "Epoch 1857, Loss: 0.009741246700286865\n",
      "Epoch 1858, Loss: 0.010523885488510132\n",
      "Epoch 1859, Loss: 0.018575394526124\n",
      "Epoch 1860, Loss: 0.009757631458342075\n",
      "Epoch 1861, Loss: 0.013242845423519611\n",
      "Epoch 1862, Loss: 0.014465145766735077\n",
      "Epoch 1863, Loss: 0.009743261151015759\n",
      "Epoch 1864, Loss: 0.011797727085649967\n",
      "Epoch 1865, Loss: 0.010323661379516125\n",
      "Epoch 1866, Loss: 0.011372877284884453\n",
      "Epoch 1867, Loss: 0.00893755815923214\n",
      "Epoch 1868, Loss: 0.012215672992169857\n",
      "Epoch 1869, Loss: 0.008983173407614231\n",
      "Epoch 1870, Loss: 0.011282946914434433\n",
      "Epoch 1871, Loss: 0.016715317964553833\n",
      "Epoch 1872, Loss: 0.00976171437650919\n",
      "Epoch 1873, Loss: 0.027477964758872986\n",
      "Epoch 1874, Loss: 0.008156807161867619\n",
      "Epoch 1875, Loss: 0.009084668941795826\n",
      "Epoch 1876, Loss: 0.011534987017512321\n",
      "Epoch 1877, Loss: 0.009154288098216057\n",
      "Epoch 1878, Loss: 0.008457398973405361\n",
      "Epoch 1879, Loss: 0.009903213009238243\n",
      "Epoch 1880, Loss: 0.009623933583498001\n",
      "Epoch 1881, Loss: 0.00969009567052126\n",
      "Epoch 1882, Loss: 0.009450194425880909\n",
      "Epoch 1883, Loss: 0.013592504896223545\n",
      "Epoch 1884, Loss: 0.01056738756597042\n",
      "Epoch 1885, Loss: 0.010051877237856388\n",
      "Epoch 1886, Loss: 0.010575042106211185\n",
      "Epoch 1887, Loss: 0.00993433129042387\n",
      "Epoch 1888, Loss: 0.007497086655348539\n",
      "Epoch 1889, Loss: 0.014489855617284775\n",
      "Epoch 1890, Loss: 0.008929225616157055\n",
      "Epoch 1891, Loss: 0.009902373887598515\n",
      "Epoch 1892, Loss: 0.0196372102946043\n",
      "Epoch 1893, Loss: 0.008410167880356312\n",
      "Epoch 1894, Loss: 0.00838614720851183\n",
      "Epoch 1895, Loss: 0.009683751501142979\n",
      "Epoch 1896, Loss: 0.009403754957020283\n",
      "Epoch 1897, Loss: 0.008374324068427086\n",
      "Epoch 1898, Loss: 0.009060452692210674\n",
      "Epoch 1899, Loss: 0.009128648787736893\n",
      "Epoch 1900, Loss: 0.012438025325536728\n",
      "Epoch 1901, Loss: 0.0153971491381526\n",
      "Epoch 1902, Loss: 0.023169727995991707\n",
      "Epoch 1903, Loss: 0.0095695024356246\n",
      "Epoch 1904, Loss: 0.0119551420211792\n",
      "Epoch 1905, Loss: 0.033009935170412064\n",
      "Epoch 1906, Loss: 0.008955283090472221\n",
      "Epoch 1907, Loss: 0.020193491131067276\n",
      "Epoch 1908, Loss: 0.009089309722185135\n",
      "Epoch 1909, Loss: 0.00775015726685524\n",
      "Epoch 1910, Loss: 0.01303520705550909\n",
      "Epoch 1911, Loss: 0.011446899734437466\n",
      "Epoch 1912, Loss: 0.009921017102897167\n",
      "Epoch 1913, Loss: 0.00850392784923315\n",
      "Epoch 1914, Loss: 0.02776036411523819\n",
      "Epoch 1915, Loss: 0.008633038960397243\n",
      "Epoch 1916, Loss: 0.007853800430893898\n",
      "Epoch 1917, Loss: 0.008883140049874783\n",
      "Epoch 1918, Loss: 0.01096493098884821\n",
      "Epoch 1919, Loss: 0.00955493189394474\n",
      "Epoch 1920, Loss: 0.010446911677718163\n",
      "Epoch 1921, Loss: 0.01056944951415062\n",
      "Epoch 1922, Loss: 0.012908853590488434\n",
      "Epoch 1923, Loss: 0.01447857916355133\n",
      "Epoch 1924, Loss: 0.0097199697047472\n",
      "Epoch 1925, Loss: 0.00864461250603199\n",
      "Epoch 1926, Loss: 0.011201220564544201\n",
      "Epoch 1927, Loss: 0.00835501216351986\n",
      "Epoch 1928, Loss: 0.009676925837993622\n",
      "Epoch 1929, Loss: 0.009475168772041798\n",
      "Epoch 1930, Loss: 0.009611181914806366\n",
      "Epoch 1931, Loss: 0.010126452893018723\n",
      "Epoch 1932, Loss: 0.007505003362894058\n",
      "Epoch 1933, Loss: 0.01225729938596487\n",
      "Epoch 1934, Loss: 0.008157536387443542\n",
      "Epoch 1935, Loss: 0.008277429267764091\n",
      "Epoch 1936, Loss: 0.012439913116395473\n",
      "Epoch 1937, Loss: 0.015798667445778847\n",
      "Epoch 1938, Loss: 0.012703408487141132\n",
      "Epoch 1939, Loss: 0.011795897036790848\n",
      "Epoch 1940, Loss: 0.008285819552838802\n",
      "Epoch 1941, Loss: 0.027102714404463768\n",
      "Epoch 1942, Loss: 0.013145899400115013\n",
      "Epoch 1943, Loss: 0.009405435062944889\n",
      "Epoch 1944, Loss: 0.012838771566748619\n",
      "Epoch 1945, Loss: 0.0261343102902174\n",
      "Epoch 1946, Loss: 0.008601401001214981\n",
      "Epoch 1947, Loss: 0.03954627364873886\n",
      "Epoch 1948, Loss: 0.014766829088330269\n",
      "Epoch 1949, Loss: 0.01057050097733736\n",
      "Epoch 1950, Loss: 0.0073645878583192825\n",
      "Epoch 1951, Loss: 0.008495166897773743\n",
      "Epoch 1952, Loss: 0.008078835904598236\n",
      "Epoch 1953, Loss: 0.011605997569859028\n",
      "Epoch 1954, Loss: 0.00980306975543499\n",
      "Epoch 1955, Loss: 0.008530187420547009\n",
      "Epoch 1956, Loss: 0.009178548119962215\n",
      "Epoch 1957, Loss: 0.008598167449235916\n",
      "Epoch 1958, Loss: 0.00996695552021265\n",
      "Epoch 1959, Loss: 0.008553157560527325\n",
      "Epoch 1960, Loss: 0.012062819674611092\n",
      "Epoch 1961, Loss: 0.00814680103212595\n",
      "Epoch 1962, Loss: 0.009653998538851738\n",
      "Epoch 1963, Loss: 0.008787901140749454\n",
      "Epoch 1964, Loss: 0.010379253886640072\n",
      "Epoch 1965, Loss: 0.01236583199352026\n",
      "Epoch 1966, Loss: 0.008080990053713322\n",
      "Epoch 1967, Loss: 0.018591508269309998\n",
      "Epoch 1968, Loss: 0.012977094389498234\n",
      "Epoch 1969, Loss: 0.010708503425121307\n",
      "Epoch 1970, Loss: 0.008918185718357563\n",
      "Epoch 1971, Loss: 0.00847444124519825\n",
      "Epoch 1972, Loss: 0.009570036083459854\n",
      "Epoch 1973, Loss: 0.01715097948908806\n",
      "Epoch 1974, Loss: 0.00834924541413784\n",
      "Epoch 1975, Loss: 0.008115357719361782\n",
      "Epoch 1976, Loss: 0.02680123969912529\n",
      "Epoch 1977, Loss: 0.015589493326842785\n",
      "Epoch 1978, Loss: 0.007640655618160963\n",
      "Epoch 1979, Loss: 0.018908696249127388\n",
      "Epoch 1980, Loss: 0.01103899721056223\n",
      "Epoch 1981, Loss: 0.008935272693634033\n",
      "Epoch 1982, Loss: 0.0077294339425861835\n",
      "Epoch 1983, Loss: 0.008230284787714481\n",
      "Epoch 1984, Loss: 0.007593228481709957\n",
      "Epoch 1985, Loss: 0.008056008256971836\n",
      "Epoch 1986, Loss: 0.012176837772130966\n",
      "Epoch 1987, Loss: 0.008353002369403839\n",
      "Epoch 1988, Loss: 0.008588680066168308\n",
      "Epoch 1989, Loss: 0.007681268732994795\n",
      "Epoch 1990, Loss: 0.008787629194557667\n",
      "Epoch 1991, Loss: 0.009084085002541542\n",
      "Epoch 1992, Loss: 0.0090097114443779\n",
      "Epoch 1993, Loss: 0.008943435736000538\n",
      "Epoch 1994, Loss: 0.00791836902499199\n",
      "Epoch 1995, Loss: 0.018100276589393616\n",
      "Epoch 1996, Loss: 0.008592643775045872\n",
      "Epoch 1997, Loss: 0.023454586043953896\n",
      "Epoch 1998, Loss: 0.02196316234767437\n",
      "Epoch 1999, Loss: 0.008658657781779766\n",
      "Epoch 2000, Loss: 0.011530942283570766\n",
      "Epoch 2001, Loss: 0.008003226481378078\n",
      "Epoch 2002, Loss: 0.017247382551431656\n",
      "Epoch 2003, Loss: 0.008362432941794395\n",
      "Epoch 2004, Loss: 0.009982293471693993\n",
      "Epoch 2005, Loss: 0.009578210301697254\n",
      "Epoch 2006, Loss: 0.013683260418474674\n",
      "Epoch 2007, Loss: 0.009570084512233734\n",
      "Epoch 2008, Loss: 0.008213231340050697\n",
      "Epoch 2009, Loss: 0.009413263760507107\n",
      "Epoch 2010, Loss: 0.0077147651463747025\n",
      "Epoch 2011, Loss: 0.015652937814593315\n",
      "Epoch 2012, Loss: 0.009249698370695114\n",
      "Epoch 2013, Loss: 0.015308777801692486\n",
      "Epoch 2014, Loss: 0.010179429315030575\n",
      "Epoch 2015, Loss: 0.012899240478873253\n",
      "Epoch 2016, Loss: 0.011123224161565304\n",
      "Epoch 2017, Loss: 0.008203188888728619\n",
      "Epoch 2018, Loss: 0.008131194859743118\n",
      "Epoch 2019, Loss: 0.007637429051101208\n",
      "Epoch 2020, Loss: 0.007913349196314812\n",
      "Epoch 2021, Loss: 0.008121175691485405\n",
      "Epoch 2022, Loss: 0.007633322849869728\n",
      "Epoch 2023, Loss: 0.009023201651871204\n",
      "Epoch 2024, Loss: 0.007594044785946608\n",
      "Epoch 2025, Loss: 0.008527307771146297\n",
      "Epoch 2026, Loss: 0.022944854572415352\n",
      "Epoch 2027, Loss: 0.009315980598330498\n",
      "Epoch 2028, Loss: 0.007501969113945961\n",
      "Epoch 2029, Loss: 0.008106013759970665\n",
      "Epoch 2030, Loss: 0.009114200249314308\n",
      "Epoch 2031, Loss: 0.009484273381531239\n",
      "Epoch 2032, Loss: 0.00863727182149887\n",
      "Epoch 2033, Loss: 0.007972837425768375\n",
      "Epoch 2034, Loss: 0.007818851619958878\n",
      "Epoch 2035, Loss: 0.008149728178977966\n",
      "Epoch 2036, Loss: 0.014777058735489845\n",
      "Epoch 2037, Loss: 0.013815714046359062\n",
      "Epoch 2038, Loss: 0.007408850826323032\n",
      "Epoch 2039, Loss: 0.007583920378237963\n",
      "Epoch 2040, Loss: 0.011235751211643219\n",
      "Epoch 2041, Loss: 0.008411168120801449\n",
      "Epoch 2042, Loss: 0.010964121669530869\n",
      "Epoch 2043, Loss: 0.00822791550308466\n",
      "Epoch 2044, Loss: 0.019559543579816818\n",
      "Epoch 2045, Loss: 0.008115427568554878\n",
      "Epoch 2046, Loss: 0.00820574164390564\n",
      "Epoch 2047, Loss: 0.007070081774145365\n",
      "Epoch 2048, Loss: 0.015494664199650288\n",
      "Epoch 2049, Loss: 0.013605592772364616\n",
      "Epoch 2050, Loss: 0.024882778525352478\n",
      "Epoch 2051, Loss: 0.013628378510475159\n",
      "Epoch 2052, Loss: 0.01283203437924385\n",
      "Epoch 2053, Loss: 0.008523507975041866\n",
      "Epoch 2054, Loss: 0.007547836285084486\n",
      "Epoch 2055, Loss: 0.01009436883032322\n",
      "Epoch 2056, Loss: 0.0076692113652825356\n",
      "Epoch 2057, Loss: 0.0071862926706671715\n",
      "Epoch 2058, Loss: 0.015853704884648323\n",
      "Epoch 2059, Loss: 0.007703935727477074\n",
      "Epoch 2060, Loss: 0.01039996650069952\n",
      "Epoch 2061, Loss: 0.027074508368968964\n",
      "Epoch 2062, Loss: 0.027591990306973457\n",
      "Epoch 2063, Loss: 0.007551691960543394\n",
      "Epoch 2064, Loss: 0.009343067184090614\n",
      "Epoch 2065, Loss: 0.00861887726932764\n",
      "Epoch 2066, Loss: 0.029413573443889618\n",
      "Epoch 2067, Loss: 0.01157714519649744\n",
      "Epoch 2068, Loss: 0.01370320562273264\n",
      "Epoch 2069, Loss: 0.008161353878676891\n",
      "Epoch 2070, Loss: 0.007323817815631628\n",
      "Epoch 2071, Loss: 0.022562764585018158\n",
      "Epoch 2072, Loss: 0.007707902230322361\n",
      "Epoch 2073, Loss: 0.01833079569041729\n",
      "Epoch 2074, Loss: 0.018401246517896652\n",
      "Epoch 2075, Loss: 0.009447581134736538\n",
      "Epoch 2076, Loss: 0.008579674176871777\n",
      "Epoch 2077, Loss: 0.030313000082969666\n",
      "Epoch 2078, Loss: 0.008859952911734581\n",
      "Epoch 2079, Loss: 0.008888131938874722\n",
      "Epoch 2080, Loss: 0.008684377186000347\n",
      "Epoch 2081, Loss: 0.00993368774652481\n",
      "Epoch 2082, Loss: 0.009438890032470226\n",
      "Epoch 2083, Loss: 0.010075208730995655\n",
      "Epoch 2084, Loss: 0.0072723617777228355\n",
      "Epoch 2085, Loss: 0.008204990066587925\n",
      "Epoch 2086, Loss: 0.008100815117359161\n",
      "Epoch 2087, Loss: 0.008541627787053585\n",
      "Epoch 2088, Loss: 0.008346868678927422\n",
      "Epoch 2089, Loss: 0.01193312555551529\n",
      "Epoch 2090, Loss: 0.007850924506783485\n",
      "Epoch 2091, Loss: 0.007795561105012894\n",
      "Epoch 2092, Loss: 0.008874748833477497\n",
      "Epoch 2093, Loss: 0.007658182177692652\n",
      "Epoch 2094, Loss: 0.020762918516993523\n",
      "Epoch 2095, Loss: 0.012713782489299774\n",
      "Epoch 2096, Loss: 0.007579925935715437\n",
      "Epoch 2097, Loss: 0.011974147520959377\n",
      "Epoch 2098, Loss: 0.015401703305542469\n",
      "Epoch 2099, Loss: 0.006827773526310921\n",
      "Epoch 2100, Loss: 0.007178497035056353\n",
      "Epoch 2101, Loss: 0.009078075177967548\n",
      "Epoch 2102, Loss: 0.01119290105998516\n",
      "Epoch 2103, Loss: 0.029865123331546783\n",
      "Epoch 2104, Loss: 0.017732327803969383\n",
      "Epoch 2105, Loss: 0.008067812770605087\n",
      "Epoch 2106, Loss: 0.007844342850148678\n",
      "Epoch 2107, Loss: 0.009968502447009087\n",
      "Epoch 2108, Loss: 0.01223011501133442\n",
      "Epoch 2109, Loss: 0.0073645771481096745\n",
      "Epoch 2110, Loss: 0.023507623001933098\n",
      "Epoch 2111, Loss: 0.00854358822107315\n",
      "Epoch 2112, Loss: 0.012002325616776943\n",
      "Epoch 2113, Loss: 0.007327171508222818\n",
      "Epoch 2114, Loss: 0.0153102558106184\n",
      "Epoch 2115, Loss: 0.011862074956297874\n",
      "Epoch 2116, Loss: 0.01581100933253765\n",
      "Epoch 2117, Loss: 0.0070204646326601505\n",
      "Epoch 2118, Loss: 0.008960823528468609\n",
      "Epoch 2119, Loss: 0.007439887151122093\n",
      "Epoch 2120, Loss: 0.008610023185610771\n",
      "Epoch 2121, Loss: 0.007394789718091488\n",
      "Epoch 2122, Loss: 0.01026865653693676\n",
      "Epoch 2123, Loss: 0.009093980304896832\n",
      "Epoch 2124, Loss: 0.01570882461965084\n",
      "Epoch 2125, Loss: 0.007283361628651619\n",
      "Epoch 2126, Loss: 0.015053494833409786\n",
      "Epoch 2127, Loss: 0.009915872476994991\n",
      "Epoch 2128, Loss: 0.007304719183593988\n",
      "Epoch 2129, Loss: 0.012078148312866688\n",
      "Epoch 2130, Loss: 0.016355231404304504\n",
      "Epoch 2131, Loss: 0.009316660463809967\n",
      "Epoch 2132, Loss: 0.0068735238164663315\n",
      "Epoch 2133, Loss: 0.007242014165967703\n",
      "Epoch 2134, Loss: 0.006746429484337568\n",
      "Epoch 2135, Loss: 0.006907797884196043\n",
      "Epoch 2136, Loss: 0.009487276896834373\n",
      "Epoch 2137, Loss: 0.010391253046691418\n",
      "Epoch 2138, Loss: 0.009154234081506729\n",
      "Epoch 2139, Loss: 0.008783326484262943\n",
      "Epoch 2140, Loss: 0.008094863966107368\n",
      "Epoch 2141, Loss: 0.009484536945819855\n",
      "Epoch 2142, Loss: 0.014619538560509682\n",
      "Epoch 2143, Loss: 0.007136174477636814\n",
      "Epoch 2144, Loss: 0.007828513160347939\n",
      "Epoch 2145, Loss: 0.009192734025418758\n",
      "Epoch 2146, Loss: 0.010412794537842274\n",
      "Epoch 2147, Loss: 0.008832409046590328\n",
      "Epoch 2148, Loss: 0.007516602054238319\n",
      "Epoch 2149, Loss: 0.01583404652774334\n",
      "Epoch 2150, Loss: 0.007882660254836082\n",
      "Epoch 2151, Loss: 0.0255995225161314\n",
      "Epoch 2152, Loss: 0.006893637124449015\n",
      "Epoch 2153, Loss: 0.008741126395761967\n",
      "Epoch 2154, Loss: 0.00805507879704237\n",
      "Epoch 2155, Loss: 0.007998847402632236\n",
      "Epoch 2156, Loss: 0.010709542781114578\n",
      "Epoch 2157, Loss: 0.007559911347925663\n",
      "Epoch 2158, Loss: 0.007958984933793545\n",
      "Epoch 2159, Loss: 0.011968845501542091\n",
      "Epoch 2160, Loss: 0.008051581680774689\n",
      "Epoch 2161, Loss: 0.007815985940396786\n",
      "Epoch 2162, Loss: 0.019278917461633682\n",
      "Epoch 2163, Loss: 0.012032672762870789\n",
      "Epoch 2164, Loss: 0.00979990791529417\n",
      "Epoch 2165, Loss: 0.007640534080564976\n",
      "Epoch 2166, Loss: 0.007575847674161196\n",
      "Epoch 2167, Loss: 0.007745738141238689\n",
      "Epoch 2168, Loss: 0.01569109596312046\n",
      "Epoch 2169, Loss: 0.007240462116897106\n",
      "Epoch 2170, Loss: 0.00808306410908699\n",
      "Epoch 2171, Loss: 0.007346032653003931\n",
      "Epoch 2172, Loss: 0.008581777103245258\n",
      "Epoch 2173, Loss: 0.00793018564581871\n",
      "Epoch 2174, Loss: 0.008105408400297165\n",
      "Epoch 2175, Loss: 0.007065256591886282\n",
      "Epoch 2176, Loss: 0.008748584426939487\n",
      "Epoch 2177, Loss: 0.0078051830641925335\n",
      "Epoch 2178, Loss: 0.0074294134974479675\n",
      "Epoch 2179, Loss: 0.0076013426296412945\n",
      "Epoch 2180, Loss: 0.006870781537145376\n",
      "Epoch 2181, Loss: 0.012029383331537247\n",
      "Epoch 2182, Loss: 0.007636589929461479\n",
      "Epoch 2183, Loss: 0.008232497610151768\n",
      "Epoch 2184, Loss: 0.007304298225790262\n",
      "Epoch 2185, Loss: 0.006926108617335558\n",
      "Epoch 2186, Loss: 0.0071259294636547565\n",
      "Epoch 2187, Loss: 0.007813476957380772\n",
      "Epoch 2188, Loss: 0.006966628599911928\n",
      "Epoch 2189, Loss: 0.00727201858535409\n",
      "Epoch 2190, Loss: 0.008698828518390656\n",
      "Epoch 2191, Loss: 0.012344373390078545\n",
      "Epoch 2192, Loss: 0.00706730131059885\n",
      "Epoch 2193, Loss: 0.013264302164316177\n",
      "Epoch 2194, Loss: 0.021935880184173584\n",
      "Epoch 2195, Loss: 0.010560492053627968\n",
      "Epoch 2196, Loss: 0.008708138950169086\n",
      "Epoch 2197, Loss: 0.007493917364627123\n",
      "Epoch 2198, Loss: 0.007574442308396101\n",
      "Epoch 2199, Loss: 0.008052022196352482\n",
      "Epoch 2200, Loss: 0.00926951039582491\n",
      "Epoch 2201, Loss: 0.006788157392293215\n",
      "Epoch 2202, Loss: 0.00755368173122406\n",
      "Epoch 2203, Loss: 0.006888107862323523\n",
      "Epoch 2204, Loss: 0.007238233461976051\n",
      "Epoch 2205, Loss: 0.007946021854877472\n",
      "Epoch 2206, Loss: 0.009413229301571846\n",
      "Epoch 2207, Loss: 0.008346827700734138\n",
      "Epoch 2208, Loss: 0.0076648942194879055\n",
      "Epoch 2209, Loss: 0.011943367309868336\n",
      "Epoch 2210, Loss: 0.006734565366059542\n",
      "Epoch 2211, Loss: 0.007522092666476965\n",
      "Epoch 2212, Loss: 0.013404262252151966\n",
      "Epoch 2213, Loss: 0.007177398540079594\n",
      "Epoch 2214, Loss: 0.007737137842923403\n",
      "Epoch 2215, Loss: 0.007126481272280216\n",
      "Epoch 2216, Loss: 0.009113905020058155\n",
      "Epoch 2217, Loss: 0.008953643962740898\n",
      "Epoch 2218, Loss: 0.007883943617343903\n",
      "Epoch 2219, Loss: 0.010035449638962746\n",
      "Epoch 2220, Loss: 0.010398555546998978\n",
      "Epoch 2221, Loss: 0.009287197142839432\n",
      "Epoch 2222, Loss: 0.012267322279512882\n",
      "Epoch 2223, Loss: 0.006755072623491287\n",
      "Epoch 2224, Loss: 0.014319178648293018\n",
      "Epoch 2225, Loss: 0.0071479640901088715\n",
      "Epoch 2226, Loss: 0.0270924624055624\n",
      "Epoch 2227, Loss: 0.007583192083984613\n",
      "Epoch 2228, Loss: 0.011050045490264893\n",
      "Epoch 2229, Loss: 0.010975007899105549\n",
      "Epoch 2230, Loss: 0.015743354335427284\n",
      "Epoch 2231, Loss: 0.0076064858585596085\n",
      "Epoch 2232, Loss: 0.0069053820334374905\n",
      "Epoch 2233, Loss: 0.006586675997823477\n",
      "Epoch 2234, Loss: 0.022408736869692802\n",
      "Epoch 2235, Loss: 0.007470998447388411\n",
      "Epoch 2236, Loss: 0.015316770412027836\n",
      "Epoch 2237, Loss: 0.011248143389821053\n",
      "Epoch 2238, Loss: 0.006643288768827915\n",
      "Epoch 2239, Loss: 0.012259281240403652\n",
      "Epoch 2240, Loss: 0.007363591808825731\n",
      "Epoch 2241, Loss: 0.006954699754714966\n",
      "Epoch 2242, Loss: 0.00943076889961958\n",
      "Epoch 2243, Loss: 0.00728379562497139\n",
      "Epoch 2244, Loss: 0.0067149014212191105\n",
      "Epoch 2245, Loss: 0.007223912514746189\n",
      "Epoch 2246, Loss: 0.00791726540774107\n",
      "Epoch 2247, Loss: 0.007520011626183987\n",
      "Epoch 2248, Loss: 0.025486556813120842\n",
      "Epoch 2249, Loss: 0.020538199692964554\n",
      "Epoch 2250, Loss: 0.027070235460996628\n",
      "Epoch 2251, Loss: 0.011919980868697166\n",
      "Epoch 2252, Loss: 0.009359403513371944\n",
      "Epoch 2253, Loss: 0.009989206679165363\n",
      "Epoch 2254, Loss: 0.007561324629932642\n",
      "Epoch 2255, Loss: 0.01748579740524292\n",
      "Epoch 2256, Loss: 0.011413782835006714\n",
      "Epoch 2257, Loss: 0.006840892136096954\n",
      "Epoch 2258, Loss: 0.017804516479372978\n",
      "Epoch 2259, Loss: 0.016306189820170403\n",
      "Epoch 2260, Loss: 0.008116185665130615\n",
      "Epoch 2261, Loss: 0.0066659278236329556\n",
      "Epoch 2262, Loss: 0.007106402888894081\n",
      "Epoch 2263, Loss: 0.011843625456094742\n",
      "Epoch 2264, Loss: 0.008067522197961807\n",
      "Epoch 2265, Loss: 0.009229828603565693\n",
      "Epoch 2266, Loss: 0.016646508127450943\n",
      "Epoch 2267, Loss: 0.006380811799317598\n",
      "Epoch 2268, Loss: 0.00987000111490488\n",
      "Epoch 2269, Loss: 0.006567441392689943\n",
      "Epoch 2270, Loss: 0.006300757639110088\n",
      "Epoch 2271, Loss: 0.011995276436209679\n",
      "Epoch 2272, Loss: 0.010663720779120922\n",
      "Epoch 2273, Loss: 0.007778504863381386\n",
      "Epoch 2274, Loss: 0.006774735637009144\n",
      "Epoch 2275, Loss: 0.015323463827371597\n",
      "Epoch 2276, Loss: 0.010804095305502415\n",
      "Epoch 2277, Loss: 0.0067232223227620125\n",
      "Epoch 2278, Loss: 0.007391646970063448\n",
      "Epoch 2279, Loss: 0.00903754960745573\n",
      "Epoch 2280, Loss: 0.007073950953781605\n",
      "Epoch 2281, Loss: 0.006719578988850117\n",
      "Epoch 2282, Loss: 0.006797685753554106\n",
      "Epoch 2283, Loss: 0.007501427084207535\n",
      "Epoch 2284, Loss: 0.008022814989089966\n",
      "Epoch 2285, Loss: 0.025846661999821663\n",
      "Epoch 2286, Loss: 0.013680371455848217\n",
      "Epoch 2287, Loss: 0.0075847492553293705\n",
      "Epoch 2288, Loss: 0.006980248726904392\n",
      "Epoch 2289, Loss: 0.009667943231761456\n",
      "Epoch 2290, Loss: 0.012691243551671505\n",
      "Epoch 2291, Loss: 0.007220694329589605\n",
      "Epoch 2292, Loss: 0.008459851145744324\n",
      "Epoch 2293, Loss: 0.009696466848254204\n",
      "Epoch 2294, Loss: 0.006892662029713392\n",
      "Epoch 2295, Loss: 0.007738213520497084\n",
      "Epoch 2296, Loss: 0.006918251048773527\n",
      "Epoch 2297, Loss: 0.0066979206167161465\n",
      "Epoch 2298, Loss: 0.006859324406832457\n",
      "Epoch 2299, Loss: 0.015035727061331272\n",
      "Epoch 2300, Loss: 0.011071578599512577\n",
      "Epoch 2301, Loss: 0.01024066936224699\n",
      "Epoch 2302, Loss: 0.00856257788836956\n",
      "Epoch 2303, Loss: 0.011015718802809715\n",
      "Epoch 2304, Loss: 0.006952375639230013\n",
      "Epoch 2305, Loss: 0.0077150603756308556\n",
      "Epoch 2306, Loss: 0.007535119075328112\n",
      "Epoch 2307, Loss: 0.008210369385778904\n",
      "Epoch 2308, Loss: 0.007286809850484133\n",
      "Epoch 2309, Loss: 0.00735851563513279\n",
      "Epoch 2310, Loss: 0.007378235459327698\n",
      "Epoch 2311, Loss: 0.020699191838502884\n",
      "Epoch 2312, Loss: 0.006210544146597385\n",
      "Epoch 2313, Loss: 0.007262897677719593\n",
      "Epoch 2314, Loss: 0.007061103358864784\n",
      "Epoch 2315, Loss: 0.015194772742688656\n",
      "Epoch 2316, Loss: 0.012245788238942623\n",
      "Epoch 2317, Loss: 0.008228608407080173\n",
      "Epoch 2318, Loss: 0.007207727525383234\n",
      "Epoch 2319, Loss: 0.009377914480865002\n",
      "Epoch 2320, Loss: 0.013039471581578255\n",
      "Epoch 2321, Loss: 0.006855039391666651\n",
      "Epoch 2322, Loss: 0.007774176076054573\n",
      "Epoch 2323, Loss: 0.006554112769663334\n",
      "Epoch 2324, Loss: 0.006851335056126118\n",
      "Epoch 2325, Loss: 0.008412769064307213\n",
      "Epoch 2326, Loss: 0.006887387949973345\n",
      "Epoch 2327, Loss: 0.006789464969187975\n",
      "Epoch 2328, Loss: 0.009065639227628708\n",
      "Epoch 2329, Loss: 0.00935414619743824\n",
      "Epoch 2330, Loss: 0.014647776260972023\n",
      "Epoch 2331, Loss: 0.007072294130921364\n",
      "Epoch 2332, Loss: 0.006849951110780239\n",
      "Epoch 2333, Loss: 0.006846436765044928\n",
      "Epoch 2334, Loss: 0.006252947263419628\n",
      "Epoch 2335, Loss: 0.008271749131381512\n",
      "Epoch 2336, Loss: 0.00733536621555686\n",
      "Epoch 2337, Loss: 0.0071623665280640125\n",
      "Epoch 2338, Loss: 0.009228434413671494\n",
      "Epoch 2339, Loss: 0.010147100314497948\n",
      "Epoch 2340, Loss: 0.008154798299074173\n",
      "Epoch 2341, Loss: 0.005831539630889893\n",
      "Epoch 2342, Loss: 0.036990489810705185\n",
      "Epoch 2343, Loss: 0.009368624538183212\n",
      "Epoch 2344, Loss: 0.006286439951509237\n",
      "Epoch 2345, Loss: 0.012454475276172161\n",
      "Epoch 2346, Loss: 0.007236775942146778\n",
      "Epoch 2347, Loss: 0.0065030851401388645\n",
      "Epoch 2348, Loss: 0.006135630887001753\n",
      "Epoch 2349, Loss: 0.006000543478876352\n",
      "Epoch 2350, Loss: 0.0066896360367536545\n",
      "Epoch 2351, Loss: 0.0067768096923828125\n",
      "Epoch 2352, Loss: 0.008721798658370972\n",
      "Epoch 2353, Loss: 0.007265827618539333\n",
      "Epoch 2354, Loss: 0.006669693626463413\n",
      "Epoch 2355, Loss: 0.01154991053044796\n",
      "Epoch 2356, Loss: 0.012890210375189781\n",
      "Epoch 2357, Loss: 0.006961113773286343\n",
      "Epoch 2358, Loss: 0.025083566084504128\n",
      "Epoch 2359, Loss: 0.006360149011015892\n",
      "Epoch 2360, Loss: 0.008360476233065128\n",
      "Epoch 2361, Loss: 0.006391812581568956\n",
      "Epoch 2362, Loss: 0.005773971322923899\n",
      "Epoch 2363, Loss: 0.007590359542518854\n",
      "Epoch 2364, Loss: 0.006303801666945219\n",
      "Epoch 2365, Loss: 0.0077349841594696045\n",
      "Epoch 2366, Loss: 0.006713500712066889\n",
      "Epoch 2367, Loss: 0.006052080076187849\n",
      "Epoch 2368, Loss: 0.006189208943396807\n",
      "Epoch 2369, Loss: 0.011659171432256699\n",
      "Epoch 2370, Loss: 0.007899579592049122\n",
      "Epoch 2371, Loss: 0.0057385931722819805\n",
      "Epoch 2372, Loss: 0.005700201727449894\n",
      "Epoch 2373, Loss: 0.007281593047082424\n",
      "Epoch 2374, Loss: 0.0073779006488621235\n",
      "Epoch 2375, Loss: 0.007916679605841637\n",
      "Epoch 2376, Loss: 0.01186846848577261\n",
      "Epoch 2377, Loss: 0.006850222125649452\n",
      "Epoch 2378, Loss: 0.015260898508131504\n",
      "Epoch 2379, Loss: 0.013528869487345219\n",
      "Epoch 2380, Loss: 0.006503707263618708\n",
      "Epoch 2381, Loss: 0.006287567317485809\n",
      "Epoch 2382, Loss: 0.00962586235255003\n",
      "Epoch 2383, Loss: 0.0075415452010929585\n",
      "Epoch 2384, Loss: 0.008334062062203884\n",
      "Epoch 2385, Loss: 0.007816716097295284\n",
      "Epoch 2386, Loss: 0.021663954481482506\n",
      "Epoch 2387, Loss: 0.006187728140503168\n",
      "Epoch 2388, Loss: 0.007980642840266228\n",
      "Epoch 2389, Loss: 0.006198371294885874\n",
      "Epoch 2390, Loss: 0.007514559663832188\n",
      "Epoch 2391, Loss: 0.024788478389382362\n",
      "Epoch 2392, Loss: 0.00960907619446516\n",
      "Epoch 2393, Loss: 0.006659396458417177\n",
      "Epoch 2394, Loss: 0.01422225683927536\n",
      "Epoch 2395, Loss: 0.007618848700076342\n",
      "Epoch 2396, Loss: 0.0066018481738865376\n",
      "Epoch 2397, Loss: 0.007193724159151316\n",
      "Epoch 2398, Loss: 0.0077070193365216255\n",
      "Epoch 2399, Loss: 0.007392695639282465\n",
      "Epoch 2400, Loss: 0.007365403696894646\n",
      "Epoch 2401, Loss: 0.009442311711609364\n",
      "Epoch 2402, Loss: 0.006565344054251909\n",
      "Epoch 2403, Loss: 0.007672185078263283\n",
      "Epoch 2404, Loss: 0.010073976591229439\n",
      "Epoch 2405, Loss: 0.0061364262364804745\n",
      "Epoch 2406, Loss: 0.013356232084333897\n",
      "Epoch 2407, Loss: 0.010449497029185295\n",
      "Epoch 2408, Loss: 0.006699253339320421\n",
      "Epoch 2409, Loss: 0.006769233383238316\n",
      "Epoch 2410, Loss: 0.008055954240262508\n",
      "Epoch 2411, Loss: 0.005883197300136089\n",
      "Epoch 2412, Loss: 0.006426029838621616\n",
      "Epoch 2413, Loss: 0.0064255972392857075\n",
      "Epoch 2414, Loss: 0.006409314461052418\n",
      "Epoch 2415, Loss: 0.007513910066336393\n",
      "Epoch 2416, Loss: 0.0062815239652991295\n",
      "Epoch 2417, Loss: 0.006221672520041466\n",
      "Epoch 2418, Loss: 0.007216838654130697\n",
      "Epoch 2419, Loss: 0.006003010552376509\n",
      "Epoch 2420, Loss: 0.016098525375127792\n",
      "Epoch 2421, Loss: 0.00969355832785368\n",
      "Epoch 2422, Loss: 0.022943874821066856\n",
      "Epoch 2423, Loss: 0.00845892634242773\n",
      "Epoch 2424, Loss: 0.0077552832663059235\n",
      "Epoch 2425, Loss: 0.0070545063354074955\n",
      "Epoch 2426, Loss: 0.006774577312171459\n",
      "Epoch 2427, Loss: 0.006219922564923763\n",
      "Epoch 2428, Loss: 0.006700119469314814\n",
      "Epoch 2429, Loss: 0.006369241513311863\n",
      "Epoch 2430, Loss: 0.0066660065203905106\n",
      "Epoch 2431, Loss: 0.006452634464949369\n",
      "Epoch 2432, Loss: 0.007291693706065416\n",
      "Epoch 2433, Loss: 0.006143876351416111\n",
      "Epoch 2434, Loss: 0.007789342664182186\n",
      "Epoch 2435, Loss: 0.0059785484336316586\n",
      "Epoch 2436, Loss: 0.005631003528833389\n",
      "Epoch 2437, Loss: 0.007612749468535185\n",
      "Epoch 2438, Loss: 0.00707199377939105\n",
      "Epoch 2439, Loss: 0.007138790097087622\n",
      "Epoch 2440, Loss: 0.006377787794917822\n",
      "Epoch 2441, Loss: 0.01373622752726078\n",
      "Epoch 2442, Loss: 0.006081890780478716\n",
      "Epoch 2443, Loss: 0.009398777969181538\n",
      "Epoch 2444, Loss: 0.006320408079773188\n",
      "Epoch 2445, Loss: 0.008540761657059193\n",
      "Epoch 2446, Loss: 0.006492442451417446\n",
      "Epoch 2447, Loss: 0.007504828739911318\n",
      "Epoch 2448, Loss: 0.005892914719879627\n",
      "Epoch 2449, Loss: 0.006434571463614702\n",
      "Epoch 2450, Loss: 0.005709034390747547\n",
      "Epoch 2451, Loss: 0.006840066518634558\n",
      "Epoch 2452, Loss: 0.012068986892700195\n",
      "Epoch 2453, Loss: 0.005674678832292557\n",
      "Epoch 2454, Loss: 0.006326694972813129\n",
      "Epoch 2455, Loss: 0.006307170260697603\n",
      "Epoch 2456, Loss: 0.007577117066830397\n",
      "Epoch 2457, Loss: 0.02130519226193428\n",
      "Epoch 2458, Loss: 0.005824446678161621\n",
      "Epoch 2459, Loss: 0.006069151218980551\n",
      "Epoch 2460, Loss: 0.006282817106693983\n",
      "Epoch 2461, Loss: 0.006023533176630735\n",
      "Epoch 2462, Loss: 0.00641190679743886\n",
      "Epoch 2463, Loss: 0.006569047458469868\n",
      "Epoch 2464, Loss: 0.0064486353658139706\n",
      "Epoch 2465, Loss: 0.005906783509999514\n",
      "Epoch 2466, Loss: 0.006187399849295616\n",
      "Epoch 2467, Loss: 0.021988948807120323\n",
      "Epoch 2468, Loss: 0.005705118179321289\n",
      "Epoch 2469, Loss: 0.006893441546708345\n",
      "Epoch 2470, Loss: 0.006620908156037331\n",
      "Epoch 2471, Loss: 0.005676210857927799\n",
      "Epoch 2472, Loss: 0.008650346659123898\n",
      "Epoch 2473, Loss: 0.0057381074875593185\n",
      "Epoch 2474, Loss: 0.0059110973961651325\n",
      "Epoch 2475, Loss: 0.006979241501539946\n",
      "Epoch 2476, Loss: 0.020267004147171974\n",
      "Epoch 2477, Loss: 0.007405764888972044\n",
      "Epoch 2478, Loss: 0.006717707961797714\n",
      "Epoch 2479, Loss: 0.006473547779023647\n",
      "Epoch 2480, Loss: 0.007254926487803459\n",
      "Epoch 2481, Loss: 0.0066423844546079636\n",
      "Epoch 2482, Loss: 0.006230595521628857\n",
      "Epoch 2483, Loss: 0.006484921555966139\n",
      "Epoch 2484, Loss: 0.006287738215178251\n",
      "Epoch 2485, Loss: 0.006093546748161316\n",
      "Epoch 2486, Loss: 0.008124571293592453\n",
      "Epoch 2487, Loss: 0.007496527396142483\n",
      "Epoch 2488, Loss: 0.006483870092779398\n",
      "Epoch 2489, Loss: 0.006333318073302507\n",
      "Epoch 2490, Loss: 0.005923039745539427\n",
      "Epoch 2491, Loss: 0.006086801644414663\n",
      "Epoch 2492, Loss: 0.007172349840402603\n",
      "Epoch 2493, Loss: 0.006227881647646427\n",
      "Epoch 2494, Loss: 0.006548434495925903\n",
      "Epoch 2495, Loss: 0.00646884273737669\n",
      "Epoch 2496, Loss: 0.007530273869633675\n",
      "Epoch 2497, Loss: 0.005493087228387594\n",
      "Epoch 2498, Loss: 0.014355890452861786\n",
      "Epoch 2499, Loss: 0.010874749161303043\n",
      "Epoch 2500, Loss: 0.006077052094042301\n",
      "Epoch 2501, Loss: 0.00625604996457696\n",
      "Epoch 2502, Loss: 0.00592048978433013\n",
      "Epoch 2503, Loss: 0.007194939069449902\n",
      "Epoch 2504, Loss: 0.008054330945014954\n",
      "Epoch 2505, Loss: 0.0063912831246852875\n",
      "Epoch 2506, Loss: 0.0077989110723137856\n",
      "Epoch 2507, Loss: 0.0071953944861888885\n",
      "Epoch 2508, Loss: 0.0058500878512859344\n",
      "Epoch 2509, Loss: 0.007948172278702259\n",
      "Epoch 2510, Loss: 0.00816042348742485\n",
      "Epoch 2511, Loss: 0.006945054978132248\n",
      "Epoch 2512, Loss: 0.007879415526986122\n",
      "Epoch 2513, Loss: 0.005706927739083767\n",
      "Epoch 2514, Loss: 0.005570226814597845\n",
      "Epoch 2515, Loss: 0.010435576550662518\n",
      "Epoch 2516, Loss: 0.005870116874575615\n",
      "Epoch 2517, Loss: 0.0088493125513196\n",
      "Epoch 2518, Loss: 0.007067990489304066\n",
      "Epoch 2519, Loss: 0.0090223029255867\n",
      "Epoch 2520, Loss: 0.009801100008189678\n",
      "Epoch 2521, Loss: 0.015172888524830341\n",
      "Epoch 2522, Loss: 0.006266794633120298\n",
      "Epoch 2523, Loss: 0.012485411018133163\n",
      "Epoch 2524, Loss: 0.007117787841707468\n",
      "Epoch 2525, Loss: 0.006137841381132603\n",
      "Epoch 2526, Loss: 0.022363906726241112\n",
      "Epoch 2527, Loss: 0.007065123412758112\n",
      "Epoch 2528, Loss: 0.005844170227646828\n",
      "Epoch 2529, Loss: 0.006059426348656416\n",
      "Epoch 2530, Loss: 0.008004807867109776\n",
      "Epoch 2531, Loss: 0.00563045172020793\n",
      "Epoch 2532, Loss: 0.010791616514325142\n",
      "Epoch 2533, Loss: 0.0075402166694402695\n",
      "Epoch 2534, Loss: 0.02234119363129139\n",
      "Epoch 2535, Loss: 0.005709561984986067\n",
      "Epoch 2536, Loss: 0.00676799938082695\n",
      "Epoch 2537, Loss: 0.00613758247345686\n",
      "Epoch 2538, Loss: 0.01052631251513958\n",
      "Epoch 2539, Loss: 0.006352732889354229\n",
      "Epoch 2540, Loss: 0.022934049367904663\n",
      "Epoch 2541, Loss: 0.01156653743237257\n",
      "Epoch 2542, Loss: 0.00585552304983139\n",
      "Epoch 2543, Loss: 0.008217587135732174\n",
      "Epoch 2544, Loss: 0.01850484311580658\n",
      "Epoch 2545, Loss: 0.006393274758011103\n",
      "Epoch 2546, Loss: 0.007107365410774946\n",
      "Epoch 2547, Loss: 0.025449451059103012\n",
      "Epoch 2548, Loss: 0.005433407612144947\n",
      "Epoch 2549, Loss: 0.006524404976516962\n",
      "Epoch 2550, Loss: 0.006328325252979994\n",
      "Epoch 2551, Loss: 0.009105888195335865\n",
      "Epoch 2552, Loss: 0.009362911805510521\n",
      "Epoch 2553, Loss: 0.006167086306959391\n",
      "Epoch 2554, Loss: 0.021455656737089157\n",
      "Epoch 2555, Loss: 0.007312241941690445\n",
      "Epoch 2556, Loss: 0.005977224558591843\n",
      "Epoch 2557, Loss: 0.005603633355349302\n",
      "Epoch 2558, Loss: 0.007057095877826214\n",
      "Epoch 2559, Loss: 0.014490054920315742\n",
      "Epoch 2560, Loss: 0.006018357817083597\n",
      "Epoch 2561, Loss: 0.005102659109979868\n",
      "Epoch 2562, Loss: 0.010058462619781494\n",
      "Epoch 2563, Loss: 0.005811726208776236\n",
      "Epoch 2564, Loss: 0.005497906822711229\n",
      "Epoch 2565, Loss: 0.006704013794660568\n",
      "Epoch 2566, Loss: 0.005350911524146795\n",
      "Epoch 2567, Loss: 0.005725945811718702\n",
      "Epoch 2568, Loss: 0.005436315201222897\n",
      "Epoch 2569, Loss: 0.010163530707359314\n",
      "Epoch 2570, Loss: 0.028854364529252052\n",
      "Epoch 2571, Loss: 0.005642409902065992\n",
      "Epoch 2572, Loss: 0.00582285737618804\n",
      "Epoch 2573, Loss: 0.005474821198731661\n",
      "Epoch 2574, Loss: 0.005361247342079878\n",
      "Epoch 2575, Loss: 0.005739707499742508\n",
      "Epoch 2576, Loss: 0.007144841365516186\n",
      "Epoch 2577, Loss: 0.00589437410235405\n",
      "Epoch 2578, Loss: 0.0053084841929376125\n",
      "Epoch 2579, Loss: 0.006678826175630093\n",
      "Epoch 2580, Loss: 0.007116497959941626\n",
      "Epoch 2581, Loss: 0.0066553885117173195\n",
      "Epoch 2582, Loss: 0.01666034199297428\n",
      "Epoch 2583, Loss: 0.0055811223573982716\n",
      "Epoch 2584, Loss: 0.015677453950047493\n",
      "Epoch 2585, Loss: 0.006454105488955975\n",
      "Epoch 2586, Loss: 0.006030308082699776\n",
      "Epoch 2587, Loss: 0.005361492279917002\n",
      "Epoch 2588, Loss: 0.008720996789634228\n",
      "Epoch 2589, Loss: 0.006399441510438919\n",
      "Epoch 2590, Loss: 0.008818607777357101\n",
      "Epoch 2591, Loss: 0.005460033193230629\n",
      "Epoch 2592, Loss: 0.006049665622413158\n",
      "Epoch 2593, Loss: 0.006228442303836346\n",
      "Epoch 2594, Loss: 0.006829352583736181\n",
      "Epoch 2595, Loss: 0.006147480104118586\n",
      "Epoch 2596, Loss: 0.007762760389596224\n",
      "Epoch 2597, Loss: 0.0066408575512468815\n",
      "Epoch 2598, Loss: 0.006449117790907621\n",
      "Epoch 2599, Loss: 0.005081002600491047\n",
      "Epoch 2600, Loss: 0.007555629126727581\n",
      "Epoch 2601, Loss: 0.005358039401471615\n",
      "Epoch 2602, Loss: 0.006850785575807095\n",
      "Epoch 2603, Loss: 0.00713253254070878\n",
      "Epoch 2604, Loss: 0.00577961141243577\n",
      "Epoch 2605, Loss: 0.006109155248850584\n",
      "Epoch 2606, Loss: 0.006028725765645504\n",
      "Epoch 2607, Loss: 0.006309278775006533\n",
      "Epoch 2608, Loss: 0.01392243430018425\n",
      "Epoch 2609, Loss: 0.005514760501682758\n",
      "Epoch 2610, Loss: 0.00723695894703269\n",
      "Epoch 2611, Loss: 0.005074566230177879\n",
      "Epoch 2612, Loss: 0.005494304001331329\n",
      "Epoch 2613, Loss: 0.006131158675998449\n",
      "Epoch 2614, Loss: 0.02046157419681549\n",
      "Epoch 2615, Loss: 0.005580676719546318\n",
      "Epoch 2616, Loss: 0.005811725277453661\n",
      "Epoch 2617, Loss: 0.00544169032946229\n",
      "Epoch 2618, Loss: 0.005627416539937258\n",
      "Epoch 2619, Loss: 0.007546405307948589\n",
      "Epoch 2620, Loss: 0.00562756834551692\n",
      "Epoch 2621, Loss: 0.0054574389941990376\n",
      "Epoch 2622, Loss: 0.005285891704261303\n",
      "Epoch 2623, Loss: 0.006075745448470116\n",
      "Epoch 2624, Loss: 0.005276320036500692\n",
      "Epoch 2625, Loss: 0.006435243878513575\n",
      "Epoch 2626, Loss: 0.00551550043746829\n",
      "Epoch 2627, Loss: 0.0052359565161168575\n",
      "Epoch 2628, Loss: 0.00521903345361352\n",
      "Epoch 2629, Loss: 0.0059691122733056545\n",
      "Epoch 2630, Loss: 0.017602402716875076\n",
      "Epoch 2631, Loss: 0.006194430869072676\n",
      "Epoch 2632, Loss: 0.005579054821282625\n",
      "Epoch 2633, Loss: 0.00952237844467163\n",
      "Epoch 2634, Loss: 0.008102410472929478\n",
      "Epoch 2635, Loss: 0.005756470840424299\n",
      "Epoch 2636, Loss: 0.0061567784287035465\n",
      "Epoch 2637, Loss: 0.00617643678560853\n",
      "Epoch 2638, Loss: 0.011631736531853676\n",
      "Epoch 2639, Loss: 0.009320449084043503\n",
      "Epoch 2640, Loss: 0.005330696702003479\n",
      "Epoch 2641, Loss: 0.010391068644821644\n",
      "Epoch 2642, Loss: 0.005858536809682846\n",
      "Epoch 2643, Loss: 0.005396976601332426\n",
      "Epoch 2644, Loss: 0.005885951686650515\n",
      "Epoch 2645, Loss: 0.0071425666101276875\n",
      "Epoch 2646, Loss: 0.005821111146360636\n",
      "Epoch 2647, Loss: 0.005343468859791756\n",
      "Epoch 2648, Loss: 0.006418636534363031\n",
      "Epoch 2649, Loss: 0.007882353849709034\n",
      "Epoch 2650, Loss: 0.004882515873759985\n",
      "Epoch 2651, Loss: 0.005292387213557959\n",
      "Epoch 2652, Loss: 0.00549552496522665\n",
      "Epoch 2653, Loss: 0.011384876444935799\n",
      "Epoch 2654, Loss: 0.005233759991824627\n",
      "Epoch 2655, Loss: 0.005954020656645298\n",
      "Epoch 2656, Loss: 0.0068856957368552685\n",
      "Epoch 2657, Loss: 0.005204204004257917\n",
      "Epoch 2658, Loss: 0.005179641302675009\n",
      "Epoch 2659, Loss: 0.00512858247384429\n",
      "Epoch 2660, Loss: 0.005425460170954466\n",
      "Epoch 2661, Loss: 0.005177078302949667\n",
      "Epoch 2662, Loss: 0.005710424389690161\n",
      "Epoch 2663, Loss: 0.005245761945843697\n",
      "Epoch 2664, Loss: 0.0062455832958221436\n",
      "Epoch 2665, Loss: 0.007399068679660559\n",
      "Epoch 2666, Loss: 0.005409791599959135\n",
      "Epoch 2667, Loss: 0.005453907418996096\n",
      "Epoch 2668, Loss: 0.0055549838580191135\n",
      "Epoch 2669, Loss: 0.0051772198639810085\n",
      "Epoch 2670, Loss: 0.007316931150853634\n",
      "Epoch 2671, Loss: 0.005465602967888117\n",
      "Epoch 2672, Loss: 0.005967957433313131\n",
      "Epoch 2673, Loss: 0.005200851242989302\n",
      "Epoch 2674, Loss: 0.00628241803497076\n",
      "Epoch 2675, Loss: 0.008986872620880604\n",
      "Epoch 2676, Loss: 0.006969430483877659\n",
      "Epoch 2677, Loss: 0.004964168649166822\n",
      "Epoch 2678, Loss: 0.005674001760780811\n",
      "Epoch 2679, Loss: 0.005209359340369701\n",
      "Epoch 2680, Loss: 0.005933527834713459\n",
      "Epoch 2681, Loss: 0.006481832824647427\n",
      "Epoch 2682, Loss: 0.005115528590977192\n",
      "Epoch 2683, Loss: 0.012717165052890778\n",
      "Epoch 2684, Loss: 0.006642522290349007\n",
      "Epoch 2685, Loss: 0.016261853277683258\n",
      "Epoch 2686, Loss: 0.006930111907422543\n",
      "Epoch 2687, Loss: 0.008810047060251236\n",
      "Epoch 2688, Loss: 0.006350237876176834\n",
      "Epoch 2689, Loss: 0.005631336476653814\n",
      "Epoch 2690, Loss: 0.005754637997597456\n",
      "Epoch 2691, Loss: 0.018239758908748627\n",
      "Epoch 2692, Loss: 0.005064078141003847\n",
      "Epoch 2693, Loss: 0.0066230408847332\n",
      "Epoch 2694, Loss: 0.018863307312130928\n",
      "Epoch 2695, Loss: 0.0053548566065728664\n",
      "Epoch 2696, Loss: 0.0053835175931453705\n",
      "Epoch 2697, Loss: 0.005415969528257847\n",
      "Epoch 2698, Loss: 0.00907609798014164\n",
      "Epoch 2699, Loss: 0.006230796221643686\n",
      "Epoch 2700, Loss: 0.00651344982907176\n",
      "Epoch 2701, Loss: 0.013554994948208332\n",
      "Epoch 2702, Loss: 0.008778413757681847\n",
      "Epoch 2703, Loss: 0.018836382776498795\n",
      "Epoch 2704, Loss: 0.006223803386092186\n",
      "Epoch 2705, Loss: 0.005376884248107672\n",
      "Epoch 2706, Loss: 0.004769202787429094\n",
      "Epoch 2707, Loss: 0.005167733412235975\n",
      "Epoch 2708, Loss: 0.015530330128967762\n",
      "Epoch 2709, Loss: 0.007014954462647438\n",
      "Epoch 2710, Loss: 0.005243800114840269\n",
      "Epoch 2711, Loss: 0.005420795641839504\n",
      "Epoch 2712, Loss: 0.005428988486528397\n",
      "Epoch 2713, Loss: 0.005113620311021805\n",
      "Epoch 2714, Loss: 0.005492547992616892\n",
      "Epoch 2715, Loss: 0.006196803413331509\n",
      "Epoch 2716, Loss: 0.005369237158447504\n",
      "Epoch 2717, Loss: 0.005983408540487289\n",
      "Epoch 2718, Loss: 0.01370210014283657\n",
      "Epoch 2719, Loss: 0.005433569196611643\n",
      "Epoch 2720, Loss: 0.005163058638572693\n",
      "Epoch 2721, Loss: 0.0072144679725170135\n",
      "Epoch 2722, Loss: 0.0053034694865345955\n",
      "Epoch 2723, Loss: 0.005925128702074289\n",
      "Epoch 2724, Loss: 0.005921675357967615\n",
      "Epoch 2725, Loss: 0.005706927739083767\n",
      "Epoch 2726, Loss: 0.0057188416831195354\n",
      "Epoch 2727, Loss: 0.005431349854916334\n",
      "Epoch 2728, Loss: 0.013258926570415497\n",
      "Epoch 2729, Loss: 0.02498682215809822\n",
      "Epoch 2730, Loss: 0.007608149200677872\n",
      "Epoch 2731, Loss: 0.007343648467212915\n",
      "Epoch 2732, Loss: 0.005738629959523678\n",
      "Epoch 2733, Loss: 0.007609653752297163\n",
      "Epoch 2734, Loss: 0.006004449911415577\n",
      "Epoch 2735, Loss: 0.005095181055366993\n",
      "Epoch 2736, Loss: 0.0059304009191691875\n",
      "Epoch 2737, Loss: 0.005799389444291592\n",
      "Epoch 2738, Loss: 0.006298001855611801\n",
      "Epoch 2739, Loss: 0.005328943487256765\n",
      "Epoch 2740, Loss: 0.005478119011968374\n",
      "Epoch 2741, Loss: 0.008977580815553665\n",
      "Epoch 2742, Loss: 0.009748870506882668\n",
      "Epoch 2743, Loss: 0.005063969641923904\n",
      "Epoch 2744, Loss: 0.006382076069712639\n",
      "Epoch 2745, Loss: 0.004981965757906437\n",
      "Epoch 2746, Loss: 0.0053496952168643475\n",
      "Epoch 2747, Loss: 0.008010073564946651\n",
      "Epoch 2748, Loss: 0.0051137590780854225\n",
      "Epoch 2749, Loss: 0.005276938900351524\n",
      "Epoch 2750, Loss: 0.005600799806416035\n",
      "Epoch 2751, Loss: 0.0063833328895270824\n",
      "Epoch 2752, Loss: 0.009026758372783661\n",
      "Epoch 2753, Loss: 0.0051506818272173405\n",
      "Epoch 2754, Loss: 0.005557542201131582\n",
      "Epoch 2755, Loss: 0.008032034151256084\n",
      "Epoch 2756, Loss: 0.008549122139811516\n",
      "Epoch 2757, Loss: 0.019304445013403893\n",
      "Epoch 2758, Loss: 0.005433965008705854\n",
      "Epoch 2759, Loss: 0.010452778078615665\n",
      "Epoch 2760, Loss: 0.005340137984603643\n",
      "Epoch 2761, Loss: 0.0049368953332304955\n",
      "Epoch 2762, Loss: 0.023236779496073723\n",
      "Epoch 2763, Loss: 0.015557607635855675\n",
      "Epoch 2764, Loss: 0.005149985197931528\n",
      "Epoch 2765, Loss: 0.0048077041283249855\n",
      "Epoch 2766, Loss: 0.004889342002570629\n",
      "Epoch 2767, Loss: 0.0061614071018993855\n",
      "Epoch 2768, Loss: 0.00664141308516264\n",
      "Epoch 2769, Loss: 0.006081469357013702\n",
      "Epoch 2770, Loss: 0.00614384887740016\n",
      "Epoch 2771, Loss: 0.007129213772714138\n",
      "Epoch 2772, Loss: 0.005714019760489464\n",
      "Epoch 2773, Loss: 0.004703281447291374\n",
      "Epoch 2774, Loss: 0.00518837571144104\n",
      "Epoch 2775, Loss: 0.006222225725650787\n",
      "Epoch 2776, Loss: 0.006126741878688335\n",
      "Epoch 2777, Loss: 0.011054660193622112\n",
      "Epoch 2778, Loss: 0.006602047476917505\n",
      "Epoch 2779, Loss: 0.004931833129376173\n",
      "Epoch 2780, Loss: 0.007923662662506104\n",
      "Epoch 2781, Loss: 0.006884975358843803\n",
      "Epoch 2782, Loss: 0.006467652507126331\n",
      "Epoch 2783, Loss: 0.005519824102520943\n",
      "Epoch 2784, Loss: 0.0051855966448783875\n",
      "Epoch 2785, Loss: 0.008125940337777138\n",
      "Epoch 2786, Loss: 0.00688606221228838\n",
      "Epoch 2787, Loss: 0.004890748765319586\n",
      "Epoch 2788, Loss: 0.005918815266340971\n",
      "Epoch 2789, Loss: 0.009083600714802742\n",
      "Epoch 2790, Loss: 0.005917135160416365\n",
      "Epoch 2791, Loss: 0.007868272252380848\n",
      "Epoch 2792, Loss: 0.005020575597882271\n",
      "Epoch 2793, Loss: 0.007039802614599466\n",
      "Epoch 2794, Loss: 0.005406154319643974\n",
      "Epoch 2795, Loss: 0.0052143787033855915\n",
      "Epoch 2796, Loss: 0.008493117988109589\n",
      "Epoch 2797, Loss: 0.006292072124779224\n",
      "Epoch 2798, Loss: 0.005153533071279526\n",
      "Epoch 2799, Loss: 0.005367191042751074\n",
      "Epoch 2800, Loss: 0.00890781357884407\n",
      "Epoch 2801, Loss: 0.0065267556346952915\n",
      "Epoch 2802, Loss: 0.00505670765414834\n",
      "Epoch 2803, Loss: 0.004943441599607468\n",
      "Epoch 2804, Loss: 0.015058769844472408\n",
      "Epoch 2805, Loss: 0.0048840963281691074\n",
      "Epoch 2806, Loss: 0.005214774515479803\n",
      "Epoch 2807, Loss: 0.006474022753536701\n",
      "Epoch 2808, Loss: 0.005087957251816988\n",
      "Epoch 2809, Loss: 0.008648284710943699\n",
      "Epoch 2810, Loss: 0.013815748505294323\n",
      "Epoch 2811, Loss: 0.004997360520064831\n",
      "Epoch 2812, Loss: 0.004736295435577631\n",
      "Epoch 2813, Loss: 0.0050573088228702545\n",
      "Epoch 2814, Loss: 0.00568007817491889\n",
      "Epoch 2815, Loss: 0.0052975076250731945\n",
      "Epoch 2816, Loss: 0.005471355747431517\n",
      "Epoch 2817, Loss: 0.006584745366126299\n",
      "Epoch 2818, Loss: 0.006135913077741861\n",
      "Epoch 2819, Loss: 0.005756256636232138\n",
      "Epoch 2820, Loss: 0.006258320529013872\n",
      "Epoch 2821, Loss: 0.004911061376333237\n",
      "Epoch 2822, Loss: 0.005293570458889008\n",
      "Epoch 2823, Loss: 0.00513126514852047\n",
      "Epoch 2824, Loss: 0.005742583889514208\n",
      "Epoch 2825, Loss: 0.0048817540518939495\n",
      "Epoch 2826, Loss: 0.005132507998496294\n",
      "Epoch 2827, Loss: 0.005950552877038717\n",
      "Epoch 2828, Loss: 0.0068811397068202496\n",
      "Epoch 2829, Loss: 0.0058647398836910725\n",
      "Epoch 2830, Loss: 0.00533880852162838\n",
      "Epoch 2831, Loss: 0.005547800567001104\n",
      "Epoch 2832, Loss: 0.006037839222699404\n",
      "Epoch 2833, Loss: 0.005113285034894943\n",
      "Epoch 2834, Loss: 0.004796501249074936\n",
      "Epoch 2835, Loss: 0.006476195063441992\n",
      "Epoch 2836, Loss: 0.00513890665024519\n",
      "Epoch 2837, Loss: 0.004636942874640226\n",
      "Epoch 2838, Loss: 0.005135387182235718\n",
      "Epoch 2839, Loss: 0.004781489260494709\n",
      "Epoch 2840, Loss: 0.0047273412346839905\n",
      "Epoch 2841, Loss: 0.0052612354047596455\n",
      "Epoch 2842, Loss: 0.0052909525111317635\n",
      "Epoch 2843, Loss: 0.005998605862259865\n",
      "Epoch 2844, Loss: 0.004996861331164837\n",
      "Epoch 2845, Loss: 0.004416126757860184\n",
      "Epoch 2846, Loss: 0.004810210317373276\n",
      "Epoch 2847, Loss: 0.00898625049740076\n",
      "Epoch 2848, Loss: 0.006698505952954292\n",
      "Epoch 2849, Loss: 0.004512191284447908\n",
      "Epoch 2850, Loss: 0.00528771011158824\n",
      "Epoch 2851, Loss: 0.011465645395219326\n",
      "Epoch 2852, Loss: 0.0061538200825452805\n",
      "Epoch 2853, Loss: 0.005928519181907177\n",
      "Epoch 2854, Loss: 0.005794015247374773\n",
      "Epoch 2855, Loss: 0.005249310750514269\n",
      "Epoch 2856, Loss: 0.00646204361692071\n",
      "Epoch 2857, Loss: 0.004759487695991993\n",
      "Epoch 2858, Loss: 0.00940562505275011\n",
      "Epoch 2859, Loss: 0.00813901424407959\n",
      "Epoch 2860, Loss: 0.006127828732132912\n",
      "Epoch 2861, Loss: 0.004907711409032345\n",
      "Epoch 2862, Loss: 0.006332229357212782\n",
      "Epoch 2863, Loss: 0.026854654774069786\n",
      "Epoch 2864, Loss: 0.005059545394033194\n",
      "Epoch 2865, Loss: 0.0056498427875339985\n",
      "Epoch 2866, Loss: 0.0057173906825482845\n",
      "Epoch 2867, Loss: 0.005367101635783911\n",
      "Epoch 2868, Loss: 0.005407208576798439\n",
      "Epoch 2869, Loss: 0.006794777698814869\n",
      "Epoch 2870, Loss: 0.004341550637036562\n",
      "Epoch 2871, Loss: 0.004968963097780943\n",
      "Epoch 2872, Loss: 0.005090375896543264\n",
      "Epoch 2873, Loss: 0.007095418404787779\n",
      "Epoch 2874, Loss: 0.00805094838142395\n",
      "Epoch 2875, Loss: 0.006279557943344116\n",
      "Epoch 2876, Loss: 0.018896661698818207\n",
      "Epoch 2877, Loss: 0.0059799193404614925\n",
      "Epoch 2878, Loss: 0.00497044064104557\n",
      "Epoch 2879, Loss: 0.005021578632295132\n",
      "Epoch 2880, Loss: 0.004636506550014019\n",
      "Epoch 2881, Loss: 0.007177163381129503\n",
      "Epoch 2882, Loss: 0.004736357368528843\n",
      "Epoch 2883, Loss: 0.004509251099079847\n",
      "Epoch 2884, Loss: 0.004884644877165556\n",
      "Epoch 2885, Loss: 0.006880569271743298\n",
      "Epoch 2886, Loss: 0.00565373245626688\n",
      "Epoch 2887, Loss: 0.020525839179754257\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\peft\\peft_model.py:812\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    811\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_base_model()(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1890\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1885\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1886\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1887\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1888\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1890\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1902\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1904\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Trening modelu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wyniki klasyfikacji:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.77        16\n",
      "           1       0.91      0.84      0.87        37\n",
      "           2       0.10      0.50      0.17         2\n",
      "           3       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.78        65\n",
      "   macro avg       0.71      0.72      0.67        65\n",
      "weighted avg       0.89      0.78      0.82        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ewaluacja\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        for i in range(len(preds)):\n",
    "            pred = preds[i][batch['attention_mask'][i] == 1]\n",
    "            label = labels[i][batch['attention_mask'][i] == 1]\n",
    "            \n",
    "            pred = pred[label != -100]\n",
    "            label = label[label != -100]\n",
    "            \n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            true_labels.extend(label.cpu().numpy())\n",
    "\n",
    "# Wyświetlenie wyników dla zbioru testowego\n",
    "print(\"\\nWyniki klasyfikacji:\")\n",
    "print(classification_report(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predykcje dla przykładowych zdań ze zbioru testowego\n",
    "label_mapping = {0: 'negatywny', 1: 'neutralny', 2: 'pozytywny', 3: 'inne'}\n",
    "\n",
    "def predict_sentence(sentence_words):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            sentence_words,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        ).to(device)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        word_predictions = []\n",
    "        word_ids = inputs.word_ids()\n",
    "        \n",
    "        current_word = None\n",
    "        current_predictions = []\n",
    "        \n",
    "        for token_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "            if word_idx != current_word:\n",
    "                if current_word is not None:\n",
    "                    # Wybierz najczęstszą predykcję dla słowa\n",
    "                    word_predictions.append(max(set(current_predictions), key=current_predictions.count))\n",
    "                current_word = word_idx\n",
    "                current_predictions = []\n",
    "            current_predictions.append(predictions[0][token_idx].item())\n",
    "        \n",
    "        # Dodaj ostatnie słowo\n",
    "        if current_predictions:\n",
    "            word_predictions.append(max(set(current_predictions), key=current_predictions.count))\n",
    "            \n",
    "        return word_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Przykładowe predykcje dla zdań ze zbioru testowego:\n",
      "\n",
      "Zdanie 1:\n",
      "Słowo: Jakość          Predykcja: neutralny\n",
      "Słowo: i               Predykcja: inne\n",
      "Słowo: praktyczność    Predykcja: pozytywny\n",
      "Słowo: wykonania       Predykcja: pozytywny\n",
      "Słowo: tego            Predykcja: inne\n",
      "Słowo: trymera         Predykcja: neutralny\n",
      "Słowo: pozostawia      Predykcja: negatywny\n",
      "Słowo: naprawdę        Predykcja: negatywny\n",
      "Słowo: wiele           Predykcja: negatywny\n",
      "Słowo: do              Predykcja: negatywny\n",
      "Słowo: życzenia        Predykcja: negatywny\n",
      "Słowo: O               Predykcja: inne\n",
      "Słowo: golarce         Predykcja: neutralny\n",
      "Słowo: w               Predykcja: inne\n",
      "Słowo: tym             Predykcja: inne\n",
      "Słowo: zestawie        Predykcja: neutralny\n",
      "Słowo: nie             Predykcja: negatywny\n",
      "Słowo: warto           Predykcja: negatywny\n",
      "Słowo: nawet           Predykcja: negatywny\n",
      "Słowo: wspominać       Predykcja: negatywny\n",
      "Słowo: Lepiej          Predykcja: neutralny\n",
      "Słowo: od              Predykcja: inne\n",
      "Słowo: razu            Predykcja: neutralny\n",
      "Słowo: ja              Predykcja: inne\n",
      "Słowo: wyrzucić        Predykcja: negatywny\n",
      "Słowo: Za              Predykcja: inne\n",
      "Słowo: połowę          Predykcja: neutralny\n",
      "Słowo: ceny            Predykcja: neutralny\n",
      "Słowo: można           Predykcja: neutralny\n",
      "Słowo: kupić           Predykcja: neutralny\n",
      "Słowo: nieco           Predykcja: neutralny\n",
      "Słowo: lepszy          Predykcja: pozytywny\n",
      "Słowo: produkt         Predykcja: neutralny\n",
      "Słowo: produkowany     Predykcja: neutralny\n",
      "Słowo: dla             Predykcja: inne\n",
      "Słowo: marketów        Predykcja: neutralny\n",
      "Słowo: jak             Predykcja: inne\n",
      "Słowo: np              Predykcja: neutralny\n",
      "Słowo: Lidl            Predykcja: neutralny\n",
      "Słowo: Jednym          Predykcja: neutralny\n",
      "Słowo: słowem          Predykcja: neutralny\n",
      "Słowo: warto           Predykcja: neutralny\n",
      "Słowo: dołożyć         Predykcja: neutralny\n",
      "Słowo: kilkanaście     Predykcja: neutralny\n",
      "Słowo: złotych         Predykcja: neutralny\n",
      "Słowo: i               Predykcja: inne\n",
      "Słowo: zakupić         Predykcja: neutralny\n",
      "Słowo: np              Predykcja: neutralny\n",
      "Słowo: Philipsa        Predykcja: neutralny\n",
      "Słowo: Różnica         Predykcja: neutralny\n",
      "Słowo: kolosalna       Predykcja: pozytywny\n",
      "Słowo: Nie             Predykcja: pozytywny\n",
      "Słowo: polecam         Predykcja: pozytywny\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nPrzykładowe predykcje dla zdań ze zbioru testowego:\")\n",
    "for i in range(min(3, len(test_texts))):  # Pokazujemy pierwsze 3 zdania\n",
    "    sentence = test_texts[i]\n",
    "    predictions = predict_sentence(sentence)\n",
    "    \n",
    "    print(f\"\\nZdanie {i+1}:\")\n",
    "    for word, pred in zip(sentence, predictions):\n",
    "        pred_label = label_mapping[pred]\n",
    "        print(f\"Słowo: {word:15} Predykcja: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zapisano w: bert_model/bert_with_peft_model\n",
      "Tokenizer zapisano w: bert_model/bert_tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Ścieżki do zapisu\n",
    "model_save_path = \"bert_model/bert_with_peft_model\"\n",
    "tokenizer_save_path = \"bert_model/bert_tokenizer\"\n",
    "\n",
    "# Zapis modelu\n",
    "model.save_pretrained(model_save_path)\n",
    "print(f\"Model zapisano w: {model_save_path}\")\n",
    "\n",
    "# Zapis tokenizera\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "print(f\"Tokenizer zapisano w: {tokenizer_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
