{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2TokenizerFast, GPT2ForTokenClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Stałe\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LEN = 128\n",
    "TRAIN_SIZE = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wczytanie i przygotowanie danych\n",
    "df = pd.read_csv('data/annotations_all_batches - WORD - SECOND BATCH.csv')\n",
    "df = df.fillna(method='ffill')\n",
    "\n",
    "# Grupowanie po sentence_id\n",
    "sentences = df.groupby('sentence_id').agg({\n",
    "    'word': lambda x: list(x),\n",
    "    'final-annotation': lambda x: list(x)\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa dataset\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        # Tokenizacja\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Dostosowanie etykiet do tokenów\n",
    "        word_ids = encoding.word_ids()\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(labels[word_id])\n",
    "                \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label_ids)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Przygotowanie danych\n",
    "texts = sentences['word'].values\n",
    "labels = sentences['final-annotation'].values\n",
    "\n",
    "# Podział na zbiór treningowy i testowy\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, train_size=TRAIN_SIZE, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inicjalizacja tokenizera i modelu\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', add_prefix_space=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ustawienie tokenu pad jako eos dla GPT-2\n",
    "# przypisać -100 tam gdzie jest eos\n",
    "model = GPT2ForTokenClassification.from_pretrained(\n",
    "    'gpt2',\n",
    "    num_labels=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Przygotowanie datasetów\n",
    "train_dataset = TokenClassificationDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "test_dataset = TokenClassificationDataset(test_texts, test_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.030292129144072533\n",
      "Epoch 2, Loss: 0.03399297967553139\n",
      "Epoch 3, Loss: 0.039301272481679916\n",
      "Epoch 4, Loss: 0.0375821627676487\n",
      "Epoch 5, Loss: 0.03289886564016342\n",
      "Epoch 6, Loss: 0.025759031996130943\n",
      "Epoch 7, Loss: 0.04060889407992363\n",
      "Epoch 8, Loss: 0.034197308123111725\n",
      "Epoch 9, Loss: 0.03520038351416588\n",
      "Epoch 10, Loss: 0.05086660757660866\n",
      "Epoch 11, Loss: 0.04610062390565872\n",
      "Epoch 12, Loss: 0.03443412110209465\n",
      "Epoch 13, Loss: 0.03022284060716629\n",
      "Epoch 14, Loss: 0.043960586190223694\n",
      "Epoch 15, Loss: 0.041107144206762314\n",
      "Epoch 16, Loss: 0.02598210796713829\n",
      "Epoch 17, Loss: 0.031245170161128044\n",
      "Epoch 18, Loss: 0.03085775300860405\n",
      "Epoch 19, Loss: 0.04445182532072067\n",
      "Epoch 20, Loss: 0.017824500799179077\n",
      "Epoch 21, Loss: 0.025048833340406418\n",
      "Epoch 22, Loss: 0.023432299494743347\n",
      "Epoch 23, Loss: 0.03064490109682083\n",
      "Epoch 24, Loss: 0.023311764001846313\n",
      "Epoch 25, Loss: 0.016738664358854294\n",
      "Epoch 26, Loss: 0.026064004749059677\n",
      "Epoch 27, Loss: 0.01835688389837742\n",
      "Epoch 28, Loss: 0.01631103828549385\n",
      "Epoch 29, Loss: 0.028031419962644577\n",
      "Epoch 30, Loss: 0.018348772078752518\n",
      "Epoch 31, Loss: 0.033732276409864426\n",
      "Epoch 32, Loss: 0.029928380623459816\n",
      "Epoch 33, Loss: 0.017931777983903885\n",
      "Epoch 34, Loss: 0.02101239748299122\n",
      "Epoch 35, Loss: 0.013973137363791466\n",
      "Epoch 36, Loss: 0.015069754794239998\n",
      "Epoch 37, Loss: 0.01669253036379814\n",
      "Epoch 38, Loss: 0.020267486572265625\n",
      "Epoch 39, Loss: 0.01552676036953926\n",
      "Epoch 40, Loss: 0.016608329489827156\n",
      "Epoch 41, Loss: 0.012027834542095661\n",
      "Epoch 42, Loss: 0.013687871396541595\n",
      "Epoch 43, Loss: 0.011439512483775616\n",
      "Epoch 44, Loss: 0.013395891524851322\n",
      "Epoch 45, Loss: 0.01394098810851574\n",
      "Epoch 46, Loss: 0.01831657998263836\n",
      "Epoch 47, Loss: 0.020574864000082016\n",
      "Epoch 48, Loss: 0.01984507590532303\n",
      "Epoch 49, Loss: 0.018651723861694336\n",
      "Epoch 50, Loss: 0.02088334783911705\n",
      "Epoch 51, Loss: 0.01171594113111496\n",
      "Epoch 52, Loss: 0.014835123904049397\n",
      "Epoch 53, Loss: 0.008790744468569756\n",
      "Epoch 54, Loss: 0.0124887740239501\n",
      "Epoch 55, Loss: 0.0186122078448534\n",
      "Epoch 56, Loss: 0.011120540089905262\n",
      "Epoch 57, Loss: 0.015875322744250298\n",
      "Epoch 58, Loss: 0.013197051361203194\n",
      "Epoch 59, Loss: 0.009992689825594425\n",
      "Epoch 60, Loss: 0.010230130515992641\n",
      "Epoch 61, Loss: 0.01140310987830162\n",
      "Epoch 62, Loss: 0.013448004610836506\n",
      "Epoch 63, Loss: 0.01165070105344057\n",
      "Epoch 64, Loss: 0.00914640910923481\n",
      "Epoch 65, Loss: 0.011439118534326553\n",
      "Epoch 66, Loss: 0.015919305384159088\n",
      "Epoch 67, Loss: 0.015938760712742805\n",
      "Epoch 68, Loss: 0.007936996407806873\n",
      "Epoch 69, Loss: 0.009593709371984005\n",
      "Epoch 70, Loss: 0.011073779314756393\n",
      "Epoch 71, Loss: 0.014317572116851807\n",
      "Epoch 72, Loss: 0.02315310761332512\n",
      "Epoch 73, Loss: 0.01260838471353054\n",
      "Epoch 74, Loss: 0.010410143062472343\n",
      "Epoch 75, Loss: 0.013786405324935913\n",
      "Epoch 76, Loss: 0.00638714199885726\n",
      "Epoch 77, Loss: 0.004953773692250252\n",
      "Epoch 78, Loss: 0.00975301768630743\n",
      "Epoch 79, Loss: 0.009202414192259312\n",
      "Epoch 80, Loss: 0.01027371734380722\n",
      "Epoch 81, Loss: 0.0042197308503091335\n",
      "Epoch 82, Loss: 0.010839404538273811\n",
      "Epoch 83, Loss: 0.011137763038277626\n",
      "Epoch 84, Loss: 0.004727884661406279\n",
      "Epoch 85, Loss: 0.00853271409869194\n",
      "Epoch 86, Loss: 0.0123601034283638\n",
      "Epoch 87, Loss: 0.009214201010763645\n",
      "Epoch 88, Loss: 0.008511323481798172\n",
      "Epoch 89, Loss: 0.01020458247512579\n",
      "Epoch 90, Loss: 0.010416526347398758\n",
      "Epoch 91, Loss: 0.007713036611676216\n",
      "Epoch 92, Loss: 0.004428996704518795\n",
      "Epoch 93, Loss: 0.006616771686822176\n",
      "Epoch 94, Loss: 0.00896516814827919\n",
      "Epoch 95, Loss: 0.009009876288473606\n",
      "Epoch 96, Loss: 0.009658897295594215\n",
      "Epoch 97, Loss: 0.010129422880709171\n",
      "Epoch 98, Loss: 0.003856497583910823\n",
      "Epoch 99, Loss: 0.011381017044186592\n",
      "Epoch 100, Loss: 0.006843917537480593\n",
      "Epoch 101, Loss: 0.00657971017062664\n",
      "Epoch 102, Loss: 0.0032459122594445944\n",
      "Epoch 103, Loss: 0.00674149626865983\n",
      "Epoch 104, Loss: 0.004085845779627562\n",
      "Epoch 105, Loss: 0.010035951621830463\n",
      "Epoch 106, Loss: 0.009299193508923054\n",
      "Epoch 107, Loss: 0.006795106455683708\n",
      "Epoch 108, Loss: 0.0068427082151174545\n",
      "Epoch 109, Loss: 0.002268619369715452\n",
      "Epoch 110, Loss: 0.006845578551292419\n",
      "Epoch 111, Loss: 0.004161760210990906\n",
      "Epoch 112, Loss: 0.00375940278172493\n",
      "Epoch 113, Loss: 0.005620510317385197\n",
      "Epoch 114, Loss: 0.006300062872469425\n",
      "Epoch 115, Loss: 0.0055353096686303616\n",
      "Epoch 116, Loss: 0.013546728529036045\n",
      "Epoch 117, Loss: 0.014379560947418213\n",
      "Epoch 118, Loss: 0.0061831786297261715\n",
      "Epoch 119, Loss: 0.01410109456628561\n",
      "Epoch 120, Loss: 0.015772389248013496\n",
      "Epoch 121, Loss: 0.007378328591585159\n",
      "Epoch 122, Loss: 0.011785042472183704\n",
      "Epoch 123, Loss: 0.005646032281219959\n",
      "Epoch 124, Loss: 0.011268893256783485\n",
      "Epoch 125, Loss: 0.019377797842025757\n",
      "Epoch 126, Loss: 0.005272545851767063\n",
      "Epoch 127, Loss: 0.005593707785010338\n",
      "Epoch 128, Loss: 0.008932719938457012\n",
      "Epoch 129, Loss: 0.0033175565768033266\n",
      "Epoch 130, Loss: 0.0033870930783450603\n",
      "Epoch 131, Loss: 0.005814448930323124\n",
      "Epoch 132, Loss: 0.004953972529619932\n",
      "Epoch 133, Loss: 0.0075592477805912495\n",
      "Epoch 134, Loss: 0.007467832416296005\n",
      "Epoch 135, Loss: 0.005319705232977867\n",
      "Epoch 136, Loss: 0.007387238554656506\n",
      "Epoch 137, Loss: 0.00865168496966362\n",
      "Epoch 138, Loss: 0.01335191261023283\n",
      "Epoch 139, Loss: 0.010248796083033085\n",
      "Epoch 140, Loss: 0.0062320479191839695\n",
      "Epoch 141, Loss: 0.0015118838055059314\n",
      "Epoch 142, Loss: 0.0020249267108738422\n",
      "Epoch 143, Loss: 0.006825077347457409\n",
      "Epoch 144, Loss: 0.0027240151539444923\n",
      "Epoch 145, Loss: 0.007024290505796671\n",
      "Epoch 146, Loss: 0.014874082989990711\n",
      "Epoch 147, Loss: 0.004792685154825449\n",
      "Epoch 148, Loss: 0.009987400844693184\n",
      "Epoch 149, Loss: 0.00499649066478014\n",
      "Epoch 150, Loss: 0.008458674885332584\n",
      "Epoch 151, Loss: 0.007474472746253014\n",
      "Epoch 152, Loss: 0.007965664379298687\n",
      "Epoch 153, Loss: 0.009348377585411072\n",
      "Epoch 154, Loss: 0.006396234035491943\n",
      "Epoch 155, Loss: 0.00633094971999526\n",
      "Epoch 156, Loss: 0.0026893557514995337\n",
      "Epoch 157, Loss: 0.005310543347150087\n",
      "Epoch 158, Loss: 0.0036605410277843475\n",
      "Epoch 159, Loss: 0.006663549691438675\n",
      "Epoch 160, Loss: 0.004678762052208185\n",
      "Epoch 161, Loss: 0.007961692288517952\n",
      "Epoch 162, Loss: 0.0042298343032598495\n",
      "Epoch 163, Loss: 0.004593143705278635\n",
      "Epoch 164, Loss: 0.006215479224920273\n",
      "Epoch 165, Loss: 0.0011621848680078983\n",
      "Epoch 166, Loss: 0.005885090678930283\n",
      "Epoch 167, Loss: 0.0024529569782316685\n",
      "Epoch 168, Loss: 0.007672876585274935\n",
      "Epoch 169, Loss: 0.0059334030374884605\n",
      "Epoch 170, Loss: 0.004720690194517374\n",
      "Epoch 171, Loss: 0.005076445639133453\n",
      "Epoch 172, Loss: 0.004199348855763674\n",
      "Epoch 173, Loss: 0.006849002558737993\n",
      "Epoch 174, Loss: 0.005193748045712709\n",
      "Epoch 175, Loss: 0.005764768458902836\n",
      "Epoch 176, Loss: 0.0027549215592443943\n",
      "Epoch 177, Loss: 0.0026710904203355312\n",
      "Epoch 178, Loss: 0.004490261431783438\n",
      "Epoch 179, Loss: 0.010228386148810387\n",
      "Epoch 180, Loss: 0.007649807259440422\n",
      "Epoch 181, Loss: 0.012555094435811043\n",
      "Epoch 182, Loss: 0.0038725228514522314\n",
      "Epoch 183, Loss: 0.0012813132489100099\n",
      "Epoch 184, Loss: 0.0039608594961464405\n",
      "Epoch 185, Loss: 0.008696438744664192\n",
      "Epoch 186, Loss: 0.005267683882266283\n",
      "Epoch 187, Loss: 0.002596865640953183\n",
      "Epoch 188, Loss: 0.005187592003494501\n",
      "Epoch 189, Loss: 0.005247118882834911\n",
      "Epoch 190, Loss: 0.0018614259315654635\n",
      "Epoch 191, Loss: 0.0047663492150604725\n",
      "Epoch 192, Loss: 0.004308884497731924\n",
      "Epoch 193, Loss: 0.00880349986255169\n",
      "Epoch 194, Loss: 0.006391019094735384\n",
      "Epoch 195, Loss: 0.0013787620700895786\n",
      "Epoch 196, Loss: 0.00886126421391964\n",
      "Epoch 197, Loss: 0.004943118430674076\n",
      "Epoch 198, Loss: 0.005722413305193186\n",
      "Epoch 199, Loss: 0.005924356169998646\n",
      "Epoch 200, Loss: 0.004141645971685648\n",
      "Epoch 201, Loss: 0.001108562108129263\n",
      "Epoch 202, Loss: 0.002147904597222805\n",
      "Epoch 203, Loss: 0.01198241114616394\n",
      "Epoch 204, Loss: 0.007643423974514008\n",
      "Epoch 205, Loss: 0.00364093529060483\n",
      "Epoch 206, Loss: 0.011821114458143711\n",
      "Epoch 207, Loss: 0.005788519512861967\n",
      "Epoch 208, Loss: 0.00223393552005291\n",
      "Epoch 209, Loss: 0.006204380188137293\n",
      "Epoch 210, Loss: 0.008066972717642784\n",
      "Epoch 211, Loss: 0.00577345909550786\n",
      "Epoch 212, Loss: 0.013144812546670437\n",
      "Epoch 213, Loss: 0.004110280424356461\n",
      "Epoch 214, Loss: 0.003936908673495054\n",
      "Epoch 215, Loss: 0.0060041421093046665\n",
      "Epoch 216, Loss: 0.0035076665226370096\n",
      "Epoch 217, Loss: 0.004703959915786982\n",
      "Epoch 218, Loss: 0.016300136223435402\n",
      "Epoch 219, Loss: 0.007187471725046635\n",
      "Epoch 220, Loss: 0.003240939462557435\n",
      "Epoch 221, Loss: 0.007517067715525627\n",
      "Epoch 222, Loss: 0.009940446354448795\n",
      "Epoch 223, Loss: 0.0100630521774292\n",
      "Epoch 224, Loss: 0.008183714002370834\n",
      "Epoch 225, Loss: 0.009726813063025475\n",
      "Epoch 226, Loss: 0.007359328214079142\n",
      "Epoch 227, Loss: 0.0074420166201889515\n",
      "Epoch 228, Loss: 0.003629458835348487\n",
      "Epoch 229, Loss: 0.005668333265930414\n",
      "Epoch 230, Loss: 0.0037081860937178135\n",
      "Epoch 231, Loss: 0.0010153955081477761\n",
      "Epoch 232, Loss: 0.0040314048528671265\n",
      "Epoch 233, Loss: 0.004518197383731604\n",
      "Epoch 234, Loss: 0.008823346346616745\n",
      "Epoch 235, Loss: 0.005205740220844746\n",
      "Epoch 236, Loss: 0.007069142535328865\n",
      "Epoch 237, Loss: 0.0049093421548604965\n",
      "Epoch 238, Loss: 0.0022276132367551327\n",
      "Epoch 239, Loss: 0.00601555872708559\n",
      "Epoch 240, Loss: 0.007135618012398481\n",
      "Epoch 241, Loss: 0.0015606668312102556\n",
      "Epoch 242, Loss: 0.0012933691032230854\n",
      "Epoch 243, Loss: 0.006321817636489868\n",
      "Epoch 244, Loss: 0.003903939388692379\n",
      "Epoch 245, Loss: 0.010040149092674255\n",
      "Epoch 246, Loss: 0.008698585443198681\n",
      "Epoch 247, Loss: 0.0033374850172549486\n",
      "Epoch 248, Loss: 0.007460006047040224\n",
      "Epoch 249, Loss: 0.014766442589461803\n",
      "Epoch 250, Loss: 0.003930899780243635\n",
      "Epoch 251, Loss: 0.005370389204472303\n",
      "Epoch 252, Loss: 0.004817535635083914\n",
      "Epoch 253, Loss: 0.0020278594456613064\n",
      "Epoch 254, Loss: 0.014749696478247643\n",
      "Epoch 255, Loss: 0.0016975365579128265\n",
      "Epoch 256, Loss: 0.006747039966285229\n",
      "Epoch 257, Loss: 0.0025464643258601427\n",
      "Epoch 258, Loss: 0.006112589035183191\n",
      "Epoch 259, Loss: 0.004789306316524744\n",
      "Epoch 260, Loss: 0.004671894013881683\n",
      "Epoch 261, Loss: 0.001211026101373136\n",
      "Epoch 262, Loss: 0.004394202493131161\n",
      "Epoch 263, Loss: 0.000806693104095757\n",
      "Epoch 264, Loss: 0.00442610215395689\n",
      "Epoch 265, Loss: 0.005383040755987167\n",
      "Epoch 266, Loss: 0.00617756275460124\n",
      "Epoch 267, Loss: 0.0013052306603640318\n",
      "Epoch 268, Loss: 0.010276553221046925\n",
      "Epoch 269, Loss: 0.004444336984306574\n",
      "Epoch 270, Loss: 0.005226716864854097\n",
      "Epoch 271, Loss: 0.0013035415904596448\n",
      "Epoch 272, Loss: 0.005928754340857267\n",
      "Epoch 273, Loss: 0.0036633918061852455\n",
      "Epoch 274, Loss: 0.0007648404571227729\n",
      "Epoch 275, Loss: 0.003935347311198711\n",
      "Epoch 276, Loss: 0.006512027233839035\n",
      "Epoch 277, Loss: 0.011477196589112282\n",
      "Epoch 278, Loss: 0.004657742101699114\n",
      "Epoch 279, Loss: 0.003979676868766546\n",
      "Epoch 280, Loss: 0.004305018577724695\n",
      "Epoch 281, Loss: 0.0007621804252266884\n",
      "Epoch 282, Loss: 0.007450241595506668\n",
      "Epoch 283, Loss: 0.0008592972881160676\n",
      "Epoch 284, Loss: 0.005831252783536911\n",
      "Epoch 285, Loss: 0.003907344304025173\n",
      "Epoch 286, Loss: 0.00371352955698967\n",
      "Epoch 287, Loss: 0.004914402961730957\n",
      "Epoch 288, Loss: 0.003707738360390067\n",
      "Epoch 289, Loss: 0.004241937305778265\n",
      "Epoch 290, Loss: 0.003957779612392187\n",
      "Epoch 291, Loss: 0.0008092850330285728\n",
      "Epoch 292, Loss: 0.0006230770377442241\n",
      "Epoch 293, Loss: 0.0075287302024662495\n",
      "Epoch 294, Loss: 0.0010474881855770946\n",
      "Epoch 295, Loss: 0.003868668107315898\n",
      "Epoch 296, Loss: 0.0010875650914385915\n",
      "Epoch 297, Loss: 0.0007210371550172567\n",
      "Epoch 298, Loss: 0.004208519589155912\n",
      "Epoch 299, Loss: 0.000643460254650563\n",
      "Epoch 300, Loss: 0.004185485187917948\n",
      "Epoch 301, Loss: 0.009231308475136757\n",
      "Epoch 302, Loss: 0.000621214450802654\n",
      "Epoch 303, Loss: 0.006769821047782898\n",
      "Epoch 304, Loss: 0.003811076981946826\n",
      "Epoch 305, Loss: 0.012150462716817856\n",
      "Epoch 306, Loss: 0.005156215745955706\n",
      "Epoch 307, Loss: 0.0072021023370325565\n",
      "Epoch 308, Loss: 0.0038575863000005484\n",
      "Epoch 309, Loss: 0.0008507198072038591\n",
      "Epoch 310, Loss: 0.006911388598382473\n",
      "Epoch 311, Loss: 0.0037028121296316385\n",
      "Epoch 312, Loss: 0.001799052581191063\n",
      "Epoch 313, Loss: 0.00463654100894928\n",
      "Epoch 314, Loss: 0.004591438919305801\n",
      "Epoch 315, Loss: 0.0019599259831011295\n",
      "Epoch 316, Loss: 0.006213955115526915\n",
      "Epoch 317, Loss: 0.0006622732616961002\n",
      "Epoch 318, Loss: 0.00509558105841279\n",
      "Epoch 319, Loss: 0.004923964384943247\n",
      "Epoch 320, Loss: 0.002134959679096937\n",
      "Epoch 321, Loss: 0.001036571222357452\n",
      "Epoch 322, Loss: 0.0006223450764082372\n",
      "Epoch 323, Loss: 0.003841238562017679\n",
      "Epoch 324, Loss: 0.00338816549628973\n",
      "Epoch 325, Loss: 0.005153014324605465\n",
      "Epoch 326, Loss: 0.0040568090043962\n",
      "Epoch 327, Loss: 0.006978357210755348\n",
      "Epoch 328, Loss: 0.0034228949807584286\n",
      "Epoch 329, Loss: 0.0007090670987963676\n",
      "Epoch 330, Loss: 0.0064115384593605995\n",
      "Epoch 331, Loss: 0.0012829347979277372\n",
      "Epoch 332, Loss: 0.003717114683240652\n",
      "Epoch 333, Loss: 0.0014387591509148479\n",
      "Epoch 334, Loss: 0.0036884292494505644\n",
      "Epoch 335, Loss: 0.006384780630469322\n",
      "Epoch 336, Loss: 0.0035573665518313646\n",
      "Epoch 337, Loss: 0.0006727560539729893\n",
      "Epoch 338, Loss: 0.008194010704755783\n",
      "Epoch 339, Loss: 0.0004997790674678981\n",
      "Epoch 340, Loss: 0.0036887568421661854\n",
      "Epoch 341, Loss: 0.005588726606220007\n",
      "Epoch 342, Loss: 0.005981443449854851\n",
      "Epoch 343, Loss: 0.001190040959045291\n",
      "Epoch 344, Loss: 0.0017644099425524473\n",
      "Epoch 345, Loss: 0.0026282460894435644\n",
      "Epoch 346, Loss: 0.006239826325327158\n",
      "Epoch 347, Loss: 0.006554414518177509\n",
      "Epoch 348, Loss: 0.0004781937168445438\n",
      "Epoch 349, Loss: 0.0030356980860233307\n",
      "Epoch 350, Loss: 0.005839629098773003\n",
      "Epoch 351, Loss: 0.0036433131899684668\n",
      "Epoch 352, Loss: 0.003934780601412058\n",
      "Epoch 353, Loss: 0.0035935125779360533\n",
      "Epoch 354, Loss: 0.00037339405389502645\n",
      "Epoch 355, Loss: 0.0005009043379686773\n",
      "Epoch 356, Loss: 0.0061064534820616245\n",
      "Epoch 357, Loss: 0.003500505117699504\n",
      "Epoch 358, Loss: 0.0008149819914251566\n",
      "Epoch 359, Loss: 0.0035254096146672964\n",
      "Epoch 360, Loss: 0.010402437299489975\n",
      "Epoch 361, Loss: 0.002618787344545126\n",
      "Epoch 362, Loss: 0.00423866743221879\n",
      "Epoch 363, Loss: 0.0008598026470281184\n",
      "Epoch 364, Loss: 0.0040984670631587505\n",
      "Epoch 365, Loss: 0.0030691244173794985\n",
      "Epoch 366, Loss: 0.0018401024863123894\n",
      "Epoch 367, Loss: 0.009243981912732124\n",
      "Epoch 368, Loss: 0.001134707243181765\n",
      "Epoch 369, Loss: 0.0006645343964919448\n",
      "Epoch 370, Loss: 0.0030091917142271996\n",
      "Epoch 371, Loss: 0.003185017965734005\n",
      "Epoch 372, Loss: 0.006134971044957638\n",
      "Epoch 373, Loss: 0.008507655933499336\n",
      "Epoch 374, Loss: 0.006184097845107317\n",
      "Epoch 375, Loss: 0.0007937320624478161\n",
      "Epoch 376, Loss: 0.005266376305371523\n",
      "Epoch 377, Loss: 0.00098701030947268\n",
      "Epoch 378, Loss: 0.0028969370760023594\n",
      "Epoch 379, Loss: 0.0033642619382590055\n",
      "Epoch 380, Loss: 0.0031113612931221724\n",
      "Epoch 381, Loss: 0.0005106572061777115\n",
      "Epoch 382, Loss: 0.003245730884373188\n",
      "Epoch 383, Loss: 0.0004220431437715888\n",
      "Epoch 384, Loss: 0.0060898978263139725\n",
      "Epoch 385, Loss: 0.003547553438693285\n",
      "Epoch 386, Loss: 0.0008792836451902986\n",
      "Epoch 387, Loss: 0.0019400171004235744\n",
      "Epoch 388, Loss: 0.003794015385210514\n",
      "Epoch 389, Loss: 0.003440276486799121\n",
      "Epoch 390, Loss: 0.000320519000524655\n",
      "Epoch 391, Loss: 0.002867632545530796\n",
      "Epoch 392, Loss: 0.00458559300750494\n",
      "Epoch 393, Loss: 0.008583866991102695\n",
      "Epoch 394, Loss: 0.006023356691002846\n",
      "Epoch 395, Loss: 0.004147592466324568\n",
      "Epoch 396, Loss: 0.0037173882592469454\n",
      "Epoch 397, Loss: 0.006533535197377205\n",
      "Epoch 398, Loss: 0.000476511922897771\n",
      "Epoch 399, Loss: 0.0061104791238904\n",
      "Epoch 400, Loss: 0.0005874851485714316\n",
      "Epoch 401, Loss: 0.005345942452549934\n",
      "Epoch 402, Loss: 0.003295778064057231\n",
      "Epoch 403, Loss: 0.000958594202529639\n",
      "Epoch 404, Loss: 0.003211003029718995\n",
      "Epoch 405, Loss: 0.003178615588694811\n",
      "Epoch 406, Loss: 0.000695857685059309\n",
      "Epoch 407, Loss: 0.0032558843959122896\n",
      "Epoch 408, Loss: 0.0034852016251534224\n",
      "Epoch 409, Loss: 0.003428297583013773\n",
      "Epoch 410, Loss: 0.00026704726042225957\n",
      "Epoch 411, Loss: 0.003283788450062275\n",
      "Epoch 412, Loss: 0.0004133933980483562\n",
      "Epoch 413, Loss: 0.008569854311645031\n",
      "Epoch 414, Loss: 0.003564999671652913\n",
      "Epoch 415, Loss: 0.005955918692052364\n",
      "Epoch 416, Loss: 0.0008012065663933754\n",
      "Epoch 417, Loss: 0.0029503663536161184\n",
      "Epoch 418, Loss: 0.0034612722229212523\n",
      "Epoch 419, Loss: 0.001061687828041613\n",
      "Epoch 420, Loss: 0.0006577782332897186\n",
      "Epoch 421, Loss: 0.005824822932481766\n",
      "Epoch 422, Loss: 0.0034283793065696955\n",
      "Epoch 423, Loss: 0.0029677902348339558\n",
      "Epoch 424, Loss: 0.003085322678089142\n",
      "Epoch 425, Loss: 0.0006121337064541876\n",
      "Epoch 426, Loss: 0.0030283755622804165\n",
      "Epoch 427, Loss: 0.0007596377399750054\n",
      "Epoch 428, Loss: 0.0006687896675430238\n",
      "Epoch 429, Loss: 0.0027953553944826126\n",
      "Epoch 430, Loss: 0.0002968170156236738\n",
      "Epoch 431, Loss: 0.0011044112034142017\n",
      "Epoch 432, Loss: 0.00035823648795485497\n",
      "Epoch 433, Loss: 0.0032783704809844494\n",
      "Epoch 434, Loss: 0.0004153673071414232\n",
      "Epoch 435, Loss: 0.0042226845398545265\n",
      "Epoch 436, Loss: 0.003236739430576563\n",
      "Epoch 437, Loss: 0.00023940333630889654\n",
      "Epoch 438, Loss: 0.0003432655648794025\n",
      "Epoch 439, Loss: 0.006064302287995815\n",
      "Epoch 440, Loss: 0.000610348884947598\n",
      "Epoch 441, Loss: 0.004652042407542467\n",
      "Epoch 442, Loss: 0.0005596723640337586\n",
      "Epoch 443, Loss: 0.0004978520446456969\n",
      "Epoch 444, Loss: 0.00682927668094635\n",
      "Epoch 445, Loss: 0.0018391030607745051\n",
      "Epoch 446, Loss: 0.003051408566534519\n",
      "Epoch 447, Loss: 0.00038522540125995874\n",
      "Epoch 448, Loss: 0.0035287360660731792\n",
      "Epoch 449, Loss: 0.00024253549054265022\n",
      "Epoch 450, Loss: 0.001748212962411344\n",
      "Epoch 451, Loss: 0.0064714993350207806\n",
      "Epoch 452, Loss: 0.0036656635347753763\n",
      "Epoch 453, Loss: 0.0008736727759242058\n",
      "Epoch 454, Loss: 0.00034475544816814363\n",
      "Epoch 455, Loss: 0.0025210496969521046\n",
      "Epoch 456, Loss: 0.0005475687794387341\n",
      "Epoch 457, Loss: 0.001699162763543427\n",
      "Epoch 458, Loss: 0.002970116911455989\n",
      "Epoch 459, Loss: 0.009341209195554256\n",
      "Epoch 460, Loss: 0.0014618283603340387\n",
      "Epoch 461, Loss: 0.00332246208563447\n",
      "Epoch 462, Loss: 0.0029578222893178463\n",
      "Epoch 463, Loss: 0.002536789746955037\n",
      "Epoch 464, Loss: 0.0008674284908920527\n",
      "Epoch 465, Loss: 0.00063078582752496\n",
      "Epoch 466, Loss: 0.003495126264169812\n",
      "Epoch 467, Loss: 0.0002718852774705738\n",
      "Epoch 468, Loss: 0.0004294125537853688\n",
      "Epoch 469, Loss: 0.005235788878053427\n",
      "Epoch 470, Loss: 0.00343010644428432\n",
      "Epoch 471, Loss: 0.003848698688670993\n",
      "Epoch 472, Loss: 0.002838053274899721\n",
      "Epoch 473, Loss: 0.0002710650733206421\n",
      "Epoch 474, Loss: 0.005597314797341824\n",
      "Epoch 475, Loss: 0.006918205879628658\n",
      "Epoch 476, Loss: 0.004304538946598768\n",
      "Epoch 477, Loss: 0.00041628992767073214\n",
      "Epoch 478, Loss: 0.005780356470495462\n",
      "Epoch 479, Loss: 0.0031946846283972263\n",
      "Epoch 480, Loss: 0.0003956639557145536\n",
      "Epoch 481, Loss: 0.00038976900395937264\n",
      "Epoch 482, Loss: 0.00208115903660655\n",
      "Epoch 483, Loss: 0.0031990387942641973\n",
      "Epoch 484, Loss: 0.0059195226058363914\n",
      "Epoch 485, Loss: 0.0004909995477646589\n",
      "Epoch 486, Loss: 0.00040658630314283073\n",
      "Epoch 487, Loss: 0.0033150874078273773\n",
      "Epoch 488, Loss: 0.008699029684066772\n",
      "Epoch 489, Loss: 0.0014935757499188185\n",
      "Epoch 490, Loss: 0.0031954580917954445\n",
      "Epoch 491, Loss: 0.001102954032830894\n",
      "Epoch 492, Loss: 0.00018889442435465753\n",
      "Epoch 493, Loss: 0.0035689123906195164\n",
      "Epoch 494, Loss: 0.0003200611681677401\n",
      "Epoch 495, Loss: 0.0028801036532968283\n",
      "Epoch 496, Loss: 0.0005863364785909653\n",
      "Epoch 497, Loss: 0.0034297446254640818\n",
      "Epoch 498, Loss: 0.0033021082635968924\n",
      "Epoch 499, Loss: 0.0029406386893242598\n",
      "Epoch 500, Loss: 0.00345814973115921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Trening modelu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wyniki klasyfikacji:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.58      0.82      0.68        74\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.29      0.42      0.34        12\n",
      "\n",
      "    accuracy                           0.52       128\n",
      "   macro avg       0.22      0.31      0.26       128\n",
      "weighted avg       0.36      0.52      0.43       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ewaluacja\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        for i in range(len(preds)):\n",
    "            pred = preds[i][batch['attention_mask'][i] == 1]\n",
    "            label = labels[i][batch['attention_mask'][i] == 1]\n",
    "            \n",
    "            pred = pred[label != -100]\n",
    "            label = label[label != -100]\n",
    "            \n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            true_labels.extend(label.cpu().numpy())\n",
    "\n",
    "# Wyświetlenie wyników dla zbioru testowego\n",
    "print(\"\\nWyniki klasyfikacji:\")\n",
    "print(classification_report(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predykcje dla przykładowych zdań ze zbioru testowego\n",
    "label_mapping = {0: 'negatywny', 1: 'neutralny', 2: 'pozytywny', 3: 'inne'}\n",
    "\n",
    "def predict_sentence(sentence_words):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            sentence_words,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        ).to(device)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        word_predictions = []\n",
    "        word_ids = inputs.word_ids()\n",
    "        \n",
    "        current_word = None\n",
    "        current_predictions = []\n",
    "        \n",
    "        for token_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "            if word_idx != current_word:\n",
    "                if current_word is not None:\n",
    "                    # Wybierz najczęstszą predykcję dla słowa\n",
    "                    word_predictions.append(max(set(current_predictions), key=current_predictions.count))\n",
    "                current_word = word_idx\n",
    "                current_predictions = []\n",
    "            current_predictions.append(predictions[0][token_idx].item())\n",
    "        \n",
    "        # Dodaj ostatnie słowo\n",
    "        if current_predictions:\n",
    "            word_predictions.append(max(set(current_predictions), key=current_predictions.count))\n",
    "            \n",
    "        return word_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Przykładowe predykcje dla zdań ze zbioru testowego:\n",
      "\n",
      "Zdanie 1:\n",
      "Słowo: Jakość          Predykcja: neutralny\n",
      "Słowo: i               Predykcja: inne\n",
      "Słowo: praktyczność    Predykcja: neutralny\n",
      "Słowo: wykonania       Predykcja: neutralny\n",
      "Słowo: tego            Predykcja: neutralny\n",
      "Słowo: trymera         Predykcja: neutralny\n",
      "Słowo: pozostawia      Predykcja: neutralny\n",
      "Słowo: naprawdę        Predykcja: neutralny\n",
      "Słowo: wiele           Predykcja: neutralny\n",
      "Słowo: do              Predykcja: inne\n",
      "Słowo: życzenia        Predykcja: neutralny\n",
      "Słowo: O               Predykcja: inne\n",
      "Słowo: golarce         Predykcja: neutralny\n",
      "Słowo: w               Predykcja: neutralny\n",
      "Słowo: tym             Predykcja: neutralny\n",
      "Słowo: zestawie        Predykcja: neutralny\n",
      "Słowo: nie             Predykcja: neutralny\n",
      "Słowo: warto           Predykcja: neutralny\n",
      "Słowo: nawet           Predykcja: neutralny\n",
      "Słowo: wspominać       Predykcja: neutralny\n",
      "Słowo: Lepiej          Predykcja: neutralny\n",
      "Słowo: od              Predykcja: negatywny\n",
      "Słowo: razu            Predykcja: neutralny\n",
      "Słowo: ja              Predykcja: inne\n",
      "Słowo: wyrzucić        Predykcja: neutralny\n",
      "Słowo: Za              Predykcja: neutralny\n",
      "Słowo: połowę          Predykcja: neutralny\n",
      "Słowo: ceny            Predykcja: neutralny\n",
      "Słowo: można           Predykcja: neutralny\n",
      "Słowo: kupić           Predykcja: neutralny\n",
      "Słowo: nieco           Predykcja: negatywny\n",
      "Słowo: lepszy          Predykcja: neutralny\n",
      "Słowo: produkt         Predykcja: neutralny\n",
      "Słowo: produkowany     Predykcja: neutralny\n",
      "Słowo: dla             Predykcja: neutralny\n",
      "Słowo: marketów        Predykcja: neutralny\n",
      "Słowo: jak             Predykcja: inne\n",
      "Słowo: np              Predykcja: neutralny\n",
      "Słowo: Lidl            Predykcja: neutralny\n",
      "Słowo: Jednym          Predykcja: neutralny\n",
      "Słowo: słowem          Predykcja: neutralny\n",
      "Słowo: warto           Predykcja: neutralny\n",
      "Słowo: dołożyć         Predykcja: inne\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nPrzykładowe predykcje dla zdań ze zbioru testowego:\")\n",
    "for i in range(min(3, len(test_texts))):  # Pokazujemy pierwsze 3 zdania\n",
    "    sentence = test_texts[i]\n",
    "    predictions = predict_sentence(sentence)\n",
    "    \n",
    "    print(f\"\\nZdanie {i+1}:\")\n",
    "    for word, pred in zip(sentence, predictions):\n",
    "        pred_label = label_mapping[pred]\n",
    "        print(f\"Słowo: {word:15} Predykcja: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
