{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: torch in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jkoro\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('data/annotations_all_batches - WORD - SECOND BATCH.csv')\n",
    "\n",
    "# Keep necessary columns and drop NaNs\n",
    "data = data[['sentence_id', 'word', 'final-annotation']].dropna()\n",
    "\n",
    "# Map labels to descriptive names\n",
    "# Update the label mapping\n",
    "label_mapping = {\n",
    "    0: 'negatywny',\n",
    "    1: 'neutralny',\n",
    "    2: 'pozytywny',\n",
    "    3: 'inne',\n",
    "}\n",
    "\n",
    "# Map final-annotation column\n",
    "data['final-annotation'] = data['final-annotation'].astype(int).map(label_mapping)\n",
    "\n",
    "# Group by sentences\n",
    "grouped = data.groupby('sentence_id').agg({'word': list, 'final-annotation': list}).reset_index()\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(grouped, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>final-annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[Super, uchwyt, Jak, go, dostałem, myślałem, ż...</td>\n",
       "      <td>[pozytywny, neutralny, inne, inne, neutralny, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[założyłem, sam, i, działazamiennik, jak, w, o...</td>\n",
       "      <td>[neutralny, neutralny, inne, neutralny, neutra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Do, Bosch, SMV53L10EU, pasuje, IDEALNIE, wpas...</td>\n",
       "      <td>[inne, neutralny, neutralny, pozytywny, pozyty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[Zestaw, zawiera, wszystko, czego, potrzeba, a...</td>\n",
       "      <td>[neutralny, pozytywny, pozytywny, pozytywny, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id                                               word  \\\n",
       "4            5  [Super, uchwyt, Jak, go, dostałem, myślałem, ż...   \n",
       "2            3  [założyłem, sam, i, działazamiennik, jak, w, o...   \n",
       "0            1  [Do, Bosch, SMV53L10EU, pasuje, IDEALNIE, wpas...   \n",
       "3            4  [Zestaw, zawiera, wszystko, czego, potrzeba, a...   \n",
       "\n",
       "                                    final-annotation  \n",
       "4  [pozytywny, neutralny, inne, inne, neutralny, ...  \n",
       "2  [neutralny, neutralny, inne, neutralny, neutra...  \n",
       "0  [inne, neutralny, neutralny, pozytywny, pozyty...  \n",
       "3  [neutralny, pozytywny, pozytywny, pozytywny, p...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>word</th>\n",
       "      <th>final-annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[Jakość, i, praktyczność, wykonania, tego, try...</td>\n",
       "      <td>[neutralny, inne, neutralny, neutralny, inne, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id                                               word  \\\n",
       "1            2  [Jakość, i, praktyczność, wykonania, tego, try...   \n",
       "\n",
       "                                    final-annotation  \n",
       "1  [neutralny, inne, neutralny, neutralny, inne, ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define all unique labels\n",
    "unique_labels = ['negatywny', 'neutralny', 'pozytywny', 'inne']\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Update TokenClassificationDataset class remains the same\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label_mapping, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_mapping = label_mapping\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.iloc[index]['word']\n",
    "        labels = self.data.iloc[index]['final-annotation']\n",
    "\n",
    "        # Tokenize and align labels\n",
    "        encoding = self.tokenizer(sentence, is_split_into_words=True, \n",
    "                                  truncation=True, padding='max_length', \n",
    "                                  max_length=self.max_len, return_tensors=\"pt\")\n",
    "        encoded_labels = [-100] * self.max_len\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        label_ids = [self.label_mapping[label] for label in labels]\n",
    "        label_index = 0\n",
    "\n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                continue\n",
    "            if word_id != word_ids[i - 1]:\n",
    "                encoded_labels[i] = label_ids[label_index]\n",
    "                label_index += 1\n",
    "\n",
    "        encoding[\"labels\"] = torch.tensor(encoded_labels, dtype=torch.long)\n",
    "        return {key: val.squeeze() for key, val in encoding.items()}\n",
    "\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = TokenClassificationDataset(train_data, tokenizer, label2id)\n",
    "test_dataset = TokenClassificationDataset(test_data, tokenizer, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetunnig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\jkoro\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(unique_labels))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ewaluacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 998.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4543756246566772,\n",
       " 'eval_runtime': 0.0993,\n",
       " 'eval_samples_per_second': 10.066,\n",
       " 'eval_steps_per_second': 10.066,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model('./bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = BertForTokenClassification.from_pretrained('./bert_model')\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jakość       --> inne      \n",
      "i            --> pozytywny \n",
      "praktyczność --> neutralny \n",
      "wykonania    --> neutralny \n",
      "tego         --> inne      \n",
      "trymera      --> neutralny \n",
      "pozostawia   --> neutralny \n",
      "naprawdę     --> neutralny \n",
      "wiele        --> neutralny \n",
      "do           --> neutralny \n",
      "życzenia     --> inne      \n",
      "O            --> neutralny \n",
      "golarce      --> neutralny \n",
      "w            --> neutralny \n",
      "tym          --> inne      \n",
      "zestawie     --> neutralny \n",
      "nie          --> inne      \n",
      "warto        --> neutralny \n",
      "nawet        --> neutralny \n",
      "wspominać    --> neutralny \n",
      "Lepiej       --> neutralny \n",
      "od           --> neutralny \n",
      "razu         --> neutralny \n",
      "ja           --> neutralny \n",
      "wyrzucić     --> inne      \n",
      "Za           --> neutralny \n",
      "połowę       --> neutralny \n",
      "ceny         --> neutralny \n",
      "można        --> inne      \n",
      "kupić        --> inne      \n",
      "nieco        --> inne      \n",
      "lepszy       --> inne      \n",
      "produkt      --> neutralny \n",
      "produkowany  --> neutralny \n",
      "dla          --> neutralny \n",
      "marketów     --> neutralny \n",
      "jak          --> inne      \n",
      "np           --> neutralny \n",
      "Lidl         --> neutralny \n",
      "Jednym       --> inne      \n",
      "słowem       --> inne      \n",
      "warto        --> neutralny \n",
      "dołożyć      --> neutralny \n",
      "kilkanaście  --> neutralny \n",
      "złotych      --> neutralny \n",
      "i            --> neutralny \n",
      "zakupić      --> inne      \n",
      "np           --> inne      \n",
      "Philipsa     --> inne      \n",
      "Różnica      --> neutralny \n",
      "kolosalna    --> neutralny \n",
      "Nie          --> inne      \n",
      "polecam      --> neutralny \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Perform evaluation\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to label ids\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Convert label ids to label names\n",
    "predicted_labels = [[id2label[p] for p in pred] for pred in predictions]\n",
    "\n",
    "# Print the predictions for the test set\n",
    "for i, sentence in enumerate(test_data['word']):\n",
    "    for word, pred in zip(sentence, *predicted_labels):\n",
    "        print(f\"{word:<12} --> {pred:<10}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
